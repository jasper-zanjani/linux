{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Linux","text":"<p>SystemD is the de facto Linux init system on modern distributions.</p> <p>A process runs in its own user address space, a protected space which can't be disturbed by other users</p> <ul> <li>all processes on a Linux system are child processes of a common parent: the init process which is executed by the kernel at boot time (PID 1)</li> <li>every Linux process inherits the environment (PATH variable, etc) and other attributes of its parent process</li> </ul> <p>Every process has a parent; a process can spawn children in a process that is actually made of two separate system calls.</p> <ul> <li>Shell-internal commands (cd, echo, etc. and variable assignments) do not spawn child processes</li> <li>Shell scripts are executed by spawning a sub-shell, which becomes the script's parent</li> <li>External commands are spawned as children of the parent as described above</li> </ul> <p>Bootloaders like GRUB (GRand Unified Bootloader) turn on power supplies and scan buses and interfaces to locate the kernel image and the root filesystem.  LILO (LInux LOader) is also another bootloader that can be found on older Linux systems.</p> <p>Microcontrollers may be listening when the system is nominally off; they typically have their own BIOS and kernels and are inaccessible from the main system:</p> <ul> <li>Baseboard Management Controller (BMC) responds to wake-on-LAN (WOL)</li> <li>Intel Management Engine (IME) x64 software suite for remote management of systems; firmware is based on <code>Minix</code> and runs on the Platform Controller Hub processor, not the main CPU</li> <li>System Management Mode (SMM) launches UEFI software</li> </ul> <p>Linux kernel is typically named vmlinux (or vmlinuz when compressed). Kernel ring buffer contains messages related to the Linux kernel. A ring buffer is a data structure that is always the same size; old messages are discarded as new ones come in, once the buffer is full. <code>dmesg</code> is used to see its contents, and the messages are also stored in <code>/var/log/dmesg</code></p> <p>Kernel modules can be loaded, listed, or removed from the running kernel.</p>"},{"location":"#security","title":"Security","text":"<p>Similar to DLL files on Windows systems, .so (\"shared object\") library files on Linux allow code to be shared by various processes.  They are vulnerable to injection attacks. </p> Library injection vulnerability <p>One file in particular, linux-vdso.so.1, finds and locates other shared libraries and is mapped by the kernel into the address space of every process.  This library-loading mechanism can be exploited through the use of the environment variable <code>LD_PRELOAD</code>, which is considered the most convenient way to load a shared library in a process at startup.  If defined, this variable is read by the system and the library is loaded immediately after linux-vdso.so.1 into every process that is run. </p> <p>This attack can be detected using the osquery tool.  This tool represents the system as a relational database which can then be queried, in particular against the process_envs table.</p> <p>Filesystem access control lists (FACL) allow you to grant permissions to more than one group, i.e. in cases where more than one department of a corporation needs access to the same files. They are made up of access control entries (ACE).  FACL permissions will be indicated in a <code>ls -l</code> command by the presence of a \"+\" after the symbolic notation for the traditional UGO permissions.  Acl is a dependency of <code>systemd</code>.</p> <p>To enable it, add \",acl\" to options in <code>fstab</code> file, then mount/unmount disk. If enabling FACL on root partition, system has to be rebooted.</p>"},{"location":"#glossary","title":"Glossary","text":""},{"location":"#alsa","title":"ALSA","text":"<p>Advanced Linux Sound Architecture (ALSA) replaced the earlier \"Open Sound System\". (src)</p> <p>ALSA kernel modules are designed to offer an interface that \"corresponds to that of the hardware\" to keep the modules simple, and similar cards will offer a similar interface.  ALSA kernel modules offer two interfaces: operational and configuration</p> <p>Operational interface are exposed at /dev/, with 3 main types of devices:</p> <ul> <li>PCM devices, for recording or playing digitized sound samples, come in two varieties - output and input - and are numbered from 0, which is generally for analog multichannel sound.</li> <li>CTL or controls are for manipulating the internal mixer and routing of the card. Controls come in 3 types;<ul> <li>Playback controls are associated with an output device or copy (input-to-output) routes</li> <li>Capture controls are associated with an input device or copy (output-to-input) routes</li> <li>Feature controls drive features of the card or mixer, usually just a switch to enable or disable the feature, though some also have levels. The Master Volume control is the most typical example, which allows control of the internal amplifier feature of the card. A more interesting example is that of a 3D spatializer that can be represented by a switch to enable or disable it as well as two levels.</li> </ul> </li> <li>MIDI to control the MIDI port, if it exists</li> <li>Optionally, sequencer devices may also exist if the card has a builtin sound synthesizer with an associated timer device</li> </ul> <p>Configuration interfaces are exposed at /proc/asound/ tree (ref <code>amixer</code>)</p> <p>Cards have input or output sockets, and the mixer is controlled by the CTL device and routes sound samples among devices and sockets.</p> <p>Typical channel assignments - 0: front left - 1: front right - 2: rear left - 3: rear right</p> Berkeley Software Distribution (BSD) <p>BSD began in the 70s and was based on AT&amp;T original code.  First source distributions required user to purchase a source license from AT&amp;T, since much of the BSD source was derivative of UNIX.</p> <p>Berkeley finally released a \"wholly-BSD\" product as Network Release 1 in 1989, which satisfied vendor demand for the TCP/IP networking code for PC.</p> <p>Work immediately began to reconstruct the remaining functionality of UNIX, which was completed in Network Release 2, released in 1991, which was based entirely on Berkeley code. Eventually this resulted in the 386BSD distribution, which then spawned five interrelated BSD distros: BSDI (now Wind River), NetBSD, FreeBSD, OpenBSD, and Darwin/Mac OS X</p> <p>Unix System Laboratories (USL) sued BSDI after BSDI attempted to market its product as a real UNIX, and other BSD distributions were affected by disputed code.  Ultimately 3 out of the 18,000 files that made up the Network Release 2 distribution were removed, which became known as BSD-lite, released in 1994.  This legal dispute was partly to blame for Linux's rapid ascent in popularity. </p>"},{"location":"#distributions","title":"Distributions","text":"<ul> <li>Alpine Linux is a security-oriented, lightweight Linux distribution used in containers and hardware.</li> <li>Clear Linux is a rolling release distro from Intel with a custom package management system based on bundles, collections of packages that contain everything an application requires, including dependencies.  Clear's update process also has the ability to do delta downloads, preserving bandwidth.  It does not provide access with unusual licenses, like ZFS, Chrome, or FFmpeg.</li> <li>SUSE<ul> <li>OpenSUSE Leap is a rebuild of SUSE Linux Enterprise Server, similar to how CentOS was historically a rebuild of RHEL.</li> <li>SUSE Linux Enterprise Server (SLES) (\"slee\") is SUSE's fixed-release distribution of Linux intended for enterprises, and as such is comparable to Red Hat's RHEL.</li> </ul> </li> </ul> display manager Basically display managers are the login screens, while the GUI manipulated during normal use represents the desktop environment (i.e. GNOME, KDE, XFCE, etc). initrd (\"initial RAM disk\") A temporary file system that's loaded into memory when the system boots"},{"location":"#pipewire","title":"Pipewire","text":"Pipewire is a media server intended to facilitate audio and video handling in Linux as a replacement for PulseAudio and JACK. It exposes a graph-based processing engine that abstracts audio and video devices."},{"location":"#pulseaudio","title":"PulseAudio","text":"<p>PulseAudio is a sound server for POSIX OSes and a fixture on many Linux distributions.</p> <p>PulseAudio is built around sources and sinks (i.e. devices) connected to source outputs and sink inputs (streams)</p> <ul> <li>Source is an input device that produces samples, usually running a thread with its own event loop, generating sample chunks which are posted to all connected source outputs</li> <li>Source output is a recording stream which consumes samples from a source</li> <li>Sink is an output device that consumes samples, usually running a thread with its own event loop mixing sample chunks from connect sink inputs</li> <li>Sink input is a playback stream, connected to a sink and producing samples for it</li> </ul> qmail <p>MTA designed as a drop-in replacement for Sendmail, notable for being the first to be \"security-aware\".  Its various modular subcomponents run independently and are mutually untrustful.  It uses SMTP to exchange messages with other MTAs. </p> <p>It was written by Dan Bernstein, a professor of mathematics famous for litigating against the US government with regard to export controls on encryption algorithms.  qmail was deprecated and removed from Arch repos in 2005.</p> SMB Client/server protocol developed in the early 1980s by Intel, Microsoft, and IBM that has become the native protocol for file and printer sharing on Windows. It is implemented in the Samba application suite."},{"location":"#wsl","title":"WSL","text":"<p>Windows Subsystem for Linux (WSL) is shipped with Windows and tied to the Windows release cycle. Windows ships from a single massive codebase, of which WSL is part.  WSL was written mostly in C and and has 3 million monthly active users.</p> <p>WSL implements user services to connect to WSL distros and to run Windows-native applications like CMD.exe.  WSL implements a 9P Protocol file server to provide seamless integration of the virtualized Linux filesystem and that of the Windows host.</p> <p>WSL 1 worked under a translation architecture where system calls were translated to NT kernel calls. This meant that applications that used system calls that were newer or more difficult to implement, like GUI applications or Docker, did not run on v1. </p> <p>WSL2 shifted to a lighweight virtualization model using the Linux kernel. Now Docker runs on WSL2 and GUI applications can run by using an X server.</p> <p>WSL v1 is available on Azure VMs if nested virtualization is enabled. WSL2 support is forthcoming.</p> <p>VHDs for WSL distributions are available at <code>%LOCALAPPDATA%\\Packages\\&lt;PackageFamilyName&gt;\\LocalState</code> where <code>&lt;PackageFamilyName&gt;</code> reflects the name of the Microsoft Store package of the distro, i.e.:</p> <ul> <li>CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc</li> <li>TheDebianProject.DebianGNULinux_76v4gfsz19hv4</li> </ul> <pre><code># Install distro\nwsl.exe --install -d Ubuntu-20.04\n\n# Remove distro\nwsl.exe --unregister Ubuntu-20.04\n</code></pre> <p>By default, WSL appears to copy the Windows native hosts file at %SystemRoot%\\System32\\drivers\\etc\\hosts to the distro's /etc/hosts file.</p>"},{"location":"Concurrency/","title":"Concurrency","text":""},{"location":"Concurrency/#glossary","title":"Glossary","text":""},{"location":"Concurrency/#linearizability","title":"Linearizability","text":"<p>In concurrent programming, an operation is linearizable if it consists of an ordered list of callbacks that may be extended by adding response events such that:</p> <ul> <li>The extended list is serializable</li> <li>The serializable sequential history is a subset of the original unextended list</li> </ul>"},{"location":"Concurrency/#lock","title":"Lock","text":"<p>When a thread wants a lock already owned by another thread, the thread is blocked and must wait until the lock becomes free.</p> <ul> <li>Spinning locks are suitable for very short timeframes</li> <li>Sleeping locks, including mutexes</li> </ul> <p>The Linux kernel also provides CPU local locks</p>"},{"location":"Concurrency/#mutex","title":"Mutex","text":"In the Linux kernel, mutex refers to a particular locking primitive that enforces serialization on shared memory systems."},{"location":"Concurrency/#serializability","title":"Serializability","text":"In transaction processing, a transaction schedule is serializable if its outcome is equal to the outcome of its individual transactions executed serially. Because transactions are normally executed concurrently, this is the major correctness criterion for concurrent transactions."},{"location":"Concurrency/#spinlock","title":"Spinlock","text":"A lock that causes a thread trying to acquire it to simply wait in a loop (\"spin\") while repeatedly checking whether the lock is available."},{"location":"Concurrency/#transaction","title":"Transaction","text":"In computer science, transactions are individual, indivisible operations that must succeed or fail as a complete unit. Transaction processing guards against hardware and software errors that might leave a transaction partially completed. These operations are executed on databases or modern filesystems to ensure the system is in a consistent state."},{"location":"Containers/","title":"Containers","text":"<p>Containers run applications in an isolated namespace, meaning it only has access to resources that are made available to it by the container runtime. Resource governance means that a container has access only to a specified number of processor cycles, system memory, and other resources. Containers allow applications to be packaged with their dependencies in container images, which will run the same regardless of underlying operating system or infrastructure and are downloaded from container registries like Docker Hub. Container registries are not to be confused with repositories, which are subcomponents of registries.</p>"},{"location":"Containers/#cgroups","title":"Cgroups","text":"History <p>\"Task Control Groups\" were first merged into kernel 2.6.24 with the ability for multiple hierarchies to be created. The logic behind the creation of multiple hierarchies was to enable maximum flexibility in policy creation. However, because a controller could only belong to a single hierarchy, after some years this began to be seen as a design flaw.</p> <p>This motivated a redesign into a single unified cgroup hierarchy, and v2 was merged in 3.16 and made stable in 4.5.</p> <p>Control groups or  cgroups are a Linux kernel feature that isolates, labels, and manages resources (CPU time, memory, and network bandwidth) for a collection of tasks (processes).</p> <p>Cgroup subsystems (also called controllers or resource controllers in documentation) represent a single resource (i.e. io, cpu, memory, devices).</p> <ul> <li>freezer suspends or resumes tasks in a cgroup</li> <li>net_cls tags network packets with a class identifier that allows the Linux traffic controller to identify packets originating from a particular cgroup task</li> <li>net_prio allows network traffic to be prioritized per interface</li> <li>ns the namespace subsystem</li> </ul> <p>Since cgroups v2, all mounted controllers reside in a single unified hierarchy. A list of these is generated by the Linux kernel at /proc/cgroups.</p> <p>You can confirm that the cgroup2 filesystem is mounted at /sys/fs/cgroup <pre><code>mount -l | grep cgroup -\n</code></pre></p>"},{"location":"Containers/#kubernetes","title":"Kubernetes","text":"History <p>Kubernetes was first announced by Google in mid-2014.  It had been developed by Google after deciding to open-source the Borg system, a cluster and container management system that formed the automation infrastructure that powered the entire Google enterprise. Kubernetes coalesced from a fusion between developers working on Borg and Compute Engine. Borg eventually evolved into Omega.</p> <p>By that time, Amazon had established a market advantage and the developers decided to change their approach by introducing a disruptive technology to drive the relevance of the Compute platform they had built.  They created a ubiquitous abstraction that could run better than anyone else.</p> <p>At the time, Google had been trying to engage the Linux kernel team and trying to overcome their skepticism.  Internally, the project was framed as offering \"Borg as a Service\", although there were concerns that Google was in danger of revealing trade secrets.</p> <p>Google ultimately donated Kubernetes to the Cloud Native Computing Foundation.</p> <p>Kubernetes's heptagonal logo is an allusion to when it was called \"Project 7\" as a reference to Star Trek's Borg character 7 of 9.</p> <p>Kubernetes (Greek for \"helmsman\", \"pilot\", or \"captain\" and \"k8s\" for short) has emerged as the leading container orchestrator in the industry since 2018.  It provides a layer that abstracts infrastructure, including computers, networks, and other computers, for applications deployed on top.</p> <p>Kubernetes can be visualized as a system built from layers, with each higher layer abstracting the complexity of the lower levels. One server serves as the master, exposing an API for users and clients, assigning workloads, and orchestrating communication between other components. The processes on the master node are also called the control plane. Other machines in the cluster are called nodes or workers and accept and run workloads using available resources.  A Kubernetes configuration files is called a kubeconfig.</p> <p>Kubernetes resources or objects, each associated with a URL, represent the configuration of a cluster. Resource and object are often used interchangeably, but more precisely the resource refers to the URL path that points to the object, and an object may be accessible through multiple resources. Every object type in the Kubernetes API has a controller (i.e. deployment controller, etc.) that reads desired state from the Spec section of the manifest and reports its actual state by writing to the Status section.</p> <p>An object's manifest, presented in JSON or YAML, represents its declarative configuration, and contains four sections:</p> <ul> <li>type metadata, specifying the type of resource</li> <li>object metadata, specifying name and other identifying information</li> <li>spec: desired state of resource</li> <li>state: produced strictly by the resource controller and represents the current status of resource</li> </ul> <p>An explanation of each field available in the API of any object type can be displayed on the command-line <pre><code>kubectl explain nodes\nkubectl explain no.spec\n</code></pre></p> Display the manifest of a node<pre><code>kubectl get node $NODE -o yaml\nkubectl describe node kind-worker-2\n</code></pre> <p>A pod is the most atomic unit of work which encompasses one or more tightly-coupled containers that will be deployed together on the same node. All containers in a pod share the same Linux namespace, hostname, IP address, network interfaces, and port space. This means containers in a pod can communicate with each other over localhost, although care must be taken to ensure individual containers do not attempt to use the same port. However their filesystems are isolated from one another unless they share a Volume.</p> <p>Every Pod occupies one address in a shared range, so communication between Pods is simple.</p> <p>Compute resources of containers can be limited at pod.spec.containers.resources.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nspec:\ncontainers:\n- image: nginx\nname: nginx\nresources:\nrequests:\nmemory: \"64Mi\"\ncpu: \"500m\"\nlimits:\nmemory: \"128Mi\"\ncpu: \"500m\"\n</code></pre> <p>Kubernetes can monitor Pod health by using probes, which can be categorized by how they measure health:</p> <ul> <li>Readiness: i.e. Is the container ready to serve user requests?</li> <li>Liveness: i.e. Is the container running as intended?</li> </ul>"},{"location":"Containers/#tasks","title":"Tasks","text":""},{"location":"Containers/#gke","title":"GKE","text":"<p>Google Kubernetes Engine nodes are actually Google Compute Engine VMs. Create GKE cluster<pre><code>gcloud container clusters create hello-cluster --num-nodes=1    # Standard cluster\ngcloud compute instances list # (1)\ngcloud container clusters create-auto hello-cluster # (2)\ngcloud container clusters describe hello-cluster\n</code></pre></p> <ol> <li>If a default zone is set, an Autopilot cluster won't be able to be created without explicitly specifying --region.</li> </ol> <p>Save a Kubernetes cluster's credentials to a kubeconfig. <pre><code>gcloud container clusters get-credentials my-cluster\n</code></pre></p>"},{"location":"Containers/#windows-server","title":"Windows Server","text":"<p>Windows Server 2016 supports Windows Server Containers and Hyper-V Containers, which create a separate copy of the operating system kernel for each container. The \"Containers\" feature must be installed on Windows Server 2016 hosts, and to create Hyper-V containers the Hyper-V role must also be installed (although the Hyper-V management tools are not necessary if VMs are not going to created). Windows container hosts need to have Windows installed to C:.</p> <p>Nano Server once could serve as Docker hosts, but no longer; Nano Servers are now intended to be deployed as containers themselves.</p> <p>The Powershell Docker module has been deprecated for years.</p>"},{"location":"Containers/#commands","title":"Commands","text":""},{"location":"Containers/#cgconfigservice","title":"cgconfig.service","text":"cgconfig, which is a part of the libcgroup package, can be used to run at start time to reestablish predefined cgroups."},{"location":"Containers/#kubectl","title":"kubectl","text":"Show available contexts<pre><code>kubectl config get-contexts\n</code></pre> Switch to a different context<pre><code>kubectl config use-context $NAMESPACE\nkubectl config use $NAMESPACE\n</code></pre> Display resources<pre><code>kubectl get nodes\nkubectl get pods\nkubectl get deployments\n</code></pre> <pre><code>kubectl describe nodes/gke-*4ff6f64a-6f4v\n</code></pre> Execute a command on a pod with only a single container, returning output<pre><code>kubectl exec $pod -- env\n</code></pre> Get a shell to a running container<pre><code>kubectl exec --stdin --tty $pod -- /bin/bash\n</code></pre> <p>When a pod contains more than one container, the container must be specified with -c/--container.  <pre><code>kubectl exec --stdin --tty $pod --container $container -- /bin/bash\n</code></pre></p> <pre><code>kubectl run nginx --image=nginx\nkubectl delete pod nginx\n</code></pre> <pre><code>kubectl create deployment nginx --image=nginx\n</code></pre> <p>Number of replicas can be set on creation of a deployment by specifying an argument to --replicas <pre><code>kubectl create deployment nginx --image=nginx --replicas=4\n</code></pre></p> <p>Replica count is set in an existing deployment by scaling <pre><code>kubectl scale deploy/nginx --replicas=2\n</code></pre></p> <p>Expose a port <pre><code>kubectl expose deployment/nginx --port=80 --type=LoadBalancer\n</code></pre></p> <p>List Kubernetes objects</p> <pre><code>kubectl api-resources\n</code></pre> <p>Get a description of a resource</p> <pre><code>kubectl explain nodes.status.addresses.address\n</code></pre>"},{"location":"Containers/#podman","title":"podman","text":"<p>On RHEL, podman can be installed as a package or as part of a module <pre><code>dnf module install container-tools\n</code></pre></p> <p>With few exceptions, podman exposes a command-line API that closely imitates that of Docker.</p> Arch Linux <p>On Arch, /etc/subuid and /etc/subgid have to be set. These are colon-delimited files that define the ranges for namespaced UIDs and GIDs to be used by each user.  Conventionally, these ranges begin at 100,000 (for the first, primary user) and contain 65,536 possible values. <pre><code>terry:100000:65536\nalice:165536:65536\n</code></pre></p> <p>Then podman has to be migrated <pre><code>podman system migrate\n</code></pre></p> <p>Podman supports pulling from various repos using aliases that are defined in /etc/containers/registries.conf.d. RHEL and derivative distributions support additional aliases, some of which reference images that require a login.</p> <p>For example, Red Hat offers a Python 2.7 runtime from the RHSCL (Red Hat Software Collections) repository on registry.access.redhat.com, which does not require authentication. However, Python 3.8 is only available from registry.redhat.io, which does. Interestingly, other Python runtimes are available from the ubi7 and ubi8 repos from unauthenticated registries.</p> <p>Container images are stored in ~/.local/share/containers/storage. <pre><code>podman pull rhscl/httpd-24-rhel7 # (1)\n</code></pre></p> <ol> <li>Alias to registry.access.redhat.com/rhscl/httpd-24-rhel7</li> </ol> <p>The Z option is necessary on SELinux systems (like RHEL and derivatives) and tells Podman to label the content with a private unshared label. On systems running SELinux, rootless containers must be explicitly allowed to access bind mounts. Containerized processes are not allowed to access files without a SELinux label. <pre><code>podman run -d -v=/home/jasper/notes/site:/usr/share/nginx/html:Z -p=8080:80 --name=notes nginx\npodman run -d -v=/home/jasper/notes/site:/usr/local/apache2/htdocs:Z -p=8080:80 --name=notes httpd-24\n</code></pre></p> <p>Mapped ports can be displayed <pre><code>podman port -a\n</code></pre></p> <p>Output a SystemD service file from a container to STDOUT (this must be redirected to a file) <pre><code>podman generate systemd notes \\\n--restart-policy=always   \\\n--name                    \\ # (3)\n--files                   \\ # (2)\n--new                     \\ # (1)\n</code></pre></p> <ol> <li>Yield unit files that do not expect containers and pods to exist but rather create them based on their configuration files.</li> <li>Generate a file with a name beginning with the prefix (which can be set with --container-prefix or --pod-prefix) and followed by the ID or name (if --name is also specified)</li> <li>In conjunction with --files, name the service file after the container and not the ID number.</li> </ol>"},{"location":"Containers/#systemd-cgls","title":"systemd-cgls","text":"systemd-cgls recursively shows the contents of the selected cgroup hierarchy in a tree."},{"location":"Containers/#glossary","title":"Glossary","text":"apiVersion <p>Kubernetes object field found in Type metadata.</p> <p>apiVersion is typically v1, but for some object types the API group is specified, i.e. for Deployments:  <pre><code>apiVersion: apps/v1\n</code></pre></p>"},{"location":"Containers/#dockerfile","title":"Dockerfile","text":"<p>A Docker image consists of read-only layers, each of which represents an instruction that incrementally the changes the image being built up.  Dockerfiles can be used to construct new images using <code>docker build</code>. The build process can be optimized by placing multiple commands in the same <code>RUN</code> instruction. Dockerfiles are named simply \"Dockerfile\" with no extension or variation.</p> Node on AlpineWindows Server NanoWindows Server Core <pre><code>FROM alpine\nRUN apk update &amp;&amp; apk add nodejs\nCOPY . /app\nWORKDIR /app\nCMD [\"node\",\"index.js\"]\n</code></pre> <pre><code>FROM microsoft/windowsservercore\nRUN powershell -command install-windowsfeature dhcp -includemanagementtools\nRUN powershell -configurationname microsoft.powershell -command add-dhcpserverv4scope -state active -activatepolicies $true -name scopetest -startrange 10.0.0.100 -endrange 10.0.0.200 -subnetmask 255.255.255.0\nRUN md boot\nCOPY ./bootfile.wim c:/boot/\nCMD powershell\n</code></pre> <pre><code>FROM microsoft/windowsservercore\nMAINTAINER @mike_pfeiffer\nRUN powershell.exe -Command Install-WindowsFeature Web-Server\nCOPY ./websrc c:/inetpub/wwwroot\nCMD [ \"powershell\" ]\n</code></pre> Deployment A uniformly managed set of Pod instances, all based on the same container image. The Deployment controller enables release capabilities, the deployment of new Pod versions with no downtime. Exposing a Deployment creates a Service. Desired State Management The Desired State Management system is used by Kubernetes to describe a cluster's desired state declaratively. emptyDir Ephemeral Kubernetes volume type that shares the Pod's lifetime and where data is stored in RAM. emptyDir volumes can use tmpfs file systems. ENTRYPOINT <p>Rarely used Docker declaration. When ENTRYPOINT is present, the CMD declaration becomes the default argument passed to the command in ENTRYPOINT.</p> <p>The Kubernetes --command flag (<code>pod.spec.containers.command</code> resource) can override the contents of ENTRYPOINT.</p> etcd Distributed key-value data store Event Kubernetes object type that contains information about what happened to the object. Events are deleted one hour after creation by default. <pre><code>kubectl get events\nkubectl get ev\n</code></pre> Unlike most other objects, Event manifests have no spec or status sections."},{"location":"Containers/#helm","title":"Helm","text":"<p>Helm is a package manager for Kubernetes. </p> <p>Helm packages are refered to as charts. Charts are a collection of files and directories that adhere to a specification. A chart is packed when tarred and gzipped.</p> <ul> <li>Chart.yaml contains metadata</li> <li>templates/ contains Kubernetes manifests potentially annotated with templating directives</li> <li>values.yaml provides default configuration</li> </ul> <p>It is managed using the helm CLI utility.</p> Create a new chart<pre><code>helm create foo\n</code></pre> <p>There is no longer a default Helm repository, although there are many available at the Artifact Hub</p> kind Kubernetes object field found in the Type metadata which specifies the type of resource, i.e. Node, Deployment, Service, Pod, etc. kubeconfig YAML configuration file located at $HOME/.kube/config by default. A colon-delimited list of kubeconfigs can be specified by setting the <code>KUBECONFIG</code> environment variable. A kubeconfig can be explicitly specified with the --kubeconfig flag. Label <p>Labels are key-value pairs that are attached to Kubernetes objects.</p> <p>Config for a Pod with two labels: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: label-demo\nlabels:\nenvironment: production\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\n</code></pre></p> Master node <p>A master node runs 3 processes, called master (control plane) components: </p> <ul> <li>kube-apiserver exposes a RESTful API and serves as a glue between other Kubernetes components</li> <li>kube-scheduler determines how to balance container workloads across nodes using an algorithm</li> <li>kube-controller-manager performs cluster operations like managing nodes and making changes to desired status</li> </ul> millicore (m)  One-thousandth of a vCPU or a CPU core and the preferred measurement unit of compute resources in Kubernetes (i.e. 128m = 12.8% of a CPU core and 2000m = 2 CPU cores)."},{"location":"Containers/#namespaces","title":"Namespaces","text":"<p>Namespaces wrap global system resources (mount points, network devices, hostnames) in an abstraction that makes it appear to processes within that namespace that they have their own isolated instance of that resource.</p> <p>Process IDs in the same namespace can have access to one another, whereas those in different namespaces cannot.  Spawning a process in a new namespace prevents it from seeing the host's context, so an interactive shell like <code>zsh</code> spawned in its own namespace will report its PID as <code>1</code>, even though the host will assign its own PID. </p> Node <p>A node or worker is any container host that accepts workloads from the master node.  Each node is equipped with a container runtime like Docker, which it uses to create and destroy containers according to instructions from the master server.</p> <p>Each node runs 2 processes:</p> <ul> <li>kubelet communicates with Kubernetes cluster services</li> <li>kube-proxy handles container network routing using iptables rules</li> </ul> PersistentVolume A PersistentVolume is a piece of storage in the cluster that has been provisioned using Storage Classes. PersistentVolumeClaim  A PersistentVolumeClaim requests either Disk or File storage of a particular StorageClass, access mode, and size. It is bound to a PersistentVolume once an available storage resource has been assigned to the pod requesting it. Pod <p>A pod is the most basic unit that K8s deals with, representing one or more tightly-coupled containers that should be controlled as a single application (typically one main container with subsidiary helper containers).  Every container should have only a single process, so if several processes need to communicate they should be implemented as separate containers in a pod.</p> <p>A pod's containers should:</p> <ul> <li>operate closely together</li> <li>share a lifecycle</li> <li>always be scheduled on the same node</li> </ul> Replica An instance of a Pod ReplicaSet ... Selector <p>A label selector provides a way to identify a set of objects and is the core grouping primitive supported by Kubernetes. It can be made of multiple requirements that are comma-separated, all of which must be satisfied.</p> <p>There are two types of selector:</p> <ul> <li>Equality-based  admits the operators =, !=, and ==.</li> <li>Set-based  admits the operators in, notin, and exists.</li> </ul> Equality-based selectorSet-based selector <pre><code>environment = production\ntier != frontend\n</code></pre> <pre><code>environment in (production, qa)\ntier notin (frontend, backend)\npartition\n!partition\n</code></pre> Service A Service is an abstraction over a logical set of Pods and a policy by which to access them, i.e. a microservice. Because Pods are mortal, the Service controller keeps track of Pod addresses and publishes this information to the consumers of Services, a function called service discovery. tmpfs RAM-backed file system used in Docker containers Volume <p>A volume is a special directory in the Docker host that can be mounted to the container that is used to achieve persistent storage.</p> <p>In Azure, a volume represents a way to store, retrieve, and persist data across pods and through the application lifecycle.  In the context of Azure, Kubernetes can use two types of data volume:</p> <ul> <li>Azure Disks using Azure Premium (SSDs) or Azure Standard (HDDs).</li> <li>Azure Files using a SMB 3.0 share backed by an Azure Storage account.</li> </ul> <p>In Kubernetes, Volumes are an abstraction of file systems accessible from within a Pod's containers.</p> <ul> <li>Network storage device, such as <code>gcePersistentVolume</code> </li> <li><code>emptyDir</code>, where the data is stored in RAM using Docker's tmpfs file system</li> <li><code>hostPath</code>, where the volume is located within the node's file system. Because pods are expected to be created and destroyed on any node (which may themselves be destroyed and recreated), hostPath volumes are discommended.</li> </ul> <p>Volumes are declared in .spec.volumes and mounted into containers in .spec.containers[*].volumeMounts.</p> emptyDirhostPathgcePersistentDisk <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: alpine\nspec:\nvolumes:\n- name: data\nemptyDir:\n\n\ncontainers:\n- name: alpine\nimage: alpine\nvolumeMounts:\n- mountPath: \"/data\"\nname: \"data\"\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: alpine\nspec:\nvolumes:\n- name: data\nhostPath:\npath: /var/data\n\ncontainers:\n- name: alpine\nimage: alpine\nvolumeMounts:\n- mountPath: \"/data\"\nname: \"data\"\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: alpine\nspec:\nvolumes:\n- name: data\ngcePersistentDisk:\npdName: my-disk\nfsType: ext4\ncontainers:\n- name: alpine\nimage: alpine\nvolumeMounts:\n- mountPath: \"/data\"\nname: \"data\"\n</code></pre> Worker :material-kubernetes see Node"},{"location":"Databases/","title":"Databases","text":"<ul> <li>Azure Data Studio</li> <li>Cosmos DB emulator</li> </ul>"},{"location":"Databases/#postgresql","title":"PostgreSQL","text":"<p>PostgreSQL configs postgresql.conf and pg_hba.conf are stored where the PostgreSQL database cluster was initialized with initdb. This directory can be initialized anywhere, but the default in Red Hat systems is /var/lib/pgsql/data.</p>"},{"location":"Databases/#tasks","title":"Tasks","text":""},{"location":"Databases/#setup-postgresql","title":"Setup PostgresQL","text":""},{"location":"Databases/#installation-and-setup","title":"Installation and setup","text":"<pre><code>```sh\n# Arch\npacman -S postgresql\n\n# On Arch, this step appears to be necessary before the postgresql service can be enabled.\n# initdb requires a directory to be explicitly specified using --pgdata or alternatively the PGDATA environment variable.\nsu - postgres -c 'initdb --pgdata /var/lib/postgres/data' # (1)\n\nsystemctl enable postgresql --now\n\n# Red Hat\ndnf install libpq-devel mariadb-devel postgresql postgresql-server\n\n# This command facilitates initialization of the database cluster, which defaults to /var/lib/pgsql/data, similar to using initdb.\npostgresql-setup --initdb # (1)\n\nsystemctl enable postgresql --now\n\n# Ubuntu\napt install libpq-dev\n```\n</code></pre> Resources <ul> <li>SQLBolt interactive tutorial</li> </ul> <pre><code>systemctl start postgresql\nsudo -u postgres psql\n</code></pre> Role setup<pre><code>CREATE ROLE username LOGIN INHERIT -- (1)\nCREATEDB -- (5)\nPASSWORD 'password'; -- (3)\nGRANT postgres TO username; -- (2)\nCREATE DATABASE username; -- (4)\nexit\n</code></pre> <ol> <li>Create a new user or \"role\". Like SSH, psql by default will use the currently logged-in username, which does not exist on a fresh installation.</li> <li>Grant group membership to the newly created user.</li> <li>Manually set a password for the newly created user. Single quotes are necessary here, as double quotes will cause an error. After role creation<pre><code>ALTER USER username WITH PASSWORD 'password' ;\n</code></pre></li> <li>The default database which is logged into is also named after the currently logged-in user.  Also the built-in command createdb can be used from the command-line.</li> <li>Database creation is a restricted operation granted by an attribute. After role creation<pre><code>ALTER USER username WITH CREATEDB;\n</code></pre></li> </ol>"},{"location":"Databases/#sql","title":"SQL","text":""},{"location":"Databases/#starships","title":"Starships","text":"<pre><code>CREATE TABLE starships (\nname text, registry text, crew integer\n);\n</code></pre> <pre><code>INSERT INTO starships (name, registry, crew) VALUES ('USS Enterprise', 'NCC-1701', 400); -- (1)\n</code></pre> <ol> <li>For some reason, a double-quote \" produces an error, and only single-quotes are accepted.</li> </ol>"},{"location":"Databases/#cosmos-db","title":"Cosmos DB","text":"..."},{"location":"Databases/#commands","title":"Commands","text":""},{"location":"Databases/#psql","title":"psql","text":"<p>Enter an interactive shell to control a PostgreSQL server.</p> Install<pre><code>dnf install postgresql\n</code></pre> <pre><code>psql -d database\n</code></pre> <p>The interactive shell allows SQL queries to be run as well as meta-commands prefixed with a backslash \\.</p> <pre><code>\\dt -- (1)\n</code></pre> <ol> <li>Display tables</li> </ol>"},{"location":"Databases/#sqlite3","title":"sqlite3","text":"<p>sqlite3 is an interactive frontend to the SQLite library.</p> <p>Meta-commands, prefixed by <code>.</code>, can be used to examine database files or perform administrative operations.</p> <pre><code>.databases  -- (1)\n.tables     -- (2)\n.show       -- (3)\n.exit       </code></pre> <ol> <li>List names and files of attached databases.</li> <li>List names of tables matching a given pattern.</li> <li>Show the current values for various settings.</li> </ol> <p>Files can be provided on invocation from the command-line or they can be provided after the .open meta-command. <pre><code>.open database.db\n</code></pre></p> <p>Without providing an argument on invocation, sqlite3 will open an in-memory database by default, which can also be explicitly specified with a meta-command. <pre><code>.open :memory:\n</code></pre></p>"},{"location":"Display/","title":"Display","text":""},{"location":"Display/#tasks","title":"Tasks","text":""},{"location":"Display/#commands","title":"Commands","text":""},{"location":"Display/#x11","title":"X11","text":"X Test X11 with the config file automatically generated after <code>Xorg -configure</code> <pre><code>X -config $HOME/xorg.conf.new\n</code></pre> xhost Enable access control to X server <pre><code>xhost -\n</code></pre> Disable access control to X server, allowing clients from any host to connect (not unsafe if you use a firewall that allows only SSH) <pre><code>xhost +\n</code></pre> Add <code>$HOST</code> to list of authorized clients for X server <pre><code>xhost +$HOST\n</code></pre> Remove <code>$HOST</code> from list of authorized clients for X server <pre><code>xhost -$HOST\n</code></pre> Add <code>$USER</code> to ACL <pre><code>xhost si:localuser:$USER\n</code></pre>"},{"location":"Display/#xmodmap","title":"xmodmap","text":"Replacing Caps Lock with Escape <pre><code>! Swap caps lock and escape\nremove Lock = Caps_Lock\nkeysym Escape = Caps_Lock\nkeysym Caps_Lock = Escape\nadd Lock = Caps_Lock\n</code></pre>"},{"location":"Display/#xorg","title":"Xorg","text":"Enable automatic configuration of X11 server <pre><code>Xorg -configure\n</code></pre>"},{"location":"Display/#xrandr","title":"xrandr","text":"Change resolution of DisplayPort1 to 1920x1080 <pre><code>xrandr --output DP1 --mode 1920x1080\n</code></pre> Disable VGA1 output <pre><code>xrandr --output VGA1 --off\n</code></pre> Display current state of the system <pre><code>xrandr -q  --query\n</code></pre>"},{"location":"Display/#xset","title":"xset","text":"Dynamically add fonts [Haeder: 307][Haeder] <pre><code>xset fp+ /usr/local/fonts\n</code></pre>"},{"location":"Files/","title":"Files","text":""},{"location":"Files/#glossary","title":"Glossary","text":""},{"location":"Files/#squashfs","title":"squashfs","text":"Squashfs is a compressed read-only filesystem for Linux using zlib compression for files, inodes, and directories."},{"location":"Files/#sgid","title":"SGID","text":"<p>When the set-group-ID bit for a directory is set, all files created therein are assigned to the directory's group and not to the file owner's default group.</p> <p>This is intended to facilitate file sharing. In this scenario, users are assigned to a group, and the group is assigned to shared directories with the SGID bit set.</p>"},{"location":"Files/#sticky-bit","title":"Sticky bit","text":"When the sticky bit is set on a directory, only root, the directory owner and the owner of a file can remove files in that directory."},{"location":"Files/#suid","title":"SUID","text":"The set-user-ID bit allows a file to be executed with the privileges of the file's owner."},{"location":"Files/#commands","title":"Commands","text":""},{"location":"Files/#chage","title":"chage","text":""},{"location":"Files/#chage_1","title":"chage","text":"Expire password in 30 days<pre><code>chage -E $(date -d +30days +%Y-%m-%d) $USER\n</code></pre>"},{"location":"Files/#chgrp","title":"chgrp","text":""},{"location":"Files/#chgrp_1","title":"chgrp","text":"<p>Change ownership of <code>$FILE</code> to <code>$USER</code> and <code>$GROUP</code></p> <pre><code>chgrp $USER:$GROUP $FILE\n</code></pre>"},{"location":"Files/#chmod","title":"chmod","text":""},{"location":"Files/#chmod_1","title":"chmod","text":"Add permissions Remove permissions <pre><code>chmod +t $FILE # Sticky bit\nchmod g+s file # SGID\nchmod u+s file # SUID\n</code></pre> <pre><code>chmod -t $FILE # Sticky bit\nchmod g-s file # SGID\nchmod u-s file # SUID\n</code></pre>"},{"location":"Files/#chown","title":"chown","text":""},{"location":"Files/#chown_1","title":"chown","text":"<p>Change a file or directory's ownership. </p> <p>To change the user and group owner of a file to <code>$USER</code> and <code>$GROUP</code>:</p> <pre><code>chown $USER:$GROUP $file\n</code></pre> <p>Recursively grant <code>$USER</code> ownership to <code>$PATH</code> <pre><code>chown -R $USER $PATH\n</code></pre></p> <p>Use a reference file to match the configuration of a particular file <pre><code>chown -vR --reference=. $PATH\n</code></pre></p> <p><code>--preserve-root</code> prevents changes to files in the root directory but only when used together with <code>--recursive</code></p> <pre><code>chown -cfR --preserve-root $USER </code></pre>"},{"location":"Files/#cp","title":"cp","text":""},{"location":"Files/#du","title":"du","text":""},{"location":"Files/#du_1","title":"du","text":"<p>du does not double-count hard-linked files, so it can be used to analyze deduplication in app distribution solutions like Flatpak.</p> <p>Here the second command will display a smaller value for the 21.08 version of the freedesktop Platform runtime, indicating that hard-linked files have not been double-counted. <pre><code>du -sh /var/lib/flatpak/runtime/org.freedesktop.Platform/x86_64/21.08\ndu -sh /var/lib/flatpak/runtime/org.freedesktop.Platform/x86_64/21.08 /var/lib/flatpak/runtime/org.freedesktop.Platform/x86_64/20.08\n</code></pre></p>"},{"location":"Files/#find","title":"find","text":""},{"location":"Files/#find_1","title":"find","text":"<p>Search for files in a directory hierarchy</p> Find all files owned by user<pre><code>find . -user $USER\n</code></pre> <p>-exec allows a command to be executed for every foudn file, which has to be terminated with an escaped semicolon, i.e. <code>\\;</code>.</p> Remove whitespace from filenames<pre><code>find . -type f -name \"* *\" -exec bash -c 'mv \"$0\" \"${0// /_}\"' {} \\;\n</code></pre> <p>Find recently modified files/folders</p> <p>There are 3 timestamps associated with files in Linux </p> <ul> <li>atime \"access time\": last time file was accessed by a command or application</li> <li>mtime \"modify time\": last time file's contents were modified</li> <li>ctime \"change time\": last time file's attribute was modified </li> </ul> <p>Numerical arguments can be specified in 3 ways:</p> <ul> <li><code>+n</code> greater than {n} days ago</li> <li><code>-n</code> less than {n} days ago</li> <li><code>n</code> exactly n days ago</li> </ul> <pre><code># Find only files that were modified more than 120 days ago\nfind . -type f -mtime +120 -ls\n\n# Modified less than 15 days ago \nfind . -type f -mtime -15 -ls\n\n# Modified exactly 10 days ago \nfind . -type f -mtime 10 -ls # Find files modified over the past day\nfind . -type f -newermt \"1 day ago\" -ls\nfind . -type f -newermt \"-24 hours\" -ls\nfind . -type f -newermt \"yesterday\" -ls\n\n# Find files created today\nfind . -type f -ctime -1 -ls </code></pre>"},{"location":"Files/#mv","title":"mv","text":""},{"location":"Files/#rename","title":"rename","text":""},{"location":"Files/#rename_1","title":"rename","text":"<p>Use regular expressions to rename multiple files</p> <pre><code># Renaming file.old to file.new\nrename 's/old/new/' this.old\n\n# Use globbing to rename all matching files\nrename 's/old/new/' *.old\nrename 's/report/review/' *\n\n# Change all uppercase letters to lowercase\nrename 'y/A-Z/a-z/' *\n</code></pre>"},{"location":"Files/#rsync","title":"rsync","text":""},{"location":"Files/#rsync_1","title":"rsync","text":"<p> <code>a</code> <code>b</code> <code> </code> <code> </code> <code>e</code> <code> </code> <code>g</code> <code> </code> <code> </code> <code> </code> <code> </code> <code>l</code> <code> </code> <code> </code> <code>o</code> <code>p</code> <code> </code> <code>r</code> <code> </code> <code>t</code> <code> </code> <code>v</code> <code> </code> <code> </code> <code> </code> <code>z</code> </p> <p>Copy $FILE locally  <pre><code>rsync -zvr $FILE $PATH\n</code></pre></p> <p>Copy $FILE to $PATH on remote $HOST <pre><code>rsync $FILE $HOST:$PATH\n</code></pre></p> <p>Copy $FILE from $HOST to local $PATH <pre><code>rsync $HOST:$FILE $PATH\n</code></pre></p> <p>Copy <code>$DIR</code> recursively <pre><code>rsync -zvr $DIR $PATH\nrsync -avz $DIR $PATH\n</code></pre></p> <p>Copy to remote systems over SSH <pre><code>rsync -zvre ssh $DIR $HOST:$REMOTEPATH\nrsync -avze ssh $DIR $HOST:$REMOTEPATH\n</code></pre></p> <p>Synchronize only specific file type <pre><code>rsync -zvre ssh --include '*.php' --exclude '*' $PATH\n</code></pre></p>"},{"location":"Files/#facl","title":"facl","text":""},{"location":"Files/#getfacl","title":"getfacl","text":""},{"location":"Files/#setfacl","title":"setfacl","text":""},{"location":"Files/#setfacl_1","title":"setfacl","text":"<p>The effect of ACLs can be illustrated with a web server. This command removes read access from a file which would otherwise be served by the Apache/httpd web server daemon. <pre><code>setfacl -m u:apache:- /var/www/html/index.html\n</code></pre></p> <p>This can be resolved by granting read to the apache service account (or removing the entry altogether) <pre><code>setfacl -m u:apache:r /var/www/html/index.html\nsetfacl -x u:apache /var/www/html/index.html\nsetfacl -b /var/www/html/index.html\n</code></pre></p>"},{"location":"Files/#attr","title":"attr","text":"<p>A family of commands exists to change file attributes on Linux file systems.</p>"},{"location":"Files/#lsattr","title":"lsattr","text":""},{"location":"Files/#chattr","title":"chattr","text":"Make file immutable<pre><code>chattr +i /etc/resolv.conf\n</code></pre>"},{"location":"Filters/","title":"Filters","text":""},{"location":"Filters/#awk","title":"awk","text":"<p>Awk programs are equivalent to sed \"instructions\" and can be defined inline or in a program file (also \"source files\").  If no input files are specified awk can accept input from standard input.</p> <pre><code>awk $OPTIONS $PROGRAM $INPUTFILES\nawk $OPTIONS -f $PROGRAMFILE $INPUTFILES\n</code></pre> <p>awk programs combine patterns and actions</p> <p>Patterns can be:</p> <ul> <li>regular expressions or fixed strings</li> <li>line numbers using builtin variable <code>NR</code></li> <li>predefined patterns <code>BEGIN</code> or <code>END</code>, whose actions are executed before and after processing any lines of the data file, respectively</li> </ul> <p>Convert \":\" to newlines in $PATH environment variable <pre><code>echo $PATH | awk 'BEGIN {RS=\":\"} {print}'\n</code></pre></p> <p>Print the first field of all files in the current directory, taking semicolon <code>;</code> as the field separator, outputting filename, line number, and first field of matches, with colon <code>:</code> between the filename and line number <pre><code>awk 'BEGIN {FS=\";\"} /enable/ {print FILENAME \":\" FNR,$1}' *\n</code></pre> search for string <code>MA</code> in all files, outputting filename, line, and line number for matches <pre><code>awk '/MA/ {OFS=\" \" print FILENAME OFS FNR OFS $0} *\n</code></pre> change field separator (<code>FS</code>) to a colon (<code>:</code>) and run <code>awkscr</code> <pre><code>awk -F: -f awkscr /etc/passwd\n</code></pre> flag also works for awk <pre><code>awk -f script files` `-f\n</code></pre> print the first field of each line in the input file <pre><code>awk '{ print $1 }' list\n</code></pre> equivalent to <code>grep MA *</code> (<code>{print}</code> is implied) <pre><code>awk '/MA/' * | awk '/MA/ {print}' *\n</code></pre> <code>-F</code> flag is followed by field separator <pre><code>awk -F, '/MA/ { print $1 }' list\n</code></pre> pipe output of <code>free</code> to <code>awk</code> to get free memory and total memory <pre><code>free -h | awk '/^Mem|/ {print $3 \"/\" $2}\n</code></pre> pipe output of <code>sensors</code> to <code>awk</code> to get CPU temperature <pre><code>sensors | awk '/^temp1/ {print $2}\n</code></pre> replace initial \"fake.\" with \"real;\" in file <code>fake_isbn</code> <pre><code>awk 'sub(^fake.,\"real;\")' fake_isbn\n</code></pre> print all lines <pre><code>awk '1 { print }' file\n</code></pre> remove file header <pre><code>awk 'NR&gt;1' file\n</code></pre> remove file header <pre><code>awk 'NR&gt;1 { print } file\n</code></pre> print lines in a range <pre><code>awk 'NR&gt;1 &amp;&amp; NR &lt; 4' file\n</code></pre> remove whitespace-only lines <pre><code>awk 'NF' file\n</code></pre> remove all blank lines <pre><code>awk '1' RS='' file\n</code></pre> extract fields <pre><code>awk '{ print $1, $3}' FS=, OFS=, file\n</code></pre> perform column-wise calculations <pre><code>awk '{ SUM=SUM+$1 } END { print SUM }' FS=, OFS=, file\n</code></pre> count the number of nonempty lines <pre><code>awk '/./ { COUNT+=1 } END { print COUNT }' file\n</code></pre> count the number of nonempty lines <pre><code>awk 'NF { COUNT+=1 } END { print COUNT }' file\n</code></pre> count the number of nonempty lines <pre><code>awk '+$1 { COUNT+=1 } END { print COUNT }' file\n</code></pre> Arrays <pre><code>awk '+$1 { CREDITS[$3]+=$1 } END { for (NAME in CREDITS) print NAME, CREDITS[NAME] }' FS=, file\n</code></pre> Identify duplicate lines <pre><code>awk 'a[$0]++' file\n</code></pre> Remove duplicate lines <pre><code>awk '!a[$0]++' file\n</code></pre> Remove multiple spaces <pre><code>awk '$1=$1' file\n</code></pre> Join lines <pre><code>awk '{ print $3 }' FS=, ORS=' ' file; echo\n</code></pre> <pre><code>awk '+$1 { SUM+=$1; NUM+=1 } END { printf(\"AVG=%f\",SUM/NUM); }' FS=, file` | format </code></pre> <pre><code>awk '+$1 { SUM+=$1; NUM+=1 } END { printf(\"AVG=%6.1f\",SUM/NUM); }' FS=, file\n</code></pre> Convert to uppercase  <pre><code>awk '$3 { print toupper($0); }' file\n</code></pre> Change part of a string <pre><code>awk '{ $3 = toupper(substr($3,1,1)) substr($3,2) } $3' FS=, OFS=, file\n</code></pre> Split the second field (\"EXPDATE\") by spaces, storing the result into the array DATE; then print credits ($1) and username ($3) as well as the month (DATE[2]) and year (DATE[3])  <pre><code>awk '+$1 { split($2, DATE, \" \"); print $1,$3, DATE[2], DATE[3] }' FS=, OFS=, file\n</code></pre> <pre><code>awk '+$1 { split($4, GRP, \":\"); print $3, GRP[1], GRP[2] }' FS=, file\n</code></pre> <pre><code>awk '+$1 { split($4, GRP, /:+/); print $3, GRP[1], GRP[2] }' FS=, file\n</code></pre> Search and replace with comma  <pre><code>awk '+$1 { gsub(/ +/, \"-\", $2); print }' FS=, file\n</code></pre> Adding date  <pre><code>awk 'BEGIN { printf(\"UPDATED: \"); system(\"date\") } /^UPDATED:/ { next } 1' file\n</code></pre> Modify a field externally  <pre><code>awk '+$1 { CMD | getline $5; close(CMD); print }' CMD=\"uuid -v4\" FS=, OFS=, file\n</code></pre> Invoke dynamically generated command <pre><code>awk '+$1 { cmd = sprintf(FMT, $2); cmd | getline $2; close(cmd); print }' FMT='date -I -d \"%s\"'  FS=, file\n</code></pre> Join data <pre><code>awk '+$1 { CMD | getline $5; print }' CMD='od -vAn -w4 -t x /dev/urandom' FS=, file\n</code></pre> Add up all first records to {sum}, then print that number out at the end <pre><code>awk '{sum += $1} END {print sum}' file\n</code></pre></p>"},{"location":"Filters/#grep","title":"grep","text":"<pre><code># Recursive search\ngrep -R $TEXT $DIRECTORY\n</code></pre>"},{"location":"Filters/#head","title":"head","text":"<pre><code># Print first 8 characters\nhead -c8 $FILE\n</code></pre>"},{"location":"Filters/#paste","title":"paste","text":"<pre><code># Make a .csv file from two lists\npaste -d ',' file1 file2\n\n# Transpose rows\npaste -s file1 file2\n</code></pre>"},{"location":"Filters/#sed","title":"sed","text":"<p>sed (\"Stream-oriented editor\") is typically used for applying repetitive edits across all lines of multiple files.  In particular it is, alongside awk one of the two primary commands which accept regular expressions in Unix systems. </p> <pre><code># Sed instructions can be defined inline or in a command file (i.e. script).\nsed $OPTIONS $INSTRUCTION $FILE\nsed $OPTIONS -f $SCRIPT $FILE\n</code></pre> <pre><code># Suppress automatic printing of pattern space\nsed -n # --quiet , --silent\n\n# Edit the file in-place, but save a backup copy of the original with **$SUFFIX** appended to the filename.\nsed -i=$SUFFIX\n</code></pre> <p>Sed instructions are made of two components: </p> <ul> <li>Addresses (i.e. patterns): zero, one, or two addresses can precede a procedure. In the absence of an address, the procedure is executed over every line of input. With one address, the procedure will be executed over every line of input that matches.</li> <li>Procedures (i.e. actions).</li> </ul> <p>Addressing can be done in one of two ways:</p> <ul> <li>Line addressing, specifying line numbers separated by a comma (e.g. <code>3,7p</code>)</li> <li>Context addressing, using a regular expression enclosed by forward slashes (e.g. <code>/From:/p</code>)</li> </ul> Line addressing<pre><code># Display a range of lines: without **-n** to suppress automatic printing of pattern space, each line will be printed twice\nsed -n '1,2p' $FILE\n\n# Do the same thing, this time by automatically printing all pattern space but suppressing from the third line on. $ refers to the last line\nsed '3,$d' $FILE\n\n# Prepending ! to the procedure reverses the sense of the command\nsed -n '3,$!p' $FILE\n\n# Replace functionality of head command by displaying the first 10 lines, then quitting\nsed 10q $FILE\n\n# Use -e to precede multiple instructions\nsed -n -e '1,2p' -e '7,9p' -e '$p' $FILE\n\n# Delete only the second line while printing all others\nsed '2d' $FILE\n\n# Delete a range of lines: from the 2nd through the 3rd\nsed '2,3d' $FILE\n</code></pre> Context addressing<pre><code># Delete a range of lines, from the first occurrence of 'second' to the line with the first occurrence of 'fourth'\nsed '/second/,/fourth/d' myfile\n\n# Suppress any line with 'test' in it\nsed '/test/d' myfile\n\n# Equivalent to grep MA *\nsed -n '/MA/p' *\n\n# Replace the first instance of pipe with colon and display the first two lines.\nsed 's/|/:/ emp.lst' | head -2\n\n# Substitute HTML tags:\nsed 's/&lt;I&gt;/&lt;EM&gt;/g'\n\n# Replace \"director\" with \"executive director\"\nsed 's/director/executive director/' emp.lst\nsed 's/director/executive &amp;/' emp.lst\nsed '/director/s//executive &amp;/' emp.lst\n\n# Replace angle brackets with their HTML codes, piped in from a heredoc:\nsed -e 's/&lt;/\\&amp;lt;/g' -e 's/&gt;/\\&amp;gt;/g' &lt;&lt; EOF\n</code></pre> Piping sed statements together<pre><code># Take lines beginning with \"fake\" and remove all instances of \"fake.\", piping them... remove all parentheses with content and count lines of output (results)\nsed -n '/^fake/s/fake\\.//p' * | sed -nr 's/\\(.*\\)//p' | wc -l\n\n# Take lines of all files in CWD beginning with \"fake\" and remove all instances of string \"fake.\" Then remove all parentheses with any content within them and print only the top 10 lines\nsed -ne '/^fake/p' * | sed -n 's/fake\\.//p' | sed -nr 's/\\(.*\\)//p' | sed 11q\n\n# Count the number of pipes replaced by piping output to `cmp`, which will use the `-l` option to output byte numbers of differing values, then counting the lines of output (YUG:456)\nsed 's/|/:/g' emp.lst | cmp -l - emp.lst | wc -l\n\n# Display the top 10 processes by memory or cpu usage.\nps axch -o cmd,%mem --sort=-%mem | sed 11q\nps axch -o cmd:15,%cpu --sort=-%cpu | sed 11q\n</code></pre>"},{"location":"Filters/#tail","title":"tail","text":"<pre><code># Output last lines beginning at 30th line from the start\ntail -n=+30 # --lines\n</code></pre>"},{"location":"Filters/#tr","title":"tr","text":"<pre><code># Change the case of a string ]\ntr [:upper:] [:lower:]\n\n# Remove a character or set of characters from a string or line of output\ntr -d \"text\"\n</code></pre>"},{"location":"Filters/#watch","title":"watch","text":"<pre><code># Check memory usage in megabytes every 5 seconds\nwatch -n 5 free -m\n</code></pre>"},{"location":"GRUB/","title":"GRUB2","text":"<p>Manual</p> <p>The GNU GRUB Manual is available here.</p> <p>GNU GRUB provides a shell environment with limited capabilities.</p> <pre><code>grub&gt;\n</code></pre> <p>Kernel command line parameters passed in on boot can be queried during runtime: <pre><code>cat /proc/cmdline\n</code></pre></p>"},{"location":"GRUB/#tasks","title":"Tasks","text":""},{"location":"GRUB/#loading-linux","title":"Loading linux","text":"<pre><code>grub&gt; set prefix=(hd0,1)/boot/grub\ngrub&gt; set root=(hd0,1)\ngrub&gt; insmod normal\ngrub&gt; normal\n</code></pre> <p>The most common task done at the GRUB command-line is simply starting linux. The prefix and root environment variables are set before loading and running the normal module.</p> <ul> <li>prefix is an absolute filename (UNIX-style path but including the device name in parentheses) indicating where GRUB modules are found</li> <li>root specifies a device for files which do not specify one</li> <li>insmod inserts a dynamic GRUB module</li> <li>normal enters normal mode and displays the GRUB menu</li> </ul> <p>Running ls  displays devices or files using the GRUB device syntax.</p> <p>Here, only the USB drive is available to the system. When additional drives are available, their</p> <pre><code>grub&gt; ls\n(proc) (hd0) (hd0,msdos1)\n</code></pre> <p>Invoking the set command without arguments will display all environment variables. Here the prefix and root variables are set before inserting and loading the normal module: <pre><code>grub&gt; set prefix=(hd0,msdos1)/boot/grub\ngrub&gt; set root=(hd0,msdos1)\ngrub&gt; insmod normal\ngrub&gt; normal\n</code></pre></p>"},{"location":"GRUB/#resetting-root-password","title":"Resetting root password","text":"Append rd.break to the list of command-line parameters to GRUB. Once the shell is ready, run the following commands <pre><code>mount -o remount,rw /sysroot\nchroot /sysroot\npasswd root\n</code></pre>"},{"location":"GRUB/#text-mode-installation","title":"Text-mode installation","text":"RHEL can be installed from the console by providing inst.text as a kernel parameter on boot by pressing ++Tab++ on the GRUB splash screen."},{"location":"GRUB/#grub-rescue-prompt","title":"GRUB rescue prompt","text":"<p>During GRUB's normal startup procedure, the prefix and root environment variables are first set before loading the normal module. When the normal module has failed to load because the GRUB folder or its contents are missing or corrupted, it displays a rescue shell <pre><code>grub rescue&gt;\n</code></pre></p> <pre><code>grub rescue&gt; set prefix=(hd0,1)/boot/grub\ngrub rescue&gt; set root=(hd0,1)\ngrub rescue&gt; insmod normal\ngrub rescue&gt; normal\n</code></pre> <p>After booting the system, GRUB should be updated and reinstalled:</p> <p>Update GRUB config file <pre><code>update-grub\n</code></pre> Reinstall GRUB <pre><code>grub-install /dev/sdx\n</code></pre></p>"},{"location":"GRUB/#commands","title":"Commands","text":""},{"location":"GRUB/#grub-mkconfig","title":"grub-mkconfig","text":"Generate GRUB configuration <pre><code>grub-mkconfig -o /boot/grub/grub.cfg\n</code></pre>"},{"location":"GRUB/#grub2-mkconfig","title":"grub2-mkconfig","text":"<p>grub2-mkconfig is used to create a GRUB2 config file from the settings defined in /etc/default/grub</p> <pre><code>grub2-mkconfig -o=/boot/grub2/grub.cfg\n</code></pre>"},{"location":"GRUB/#grub2-editenv","title":"grub2-editenv","text":"<p>Disable the Nouveau display driver while installing the proprietary Nvidia display driver on Fedora </p> <pre><code>grub2-editenv - set \"$(grub2-editenv - list | grep kernelopts) nouveau.modeset=0\"\n</code></pre>"},{"location":"GRUB/#update-grub","title":"update-grub","text":"Update GRUB config file <pre><code>update-grub\n</code></pre>"},{"location":"IAM/","title":"Users","text":""},{"location":"IAM/#tasks","title":"Tasks","text":""},{"location":"IAM/#user-management","title":"User management","text":"Lock user<pre><code>usermod -L $USER # --lock\npasswd -l $USER  # --lock\n</code></pre> Unlock user<pre><code>usermod -U $USER # --unlock\npasswd -u $USER  # --unlock\n</code></pre>"},{"location":"IAM/#groups","title":"Groups","text":"Display groups of effective user<pre><code>id -Gn\ngetent group | grep $(whoami) -\n</code></pre>"},{"location":"IAM/#commands","title":"Commands","text":""},{"location":"IAM/#chage","title":"chage","text":"Expire password in 30 days<pre><code>chage -E $(date -d +30days +%Y-%m-%d) $USER\n</code></pre>"},{"location":"IAM/#getent","title":"getent","text":"<p>Get entries from the passwd file <pre><code>getent passwd bob\n</code></pre></p> <pre><code>getent group dba_admins\n</code></pre>"},{"location":"IAM/#lastb","title":"lastb","text":"Display failed logins for user<pre><code>lastb $USER\n</code></pre>"},{"location":"IAM/#sudo","title":"sudo","text":"Edit a file with elevated privileges<pre><code>sudo -e /etc/ssh/sshd_config\n</code></pre> <p>The /etc/sudoers file (or files placed under /etc/sudoers.d/) contains user specifications that define commands that users may execute.</p> <pre><code>$USER $HOST = ($RUNAS) $CMD\n</code></pre> <ul> <li>$USER: usernames, UIDs, group names when prefixed with % i.e. %wheel, or GIDs when prefixed with %#</li> <li>$HOST: hostnames, IP addresses, or a CIDR range (i.e. 192.0.2.0/24)</li> <li>$RUNAS: optional clause that controls the user or group sudo will run the command as. If a username is specified, sudo will not accept a -g argument when runing sudo. </li> <li>$CMD: full path to an executable, or a comma-delimited list of commands.</li> </ul> <p>Any of these elements can be replaced with the keyword ALL.</p> Ansible service account<pre><code>ansible ALL=(ALL) NOPASSWD: ALL\n</code></pre> Allow user to run only the mkdir command<pre><code>user ALL=/bin/mkdir\n</code></pre> Allow user to run all commands without authenticating<pre><code>user ALL=(ALL) NOPASSWD: ALL\n</code></pre> <p>Change timeout to 10 minutes <pre><code>Defaults timestamp_timeout=10\n</code></pre></p> <p>Change timeout to 10 minutes only for user <code>linuxize</code> <pre><code>Defaults:linuxize timestamp_timeout=10\n</code></pre></p>"},{"location":"IAM/#gpasswd","title":"gpasswd","text":"<p>Administer /etc/group and /etc/gshadow </p> <p>Add user to group<pre><code>gpasswd -a $USER $GROUP\n</code></pre> Add user as admin of group<pre><code>gpasswd -A $USER $GROUP\n</code></pre> Remove user from group<pre><code>gpasswd -d $USER $GROUP\n</code></pre></p>"},{"location":"IAM/#groupadd","title":"groupadd","text":""},{"location":"IAM/#groupdel","title":"groupdel","text":""},{"location":"IAM/#groupmod","title":"groupmod","text":""},{"location":"IAM/#useradd","title":"useradd","text":"Add user<pre><code>useradd $USER               \\\n-m                  \\ # Create home directory\n-d $PATH            \\ # Specify home directory\n-s /bin/bash        \\ # Default shell\n-c $FULLNAME        \\ # Note full name in comment\n-G $GROUP1 $GROUP2  \\ # Add groups        \n-u $UID             \\ # Specify user ID\n-e $DATE            \\ # Specify expiration date (YYYY-MM-DD)\n-r                  \\ # System user\n</code></pre> <p>Useradd's config is at /etc/default/useradd but it also inherits settings from /etc/login.defs.</p> Example config<pre><code># useradd defaults file for ArchLinux\n# original changes by TomK\nGROUP=users\nHOME=/home\nINACTIVE=-1\nEXPIRE=\nSHELL=/bin/bash\nSKEL=/etc/skel\nCREATE_MAIL_SPOOL=no\n</code></pre> <p>These settings can be displayed with: <pre><code>useradd -D\n</code></pre></p>"},{"location":"IAM/#userdel","title":"userdel","text":"Delete an existing user account as well as the user's home directory <pre><code>userdel -r $USER\n</code></pre>"},{"location":"IAM/#usermod","title":"usermod","text":""},{"location":"Kernel/","title":"Kernel","text":""},{"location":"Kernel/#commands","title":"Commands","text":""},{"location":"Kernel/#sysctl","title":"sysctl","text":"<p>View and configure kernel parameters at runtime. Kernel parameters are tunable values that can be adjusted during runtime.</p> <p>Kernel parameters can be delimited with dots or slashes</p> <pre><code>sysctl kernel.hostname\nsysctl kernel/hostname\n\n# Suppress the name of the key\nsysctl -n kernel.hostname # --values\n</code></pre> <p>Kernel parameters are set persistently by defining values in /etc/sysctl.conf or other .conf files placed in /etc/sysctl.d/.</p> /etc/sysctl.conf<pre><code>net.ipv4.ip_forward=1\n</code></pre> <p>These values are then loaded into memory ad-hoc with: <pre><code>sysctl -p # --load\n</code></pre></p> <p>The runtime can be manipulated directly from the command-line with a different flag\" <pre><code>sysctl -w net.ipv4.ip_forward=1\n</code></pre></p> <p>Alternatively, values can be echoed to the virtual filesystem exposed at /proc/sys <pre><code>echo 1 &gt; /proc/sys/net/ipv4/ip_forward\n</code></pre></p> <p>Disable IPv6 <pre><code>net.ipv6.conf.all.disable_ipv6=1\nnet.ipv6.conf.default.disable_ipv6=1\n</code></pre></p> <p>Alternatively, kernel parameters can be viewed or even edited through the virtual filesystem mounted at /proc/sys</p>"},{"location":"Kernel/#uname","title":"uname","text":"<pre><code># Display operating system \"Linux\"\nuname\n\n# Kernel architecture\nuname -m\n\n# Kernel release version\nuname -r\n\nuname -a\n</code></pre>"},{"location":"Kernel/#modules","title":"Modules","text":"<p>A family of commands exists to manipulate Linux modules, including:</p> <ul> <li>Floppy, which can be used safely as a stand-in for any module while learning the commands</li> <li>KVM</li> <li>Wireguard</li> </ul> <p>Display currently loaded modules. Output in three columns:</p> <ol> <li>Module name</li> <li>Module size (bytes)</li> <li>Processes, filesystems, or other modules using the module</li> </ol>"},{"location":"Kernel/#rmmod","title":"rmmod","text":"<pre><code>rmmod floppy # (1)\n</code></pre> <ol> <li>Equivalent to <pre><code>modprobe -r floppy\n</code></pre></li> </ol>"},{"location":"Network/","title":"Networking","text":"<p>The Linux kernel supports several packet-filtering mechanisms.</p> <ul> <li>Netfilter using the venerable iptables utility</li> <li>nftables subsystem, introduced with kernel 3.13 (2014), had been commonly assumed to eventually take the place of iptables. Firewall rules are implemented in an in-kernel VM.</li> <li>bpfilter </li> </ul> <p>Netfilter is a software firewall and packet filtering framework introduced with Linux 2.4.0 (2001) and controlled by the iptables command.</p> <p>Netfilter rules are stored in tables and in chains, and tables are associated with various chains.</p> <p>By convention, table names are specified in lowercase and chain names in uppercase. Every packet starts at the top of a chain and is matched rule by rule. When a match is found the specified action, called the target, is triggered: i.e. \"DROP\" or \"ACCEPT\".</p> Tables INPUT OUTPUT FORWARD PREROUTING POSTROUTING filter \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f nat \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f mangle \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f raw \u2714\ufe0f \u2714\ufe0f <p>There are five builtin netfilter chains, though user-defined chains are also possible:</p> <ul> <li>INPUT used for filtering incoming packets where the host itself is the destination packet.</li> <li>OUTPUT for outgoing packets, where the host is the source of the packet.</li> <li>FORWARD for filtering routed packets, where the host is router.</li> <li>PREROUTING used for DNAT or port forwarding</li> <li>POSTROUTING used for SNAT</li> </ul> <p>Netfilter tables:</p> <ul> <li>filter default</li> <li>nat for SNAT and DNAT</li> <li>mangle for packet alteration</li> <li>raw used only to mark packets that should not be handled by the connection tracking system using the NOTRACK target</li> </ul>"},{"location":"Network/#networkmanager","title":"NetworkManager","text":"<p>NetworkManager provides a high-level interface for the configuration of network interfaces. It was developed by Red Hat and released in late 2004.</p> <p>The config file format native to NetworkManager is the ini-format keyfile stored in /etc/NetworkManager/system-connections. These files define network interfaces, or connection profiles in NetworkManager's terminology.</p>"},{"location":"Network/#nmcli","title":"nmcli","text":"<p>Control NetworkManager and report network status</p> <p>Display devices and statuses <pre><code>nmcli device status\n</code></pre></p> <p>Display information on interfaces as well as status Including other network connections not managed by network manager (\"unmanaged\") or not connected (\"unavailable\")  <pre><code>nmcli dev status\n</code></pre></p> <p>Display what connections are enabled  <pre><code>nmcli general status\n</code></pre></p> <p>Display UUIDs associated with network connections  <pre><code>nmcli connection show --active\n</code></pre></p> <p>Display much more information on network devices <pre><code>nmcli device show\n</code></pre></p> <p>Configure settings for network interface {ens01} via interactive shell <pre><code>nmcli connection edit ens01\n</code></pre></p> <p>List all connections NetworkManager has <pre><code>nmcli connection show\n</code></pre></p> <p>Show settings for network interface {ens01} <pre><code>nmcli device show ens01\n</code></pre></p> <p>Show status for all devices <pre><code>nmcli device status\n</code></pre></p> <p>Display currently configured hostname <pre><code>nmcli general hostname\n</code></pre></p> <p>Set hostname to {hostname} <pre><code>nmcli general hostname hostname\n</code></pre></p> <p>Show overall status of NetworkManager <pre><code>nmcli general status\n</code></pre></p> Migrate a connection profile<pre><code>nmcli connection migrate eth0\n</code></pre>"},{"location":"Network/#nmtui","title":"nmtui","text":"<pre><code>dnf install NetworkManager-tui\n</code></pre> nmtui is a curses-based TUI for control of NetworkManager."},{"location":"Network/#netplan","title":"Netplan","text":"<p>netplan is a utility for network configuration using YAML files that is the default network management tool used by recent versions of Ubuntu (since Ubuntu 17.10). Netplan is used as the default network management tool (previously ifconfig and its config at /etc/network/interfaces was used).</p> <p>Netplan supports two renderers or backends: NetworkManager and networkd.</p> <p>Netplan configs are YAML format and placed in /etc/netplan. Ubuntu installations usually come with a single config in this location named 01-network-manage-all-yaml, but many configs can be created in subdirectories. These are processed in lexicographical order regardless of subdirectory (unless there are multiple files with the same name). If a boolean or scalar parameter is defined in more than one config, the last value is assumed. Values that are sequences are concatenated.</p> Default config<pre><code># Let NetworkManager manage all devices on this system\nnetwork:\nversion: 2\nrenderer: NetworkManager # (1)\n</code></pre> <ol> <li>This may require the python3-networkmanager package to be installed first.</li> </ol> Static IP configuration<pre><code>network:\nversion: 2\nethernets:\neth0:\naddresses:\n- 192.168.2.100/24\ngateway4: 192.168.2.1\nnameservers:\naddresses:\n- 192.168.1.1\nsearch: []\n</code></pre>"},{"location":"Network/#netplan_1","title":"netplan","text":"<p>The netplan utility can be used to load the on-disk configuration.</p> Reload configuration temporarily<pre><code>netplan try\n</code></pre>"},{"location":"Network/#tasks","title":"Tasks","text":""},{"location":"Network/#bridge","title":"Bridge","text":"<p>A bridge is used to unite two or more network segments, typically used to establish communication channels between VMs, containers, and the host.</p> <p>Unlike the virtual bridge that Windows uses for WSL2 distributions, the bridge in Linux is strictly L2. That is, VMs connecting to the bridge are assigned IPs by the same DHCP server (i.e. the router) in the same subnet as that of the physical hosts. In Windows, the virtual bridge assigns an internal IP in a private range (usually 172.16.0.0/12), and connectivity to the host or the Internet has to be accomplished via NAT.</p> <pre><code>ip link add virbr0 type bridge # (1)\nip link set virbr0 up\n</code></pre> <ol> <li>The link can be deleted thus: <pre><code>ip link delete virbr0\n</code></pre></li> </ol> <p>Adding an interface to the bridge is done by setting its master. <pre><code>ip link set enp2s0f0 master virbr0 # (1)\n</code></pre></p> <ol> <li>This can be undone as follows: <pre><code>ip link set enp2s0f0 nomaster\n</code></pre></li> </ol> <p>The iproute2 bridge utility can be used to verify the command has taken effect: <pre><code>bridge link\n</code></pre></p> <p>This may interrupt network connectivity. In this case, the IP address must be removed from the linked interface and assigned to the bridge <pre><code>ip address delete 192.168.1.3 dev enp2s0f0\nip address add 192.168.1.3 dev virbr0\n</code></pre></p> <p>The default route in the routing table must also be amended. Note this is not the IP address of the interface but rather that of the gateway. Also note that this gateway must already have its own network segment defined. That is, in order for a default route to be defined at least one static route must also be defined, which is the gateway's own local subnet. <pre><code>ip route delete default\nip route add default via 192.168.1.1\n</code></pre></p>"},{"location":"Network/#downloading-files","title":"Downloading files","text":"<p>Wget defaults to file operations in a way that is more natural for downloading.</p> <pre><code>wget $url\n</code></pre> <p>Curl depends on piping and defaults to STDOUT in a manner similar to cat.</p> <pre><code>curl -O $url </code></pre>"},{"location":"Network/#wireguard-tunnel","title":"Wireguard tunnel","text":"Red Hat Ubuntu <pre><code>dnf install wireguard-tools\n</code></pre> <pre><code>apt install wireguard\n</code></pre> <p>Successful installation can be confirmed by running the following, which should produce no output (and no error) on success. <pre><code>sudo modprobe wireguard\n</code></pre></p> <p>The first step in creating a Wireguard tunnel is to create a private key on each endpoint of the tunnel. The genkey subcommand creates a 44-character base64 encoded key ending in <code>=</code> which can be redirected to a file. If the file will be world-readable, the utility will ask you to change the umask. <pre><code>wg genkey # \u2593\u2593\u2591\u2591\u2591\u2593\u2591\u2592\u2593\u2592\u2593\u2592\u2591\u2593\u2591\u2592\u2591\u2593\u2591\u2592\u2592\u2591\u2593\u2591\u2593\u2591\u2592\u2591\u2592\u2593\u2592\u2592\u2592\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2592\u2592\u2593\u2593=\n</code></pre></p> <p>The public key can be generated by piping the private key from STDIN or from the file. <pre><code>wg pubkey private &gt; public\nwg genkey | tee private | wg pubkey &gt; public\n</code></pre></p> <p>Then a Wireguard interface is created, typically named wg0, using network management utilities. <pre><code>ip link add wg0 type wireguard\n</code></pre></p> <p>An IP address is assigned to that interface, to be used within the tunnel: <pre><code>ip addr add 10.0.0.1/24 dev wg0\n</code></pre></p> <p>Now the private key is associated with the interface: <pre><code>wg set wg0 private-key ~/.config/wireguard/private\n</code></pre></p> <p>Finally, the interface is brought up: <pre><code>ip link set wg0 up\n</code></pre></p> <p>The public key of the peer is now associated with the Wireguard interface and the public IP and port of the other endpoint are specified. <pre><code>wg set wg0 peer $PUBKEY allowed-ips 10.0.0.2/32 endpoint $IP\n</code></pre></p> <p>The tunnel is dismantled by removing the interface. <pre><code>ip link delete wg0\n</code></pre></p> <p>Alternatively, many of these steps can be consolidated into creating a config for the Wireguard interface at /etc/wireguard/wg0.conf with the following contents: <pre><code>[Interface]\nPrivateKey = \u2593\u2593\u2591\u2591\u2591\u2593\u2591\u2592\u2593\u2592\u2593\u2592\u2591\u2593\u2591\u2592\u2591\u2593\u2591\u2592\u2592\u2591\u2593\u2591\u2593\u2591\u2592\u2591\u2592\u2593\u2592\u2592\u2592\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2592\u2592\u2593\u2593=\nAddress = 10.0.0.1/24\nPostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\nPostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE\nListenPort = 51820\n\n[Peer]\nPublicKey = \u2593\u2592\u2593\u2592\u2593\u2593\u2591\u2593\u2592\u2592\u2591\u2591\u2592\u2592\u2592\u2593\u2592\u2591\u2592\u2593\u2592\u2591\u2592\u2592\u2593\u2592\u2591\u2592\u2591\u2591\u2591\u2591\u2592\u2591\u2592\u2592\u2592\u2591\u2591\u2592\u2592\u2593\u2591\u2592=\nAllowedIps = 10.0.0.2/32\nEndpoint = 123.45.67.89:51820\n</code></pre></p> <p>Then to bring it up quickly: <pre><code>wg-quick up wg0\n</code></pre></p> <p>The same utility can be used to teardown the tunnel <pre><code>wg-quick down wg0\n</code></pre></p>"},{"location":"Network/#static-ip","title":"Static IP","text":"<p>Static IP configuration varies by the network management toolset and backend presenton a system.  Ubuntu systems use Netplan whereas other distributions most commonly use Network Manager.</p> NetplanNetwork Manager <pre><code>network:\nversion: 2\nethernets:\neth0:\naddresses:\n- 192.168.2.100/24\ngateway4: 192.168.2.1\nnameservers:\naddresses:\n- 192.168.1.1\nsearch: []\n</code></pre> <pre><code>[connection]\nid=Ethernet\nuuid=abcdef01-2345-6789-0abc-def012345678\ntype=ethernet\ninterface-name=eth0\n\n[ethernet]\n\n[ipv4]\naddress1=192.168.2.100/24,192.168.2.1\ndns=10.40.7.2\nmethod=manual\n\n[ipv6]\naddr-gen-mode=stable-privacy\nmethod=auto\n</code></pre> <p>Setting a static IP address on Red Hat distributions could involve multiple methods:</p> <ul> <li>nmcli commands</li> <li>NetworkManager keyfiles</li> <li>ifcfg files (prior to distributions downstream to Fedora 36)</li> </ul>"},{"location":"Network/#commands","title":"Commands","text":""},{"location":"Network/#curl","title":"curl","text":"<p>Accept a self-signed certificate by skipping verification <pre><code>curl -k https://192.168.1.10\n</code></pre></p> <p>Use the  <code>dict</code> network protocol  to retrieve the definition of a word. ref <pre><code>curl dict://dict.org/d:&lt;word&gt;\n</code></pre> Sending a POST method to a FastAPI app (src) <pre><code>curl -X POST \"http://127.0.0.1:8000/purchase/item/\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"name\\\":\\\"sample item\\\",\\\"info\\\":\\\"This is info for the item\\\",\\\"price\\\":40,\\\"qty\\\":2}\"\n</code></pre></p>"},{"location":"Network/#firewall-cmd","title":"firewall-cmd","text":"<p>Frontend to Netfilter in Red Hat distributions.</p> <pre><code>firewall-cmd --state # \"running\"\n</code></pre> <p>Firewalld has a runtime configuration and a saved, persistent configuration. Only the runtime configuration will be consulted for any command, unless the persistent configuration is specified with --permanent.</p> <p>The runtime configuration can be saved with this command, which obviates the need to execute every change twice. <pre><code>firewall-cmd --runtime-to-permanent\n</code></pre></p> <p>Alternatively, the persistent configuration can be loaded into memory: <pre><code>firewall-cmd --reload\n</code></pre></p> Display firewall rules<pre><code>firewall-cmd --list-all --permanent\n</code></pre> <p>Firewalld uses zones to define the level of trust for network connections. A connection can only be part of one zone, but a zone can be used for many network connections. Builtin zones have XML-format configs found in /usr/lib/firewalld/zones. <pre><code>firewall-cmd --get-active-zones     # Display active zones along with interfaces\nfirewall-cmd --info-zone=public     # Inspect zone\nfirewall-cmd --new-zone=testlab     # Create new zone\n</code></pre></p> <p>Firewalld rules are generally managed through builtin services. These bundle network settings together for well-known applications like SSH, etc. Builtin services are also XML-format configs found in /usr/lib/firewalld/services.</p> Services<pre><code>firewall-cmd --list-services\nfirewall-cmd --add-service=http\nfirewall-cmd --remove-service=http\n</code></pre> <p>Firewalld's config file is at /etc/firewalld/firewalld.conf /etc/firewalld/firewalld.conf<pre><code>AllowZoneDrifting=no\n</code></pre></p> <p>Since RHEL 8, firewalld's backend has been changed to nftables. /etc/firewalld/firewalld.conf<pre><code>FirewallBackend=nftables\n</code></pre></p>"},{"location":"Network/#ip","title":"ip","text":"ip address<pre><code>ip address add 192.168.2.2 dev eth0\n</code></pre> ip route<pre><code># Add static route (this is sometimes done automatically by the system after adding an address)\nip route add 192.168.2.0/24 dev eth0\n\n# Add default route\nip route add default via 192.168.2.1 dev eth0\n</code></pre> ip link<pre><code># Create new links\nip link add virbr0 type bridge\nip link add wg0 type wireguard\n\n# Listen for netlink messages\nip monitor # Change the default gateway to 192.168.1.1 on eth0\nip route change default via 192.168.1.1 dev eth0\n\n# Bring interface up\"\nip link set wlp2s0 up\n</code></pre> ip neighbor<pre><code># Display ARP cache\nip neighbor show\n\n# Delete ARP entry\nip neighbor delete $IP_ADDR dev eth0 </code></pre> ip netns<pre><code>ip netns # (1)\n\n# We can create a network namespace then add two virtual Ethernet interfaces.\n# These are **peers**, meaning they are linked together as if connected to the same switch.\nip netns add netns0\nip link add veth0 type veth peer name veth1 netns netns0\n\n# We can then run a command in the **context** of a namespace. Without providing a context, \n# the default namespace is used and we can display veth0 but not veth1. If there are no other\n# links in the namespace (which there shouldn't be) then the **number** of the interface's \n# peer appears in the link's name. By running a command in the context of the new namespace \n# we can display veth1. The interface number of the link it's paired with in the \n# default namespace also appears in this link's name.\nip link show                        # \"veth0@if2\"\nip netns exec netns0 ip link show   # \"veth1@if4\"\n\n# Now we assign an address to the namespaced link and bring it up\nip netns exec netns0 ip addr add 10.0.0.1/24 dev veth1\nip netns exec netns0 ip link set dev veth1 up\n\n# Pinging this IP from outside the namespace will not work because there is no route.\n# Adding an IP in the same subnet to veth0 creates the route.\nip addr add 10.0.0.2/24 dev veth0\n\n# The interface must be brought up, which automatically adds a route to the routing table.\nip link set dev veth0 up\n</code></pre> <ol> <li>Network namespaces are mounted to /var/run/netns</li> </ol>"},{"location":"Network/#iptables","title":"iptables","text":"<p>A frontend for the kernel-level netfilter service, similar to firewalld. </p> <p>Rules are saved in a rulesfile which once may have been found at /etc/sysconfig/iptables, but this file does not exist on recent Fedora installations.</p> <p>Display rules as written on disk <pre><code>iptables --list-rules\n</code></pre></p> <p>Reload configuration file <pre><code>iptables -F\n</code></pre></p> <p>Accept SSH traffic from a particular IP <pre><code>iptables -A INPUT -p ssh -s 10.0.222.222 -j ACCEPT\n</code></pre></p> <p>Accept incoming TCP traffic to port 80 <pre><code>iptables -A INPUT -p tcp --dport 80 -j ACCEPT\n</code></pre></p> <p>Change FORWARD chain policy <pre><code>iptables -P FORWARD ACCEPT # (1)\n</code></pre></p> <ol> <li>By default, the INPUT chain accepts incoming packets. However, this policy can be changed by specifying a DROP rule specification.</li> </ol> <p>Allow incoming SSH connections only from a single IP address <pre><code>iptables -A INPUT -p tcp --dport 22 -j DROP\niptables -A INPUT -p tcp --dport 22 -s 1.2.3.4 -j ACCEPT\n</code></pre></p> <p>Do not respond to pings <pre><code>iptables -t filter -A INPUT -p icmp -j DROP\n</code></pre></p>"},{"location":"Network/#netcat","title":"netcat","text":"<p>The netcat utility allows testing of a host's ports, similar to ping, but more versatile because ping only uses the portless ICMP protocol.  GNU and OpenBSD versions available</p> <p>Connect to host on port 80 <pre><code>nc example.com 80\n</code></pre> Scan ports</p> SingleMultipleRange of ports <pre><code>nc -v -w 2 z 192.168.56.1 22\n</code></pre> <pre><code>nc -v -w 2 z 192.168.56.1 22 80\n</code></pre> <pre><code>nc -v -w 2 z 192.168.56.1 22-25\n</code></pre> <p>Transfer files between servers This example uses the <code>pv</code> utility to monitor progress. <pre><code># Run `nc` in listening mode (`-l` option) on port 3000\ntar -zcf - debian-10.0.0-amd64-xfce-CD-1.iso | pv | nc -l -p 3000 -q 5\n\n# On the receiving client, to obtain the file:\nnc 192.168.1.4 3000 | pv | tar -zxf -\n</code></pre> Create a command-line chat server <pre><code># Create chat server listening on port 5000\nnc -l -vv -p 5000\n\n# Launch a chat session on the other system\nnc 192.168.56.1 5000\n</code></pre> Find a service running on port Obtain port banners (<code>-n</code> disables DNS lookup) <pre><code>nc -v -n 192.168.56.110 80\n</code></pre> Create stream sockets Create and listen on a UNIX-domain stream socket <pre><code>nc -lU /var/tmp/mysocket &amp;\nss -lpn | grep \"/var/tmp/\"\n</code></pre> Create a backdoor Netcat needs to listen on a chosen port (here 3001): <code>-d</code> disables reading from stdin; <code>-e</code> specifies the command to run on the target system <pre><code>nc -L -p 3001 -d -e cmd.exe\n</code></pre> Connect to {port} at {host} <pre><code>nc host port\n</code></pre> Netcat command that retrieves a webpage <pre><code>nc host port get\n</code></pre></p>"},{"location":"Network/#nft","title":"nft","text":"<pre><code>nft list ruleset\nnft list tables\nnft list table ip filter # display just the filter table\nnft flush ruleset\n</code></pre>"},{"location":"Network/#nmap","title":"nmap","text":"Scan hosts from a text file <pre><code>nmap -iL hosts.txt\n</code></pre> Identify a host's operating system <pre><code>nmap -A localhost.example.com\n</code></pre> Determine whether a host has a firewall enabled <pre><code>nmap -sA localhost.example.com\n</code></pre> Scan a specified range of ports <pre><code>nmap -p 10-300 localhost.example.com\n</code></pre> Perform a SYN TCP scan, stealthier than the TCP connect scan <pre><code>nmap -sT localhost.example.com\n</code></pre> Aggressive scan <pre><code>nmap -A 192.168.1.0/24\n</code></pre> Ping scan home network (not bothering with ports) <pre><code>nmap -sn 192.168.1.0/24\n</code></pre> Fast port scan using SYN packets <pre><code>nmap -sS -F 192.168.1.0/24\n</code></pre> Port scan using SYN (\"synchronize\") packet, first element of TCP handshake <pre><code>nmap -sS 192.168.1.0/24\n</code></pre> Port scan using normal TCP <pre><code>nmap -sT 192.168.1.0/24\n</code></pre> Port scan using UDP <pre><code>nmap -sU 192.168.1.0/24\n</code></pre> Xmas scan <pre><code>nmap -sX\n</code></pre> Scan a range of IPs [ref][Sec+ Lab] <pre><code>nmap 192.168.27.0/24 &gt; hosts.txt\n</code></pre> Identify operating system and scan ports using TCP SYN packets [ref][Sec+ Lab] <pre><code>nmap -O -sS 192.168.27.0/24 &gt; hosts.txt\n</code></pre>"},{"location":"Network/#tcpdump","title":"tcpdump","text":"<p>Inspect actual IP packets</p> <p>Display all network data <pre><code>tcpdump -i eth0   \n</code></pre> Set snapshot length of capture (default: 65,535B) <pre><code>tcpdump -s\n</code></pre></p>"},{"location":"Network/#ufw","title":"ufw","text":"<p>Program for managing a Netfilter firewall.</p> <p>Allow traffic associated with various services <pre><code>ufw allow ssh\nufw allow http\nufw allow https\n</code></pre></p>"},{"location":"Network/#wg","title":"wg","text":"<p>This is the main CLI frontend for Wireguard, the UDP-based tunneling protocol and application that was introduced with kernel 5.6.</p>  Fedora Ubuntu <pre><code>dnf install wireguard-tools\n</code></pre> <pre><code>apt install wireguard\n</code></pre>"},{"location":"Network/#wget","title":"wget","text":"Accept a self-signed certificate by skipping verification <pre><code>wget --no-check-certificate $URL\n</code></pre>"},{"location":"Network/#glossary","title":"Glossary","text":""},{"location":"Network/#ebpf","title":"eBPF","text":"<p>eBPF is an extended version of the Berkeley Packet Filter (BPF).  It is a sandboxed environment that allows code to be inserted into the running kernel.  Kernel functionality must normally be extended by building an entirely new kernel with custom modules or upstream patching of the Linux kernel.</p> <p>eBPF's architecture includes a JIT compiler that compiles the program's generic bytecode, which means eBPF programs run as efficiently as natively compiled kernel code.</p> <p>eBPF programs can be bound to kernel events, such as receipt of a packet from the NIC.</p> <p>bpftool is a core eBPF CLI tool.</p> <pre><code>bpftool prog     # Display running eBPF programs\nbpftool map show # Display maps\n</code></pre>"},{"location":"Network/#ifcfg","title":"ifcfg","text":"<p>Historically, ifcfg (interface configuration) files were ini-format files found in /etc/sysconfig/network-scripts/ in Red Hat distributions. They were used to control network interfaces on the legacy \"network\" service, now part of the network-scripts package, which included the sysconfig.txt file which documents the ifcfg file format.</p> <p>After the introduction of NetworkManager, this format survived and was expanded with new  directives specific to NetworkManager.</p> <p>By convention, the string value of the DEVICE directive was the suffix of the filename itself. ifcfg-eth0<pre><code>DEVICE=eth0\nBOOTPROTO=dhcp\nONBOOT=yes\nTYPE=Ethernet\n</code></pre></p> <p>The nmcli utility exposes a command that can change the configuration backend from ifcfg to a NetworkManager keyfile. Migrate a connection profile<pre><code>nmcli connection migrate eth0\n</code></pre></p> <p>Ifcfg file support was finally removed in RHEL 9 and Fedora 36. If no ifcfg files are present, the configuration backend that supports them can be removed. <pre><code>dnf remove NetworkManager-initscripts-ifcfg-rh\n</code></pre></p>"},{"location":"Network/#netfilter","title":"Netfilter","text":"Netfilter is a software firewall and packet filtering framework introduced with Linux 2.4.0 (2001) and controlled by the iptables command."},{"location":"Package/","title":"Package management","text":""},{"location":"Package/#commands","title":"Commands","text":""},{"location":"Package/#add-apt-repository","title":"add-apt-repository","text":"<p>APT repositories (/etc/apt/sources.list) are made of three parts, delimited by whitespace:</p> <ul> <li>Source type: <code>deb</code> for binary packages or <code>deb-src</code> for source packages</li> <li>Base URL of the source: beginning with <code>http://</code>, <code>ftp://</code>, <code>file://</code>, or even <code>cdrom:</code></li> <li>Name of the chosen distribution followed by sections that differentiate packages by license. Kali contains <code>main</code>, <code>non-free</code>, and <code>contrib</code>.</li> </ul> <pre><code>deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic main restricted\ndeb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic universe\ndeb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic-updates main restricted\ndeb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic-updates universe\n</code></pre> <pre><code>add-apt-repository \"deb http://security.ubuntu.com/ubuntu trusty-security main universe\" # Ubuntu\nadd-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" # Docker\nadd-apt-repository \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" # gcloud\nadd-apt-repository \"deb http://security.ubuntu.com/ubuntu trusty-security main universe\" # mailx\nadd-apt-repository \"deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse\" # MongoDB\nadd-apt-repository -y \"ppa:kgilmer/regolith-stable\" # Regolith Linux\n</code></pre>"},{"location":"Package/#apt-key","title":"apt-key","text":"<p>apt-key is typically used by piping a GPG key from curl. <pre><code># Google Cloud SDK\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - \ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\n\n# Docker in WSL\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - </code></pre></p> <p>Add key specified by apt in error message <pre><code>apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 68980A0EA10B4DE8\n</code></pre></p> <p>Install key from Mono <pre><code>apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF\n</code></pre></p>"},{"location":"Package/#apt","title":"apt","text":""},{"location":"Package/#dnf","title":"dnf","text":"<p>View history of dnf commands <pre><code>dnf history\ndnf history userinstalled # View all packages installed by user\n</code></pre></p> <p>Package groups can be specified using the group command or by prefixing the package group name with <code>@</code></p> <pre><code>dnf info @virtualization # dnf group info virtualization\ndnf install @virtualization # dnf group install virtualization\ndnf install --with-optional @virtualization # Include optional packages\n</code></pre> <p>Remove the configuration backend supporting the use of legacy ifcfg files in NetworkManager. <pre><code>dnf remove NetworkManager-initscripts-ifcfg-rh\n</code></pre></p> <p>Modules are special package groups representing an application, runtime, or a set of tools.  The Node.js module allows you to select several streams corresponding to major versions. <pre><code>dnf module install nodejs:12\n</code></pre></p> <p>Global dnf configuration is stored in either /etc/yum.conf or /etc/dnf.conf.</p> <pre><code>[main]\n; Exclude packages from updates permanently\nexclude=kernel* php*\n; Suppress confirmation\nassumeyes=True\n</code></pre> <p>The configuration can be dumped from the command-line (as root) <pre><code>dnf config-manager --dump\n</code></pre></p>"},{"location":"Package/#repos","title":"Repos","text":"<p>Repositories are INI files placed in  /etc/yum.repos.d/, but they can also be displayed and manipulated from the command-line.</p> Repositories<pre><code># Display repos\ndnf repolist # -v\n\n# Display enabled repos\ndnf repolist --enabled\n\n# Display a single repo\ndnf repoinfo docker-ce-stable\n\n# Add repo\ndnf config-manager --add-repo $REPO-URL\n\n# Disable repo\ndnf config-manager --set-disabled $REPO-NAME\n</code></pre> Example repos<pre><code>[docker-ce-stable]\nname=Docker CE Stable - $basearch\nbaseurl=https://download.docker.com/linux/fedora/$releasever/$basearch/stable\nenabled=1\ngpgcheck=1\ngpgkey=https://download.docker.com/linux/fedora/gp\n\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\n\n[google-cloud-sdk]\nname=Google Cloud SDK\nbaseurl=https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=0\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\n</code></pre> <p>Modules are collections of packages that are installed together. They often also have profiles available, which are variants of the module: i.e. client, server, common, devel, etc. <pre><code>dnf module list php\ndnf module install php:7.4/devel\ndnf module reset php\n</code></pre></p>"},{"location":"Package/#flatpak","title":"flatpak","text":"<p>Flatpak is one of several recent containerized application distribution solutions for Linux.</p> <p>Flatpak runtimes are compiled reproducibly using BuildStream and they are installed in /var/lib/flatpak/runtime. Like Steam, flatpak uses BubbleWrap to implement sandboxing.</p> <p>Flathub is the de facto Flatpak repo, but it must be added to flatpak installations manually. <pre><code>flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo\n\n# Confirming success\nflatpak remotes </code></pre></p> <p>Display installed flatpak applications, including runtime <pre><code>flatpak list --app --runtime\n</code></pre></p> <p>Output columns can also be specified individually after <code>--column</code> (comma-delimited) <pre><code>flatpak list --app --columns=name,application,runtime\n</code></pre></p> <p>Flatpak applications sometimes do not adopt the system theme. The workaround involves first granting some or all applications access to the themes folder. <pre><code>flatpak override --filesystem=$HOME/.themes\n</code></pre></p> <p>Then apply the theme by setting the <code>GTK_THEME</code> environment variable. The value of this variable must be the folder name of a theme installed to the themes folder (typically ~/.themes). <pre><code>flatpak override --env=GTK_THEME=my-theme </code></pre></p> <p>The value of the current theme can be retrieved using gsettings <pre><code>gsettings get org.gnome.desktop.interface gtk-theme\n</code></pre></p>"},{"location":"Package/#pacman","title":"pacman","text":"<pre><code>pacman -Q # --query\n</code></pre> <p>Display all orphaned dependencies (no longer needed) <pre><code>pacman -Qdt # --query --deps --unrequired\n</code></pre></p> <p>Display only explicitly installed packages and versions <pre><code>pacman -Qe # --query --explicit\n</code></pre></p> <p>Display explicitly installed packages, limiting output to program names <pre><code>pacman -Qeq # pacman --query --explicit --quiet\n</code></pre></p> <p>Display all packages installed from the AUR <pre><code>pacman -Qm # --query --foreign\n</code></pre></p> <p>Display all packages installed from main repos <pre><code>pacman -Qn # --query --native\n</code></pre></p> <p>Find which package owns {file} <pre><code>pacman -Qo file # --query --owns\n</code></pre></p> <p>List all install packages, filtering output to packages that are out-of-date on the local system <pre><code>pacman -Qu # --query --upgrades\n</code></pre></p> <p>Remove <code>$PACKAGE</code> <pre><code>pacman -R $PACKAGE # --remove package\n</code></pre></p> <p>Remove <code>$PACKAGE</code>, dependencies, and config files <pre><code>pacman -Rns $PACKAGE # --remove --recursive --nosave\n</code></pre></p> <p>Remove <code>$PACKAGE</code> as well as its dependencies <pre><code>pacman -Rs # --remove --recursive\n</code></pre></p> <p>Install <code>$PACKAGE</code> from the AUR <pre><code>pacman -S $PACKAGE # --sync\n</code></pre></p> <p>Remove all packages from the cache as well as unused sync databases <pre><code>pacman -Scc # --sync --clean --clean\n</code></pre></p> <p>Display information about {package} <pre><code>pacman -Si $PACKAGE # --sync --info package\n</code></pre></p> <p>Search for <code>$PACKAGE</code> in AUR repos <pre><code>pacman -Ss $PACKAGE # --sync --search package\n</code></pre></p> <p>Search for packages matching <code>$PATTERN</code> <pre><code>pacman -Ss $PATTERN # --sync --search pattern\n</code></pre></p> <p>Update package database <pre><code>pacman -Sy #  --sync --refresh\n</code></pre></p> <p>Update all packages from AUR and official repos <pre><code>pacman -Syu # --sync --refresh --sysupgrade\n</code></pre></p> <p>Force refresh of all package databases, even if they appear to be up-to-date <pre><code>pacman -Syy # --sync --refresh --refresh\n</code></pre></p> <p>Download program updates but don't install them <pre><code>pacman -Syyuw # --sync --refresh --refresh --sysupgrade --downloadonly\n</code></pre></p> <p>Get number of total installed packages <pre><code>pacman -Q | wc -l\n</code></pre></p>"},{"location":"Package/#rpm","title":"rpm","text":"<p>Query repos for information on a package <pre><code>rpm -qi $PACKAGE # --query --info\n</code></pre></p> <p>Upgrade or install a package, with progress bars <pre><code>rpm -Uvh $PACKAGE # --upgrade --verbose --hash\n</code></pre></p> <p>Display version of Fedora <pre><code>rpm -E %fedora\n</code></pre></p> <p>Import a keyring <pre><code>rpm --import \"https://build.opensuse.org/projects/home:manuelschneid3r/public_key\"\n</code></pre></p>"},{"location":"Package/#snap","title":"snap","text":"<p>Snap is one of several recent containerized application distribution solutions for Linux.</p> <p>Snap apps are slow to start because data is stored in squashfs images.</p> <p>Installation</p>  Red Hat <pre><code>dnf install -y snapd\nln -s /var/lib/snapd/snap /snap\n</code></pre>"},{"location":"Process/","title":"Process management","text":""},{"location":"Process/#commands","title":"Commands","text":""},{"location":"Process/#chrt","title":"chrt","text":"Lower the priority of tasks relative to others <pre><code>chrt -o\n</code></pre>"},{"location":"Process/#nice","title":"nice","text":"<p>Priorities range from 0-19 in <code>csh</code> (10 is default): lower values mean a higher priority.</p> <p>View priorities of jobs <pre><code>ps -l\n</code></pre></p> <p>Run <code>cmd</code> at a higher priority <pre><code>nice -5 cmd &amp;\n</code></pre> Run <code>$CMD</code> at a nice value of (positive) 10 <pre><code>nice -10 $CMD\nnice -n 10\nnice $CMD\n</code></pre></p>"},{"location":"Process/#ps","title":"ps","text":"<p>Display processes in a tree-like display illustrating parent-child relationships <pre><code>ps -f # --forest\n</code></pre></p> <p>Show system processes</p> BSD syntaxPOSIX syntax <pre><code>ps ax\n</code></pre> <pre><code>ps -ef\n</code></pre> <p>Display full listing of processes <pre><code>ps u # -f\n</code></pre></p> <p>Display user processes <pre><code>ps xG # -a\n</code></pre></p> <p>Display SELinux contexts for processes <pre><code>ps auxZ\n</code></pre></p> <p>Display kernel threads <pre><code>ps -ef\n</code></pre></p>"},{"location":"Process/#taskset","title":"taskset","text":"Send a task to a specific core <pre><code>taskset -c\n</code></pre>"},{"location":"Random/","title":"Random","text":"<p>There are two random-number devices in the kernel. Historically:</p> <ul> <li>/dev/random  blocked until it had sufficient entropy to return a random value</li> <li>/dev/urandom never blocked but resorted to a pseudorandom number generator (PRNG) in the case of insufficient entropy</li> </ul> <p>However, in 2020 the behavior of /dev/random was changed to make it behave more like the getrandom syscall, in that it blocks only on initialization and provides cryptographic-strength random numbers thereafter without blocking. This has resulted in a blurring of the lines between the two random devices and an effort to remove /dev/urandom for good.</p>"},{"location":"Red-Hat/","title":"Red Hat","text":""},{"location":"Red-Hat/#subscriptions","title":"Subscriptions","text":"<p>Subscriptions are managed through the Red Hat entitlement service, integrated with the Customer Portal. Entitlement certificates are stored as X.509 PEM certificate files in /etc/pki/entitlement for Simple Content Access (SCA)-enabled accounts.</p> Inspect X.509 certificate<pre><code>openssl x509 -text -in /etc/pki/entitlement/9012345678901234567.pem\n</code></pre>"},{"location":"Red-Hat/#subscription-manager","title":"subscription-manager","text":"<pre><code># Display status of subscriptions and products\nsubscription-manager status\n\n# Display installed products\nsubscription-manager list\n</code></pre>"},{"location":"Red-Hat/#applications","title":"Applications","text":""},{"location":"Red-Hat/#tuned","title":"TuneD","text":"<p>TuneD is a service that monitors the system and optimizes its performance under certain workloads. TuneD provides predefined profiles for power-saving and performance-boosting use cases.</p> <ul> <li>throughput-performance optimizes for throughput</li> <li>virtual-guest optimizes for performance</li> <li>balanced balances performance and power consumption</li> <li>powersave optimizes for power consumption</li> </ul> <p>These can be listed from the command-line:</p> <pre><code>tuned-adm list profiles\ntuned-adm active\ntuned-adm recommend\n\n# Select a profile\ntuned-adm profile powersave\n\n# Select a merged profile\ntuned-adm profile virtual-guest powersave\n</code></pre> /etc/tuned/tuned-main.conf<pre><code># Enable dynamic tuning, which monitors system components during uptime and makes system changes dynamically.\ndynamic_tuning=1\n</code></pre>"},{"location":"Red-Hat/#cockpit","title":"Cockpit","text":"Cockpit is builtin to Red Hat distributions and, once started as a normal SystemD service, is available at port 9090."},{"location":"Red-Hat/#storage","title":"Storage","text":""},{"location":"Red-Hat/#autofs","title":"Autofs","text":""},{"location":"Red-Hat/#stratis","title":"Stratis","text":""},{"location":"Red-Hat/#vdo","title":"VDO","text":""},{"location":"Red-Hat/#labs","title":"Labs","text":""},{"location":"Red-Hat/#ex200","title":"EX200","text":""},{"location":"Red-Hat/#iam","title":"IAM","text":"<p>We're going to lay the groundwork here and use these local accounts for all the subsequent tasks. You can write a script to do this, or do it by hand, from the data in the input file for the script. The file contents are:</p> <pre><code>manny:1010:dba_admin,dba_managers,dba_staff \nmoe:1011:dba_admin,dba_staff \njack:1012:dba_intern,dba_staff \nmarcia:1013:it_staff,it_managers \njan:1014:dba_admin,dba_staff \ncindy:1015:dba_intern,dba_staff \n</code></pre> <p>Set all user passwords to dbapass.  Also, change the users' PRIMARY groups' GID to match their UID.  Don't forget to check their home directories to make sure permisisons are correct!</p> <p>Enable the following command aliases:</p> <ul> <li>SOFTWARE</li> <li>SERVICES</li> <li>PROCESSES</li> </ul> <p>Add a new command alias named MESSAGES: <pre><code>/bin/tail -f /var/log/messages\n</code></pre> Enable superuser privilages for the following local groups:</p> <ul> <li>dba_managers: everything</li> <li>dba_admin: Command aliases: SOFTWARE, SERVICES, PROCESSES</li> <li>dba_intern: Command alias: MESSAGES</li> </ul>"},{"location":"Red-Hat/#repos","title":"Repos","text":"<p>You'll need to configure three repositories and install some software:</p> <ul> <li> <p>RHEL 8 BaseOS:</p> <ul> <li>Repository ID: [rhel-8-baseos-rhui-rpms]</li> <li>The mirrorlist is: https://rhui3.REGION.aws.ce.redhat.com/pulp/mirror/content/dist/rhel8/rhui/$releasever/$basearch/baseos/os</li> <li>The GPG key is located at: /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release</li> <li>You will need to add SSL configuration:</li> </ul> <pre><code>sslverify=1 sslclientkey=/etc/pki/rhui/content-rhel8.key sslclientcert=/etc/pki/rhui/product/content-rhel8.crt sslcacert=/etc/pki/rhui/cdn.redhat.com-chain.crt </code></pre> </li> <li> <p>RHEL 8 AppStream:</p> <ul> <li>Repository ID: [rhel-8-appstream-rhui-rpms]</li> <li>The mirrorlist is: https://rhui3.REGION.aws.ce.redhat.com/pulp/mirror/content/dist/rhel8/rhui/$releasever/$basearch/appstream/os</li> <li>The GPG key is located at: /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release</li> <li>You will need to add SSL configuration:</li> </ul> <pre><code>sslverify=1\nsslclientkey=/etc/pki/rhui/content-rhel8.key\nsslclientcert=/etc/pki/rhui/product/content-rhel8.crt\nsslcacert=/etc/pki/rhui/cdn.redhat.com-chain.crt\n</code></pre> </li> <li> <p>EPEL:</p> <ul> <li>Repository ID: [epel]</li> <li>The baseurl is: https://download.fedoraproject.org/pub/epel/$releasever/Everything/$basearch</li> </ul> </li> </ul> <p>Configure the repositories on the first server, then make an archive of the files, securely copy them to the second server, then unarchive the repository files on the second server.</p> <ul> <li>Install the default AppStream stream/profile for container-tools</li> <li>Install the youtube-dl package (from EPEL)</li> <li>Check for system updates, but don't install them</li> </ul>"},{"location":"Red-Hat/#networking","title":"Networking","text":"<p>On the first server, configure the second interface's IPv4/IPv6 addresses using nmtui.</p> <ul> <li>IPv4: 10.0.1.20/24</li> <li>IPv6: 2002:0a00:0114::/64</li> <li>Manual, not Automatic (DHCP) for both interfaces</li> <li>Only IP addresses, no other fields</li> <li>Configure only, do not activate</li> </ul>"},{"location":"Red-Hat/#logging","title":"Logging","text":"By default, the systemd journal logs to memory in RHEL 8, in the location /run/log/journal. While this works fine, we'd like to make our journals persistent across reboots. Configure the systemd journal logs to be persistent on both servers, logging to /var/log/journal."},{"location":"Red-Hat/#scheduling","title":"Scheduling","text":"<p>Create one at task and one cron job on the first server:</p> <ul> <li>The at job will create a file containing the string \"The at job ran\" in the file named /web/html/at.html, two minutes from the time you schedule it.</li> <li>The cron job will append to the /web/html/cron.html file every minute, echoing the date to the file.</li> </ul> <p>These files will be available via the web server on the first server after the \"Troubleshoot SELinux issues\" objective is completed.</p>"},{"location":"Red-Hat/#chrony","title":"Chrony","text":"<p>Time sync is not working on either of our servers. We need to fix that.</p> <p>Configure chrony to use the following server: <pre><code>server 169.254.169.123 iburst \n</code></pre> Make sure your work is persistent and check your work!</p>"},{"location":"Red-Hat/#grub","title":"GRUB","text":"<p>On server1, make the following changes:</p> <ul> <li>Increase the timeout using <code>GRUB_TIMEOUT=10</code></li> <li>Add the following line: <code>GRUB_TIMEOUT_STYLE=hidden</code></li> <li>Add quiet to the end of the <code>GRUB_CMDLINE_LINUX</code> line</li> </ul> <p>Validate the changes in /boot/grub2/grub.cfg. Do not reboot the server.</p>"},{"location":"Red-Hat/#storage_1","title":"Storage","text":"<p>On the second server:</p> <ul> <li> <p>Create a VDO device with the first unused 5GB device.</p> <ul> <li>Name: web_storage</li> <li>Logical Size: 10GB</li> </ul> </li> <li> <p>Use the VDO device as an LVM physical volume. Create the following:</p> </li> <li> <p>Volume Group: web_vg</p> <ul> <li> <p>Three 2G Logical Volumes with xfs file systems, mounted persistently at /mnt/web_storage_{dev,qa,prod}q:</p> <ul> <li>web_storage_dev</li> <li>web_storage_qa</li> <li>web_storage_prod</li> </ul> </li> </ul> </li> </ul> <p>We need to increase the swap on the second server. We're going to use half of our first unused 2G disk for this additional swap space. Configure the swap space non-destructively and persistently.</p> <p>On the second server, using the second 2G disk, create the following:</p> <ul> <li>Stratis pool: appteam</li> <li>Stratis file system, mounted persistently at /mnt/app_storage: appfs1</li> </ul>"},{"location":"Red-Hat/#shares","title":"Shares","text":"<p>Configure autofs on the first server to mount the user home directories on the second server at /export/home.</p> <ul> <li>On the second server, configure a NFS server with the following export:</li> </ul> <pre><code>/home &lt;first_server_private_IP&gt;(rw,sync,no_root_squash)\n</code></pre> <ul> <li>On the first server, configure autofs to mount the exported /home directory on the second server at /export/home. Change the home directories for our six users (manny|moe|jack|marcia|jan|cindy) to be <code>/export/home/&lt;user&gt;</code> and test.</li> </ul> <p>On the second server, create a directory at /home/dba_docs with:</p> <ul> <li>Group ownership: dba_staff</li> <li>Permissions: 770, SGID and sticky bits set</li> </ul> <p>Create a link in each shared user's home directory to this directory, for easy access.</p> <p>Set the following ACLs:</p> <ul> <li>Read-only for jack and cindy</li> <li>Full permissions for marcia</li> </ul>"},{"location":"Red-Hat/#container-as-service","title":"Container as service","text":"<p>As the cloud_user user on the first server, create a persistent systemd container with the following:</p> <ul> <li>Image: registry.access.redhat.com/rhscl/httpd-24-rhel7</li> <li>Port mappings: 8080 on the container to 8000 on the host</li> <li>Persistent storage at ~/web_data, mounted at /var/www/html in the container</li> <li>Container name: web_server</li> </ul>"},{"location":"Red-Hat/#selinux","title":"SELinux","text":"The Apache web server on the first server won't start! Investigate this issue, and correct any other SELinux issues related to httpd that you may find."},{"location":"Red-Hat/#firewall","title":"Firewall","text":"<p>Make sure the firewall is installed, enabled and started on both servers. Configure the following services/ports:</p> <ul> <li> <p>Server 1:</p> <ul> <li>ssh</li> <li>http</li> <li>Port 85 (tcp)</li> <li>Port 8000 (tcp)</li> </ul> </li> <li> <p>Server 2:</p> <ul> <li>ssh</li> <li>nfs</li> <li>nfs3</li> <li>rpc-bind</li> <li>mountd</li> </ul> </li> </ul>"},{"location":"Red-Hat/#commands","title":"Commands","text":""},{"location":"Red-Hat/#dnf","title":"dnf","text":"<p>View history of dnf commands <pre><code>dnf history\ndnf history userinstalled # View all packages installed by user\n</code></pre></p> <p>Package groups can be specified using the group command or by prefixing the package group name with <code>@</code></p> <pre><code>dnf info @virtualization # dnf group info virtualization\ndnf install @virtualization # dnf group install virtualization\ndnf install --with-optional @virtualization # Include optional packages\n</code></pre> <p>Remove the configuration backend supporting the use of legacy ifcfg files in NetworkManager. <pre><code>dnf remove NetworkManager-initscripts-ifcfg-rh\n</code></pre></p> <p>Modules are special package groups representing an application, runtime, or a set of tools.  The Node.js module allows you to select several streams corresponding to major versions. <pre><code>dnf module install nodejs:12\n</code></pre></p> <p>Global dnf configuration is stored in either /etc/yum.conf or /etc/dnf.conf.</p> <pre><code>[main]\n; Exclude packages from updates permanently\nexclude=kernel* php*\n; Suppress confirmation\nassumeyes=True\n</code></pre> <p>The configuration can be dumped from the command-line (as root) <pre><code>dnf config-manager --dump\n</code></pre></p>"},{"location":"Red-Hat/#repos_1","title":"Repos","text":"<p>Repositories are INI files placed in  /etc/yum.repos.d/, but they can also be displayed and manipulated from the command-line.</p> Repositories<pre><code># Display repos\ndnf repolist # -v\n\n# Display enabled repos\ndnf repolist --enabled\n\n# Display a single repo\ndnf repoinfo docker-ce-stable\n\n# Add repo\ndnf config-manager --add-repo $REPO-URL\n\n# Disable repo\ndnf config-manager --set-disabled $REPO-NAME\n</code></pre> Example repos<pre><code>[docker-ce-stable]\nname=Docker CE Stable - $basearch\nbaseurl=https://download.docker.com/linux/fedora/$releasever/$basearch/stable\nenabled=1\ngpgcheck=1\ngpgkey=https://download.docker.com/linux/fedora/gp\n\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\n\n[google-cloud-sdk]\nname=Google Cloud SDK\nbaseurl=https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=0\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\n</code></pre> <p>Modules are collections of packages that are installed together. They often also have profiles available, which are variants of the module: i.e. client, server, common, devel, etc. <pre><code>dnf module list php\ndnf module install php:7.4/devel\ndnf module reset php\n</code></pre></p>"},{"location":"Red-Hat/#firewall-cmd","title":"firewall-cmd","text":"<p>Frontend to Netfilter in Red Hat distributions.</p> <pre><code>firewall-cmd --state # \"running\"\n</code></pre> <p>Firewalld has a runtime configuration and a saved, persistent configuration. Only the runtime configuration will be consulted for any command, unless the persistent configuration is specified with --permanent.</p> <p>The runtime configuration can be saved with this command, which obviates the need to execute every change twice. <pre><code>firewall-cmd --runtime-to-permanent\n</code></pre></p> <p>Alternatively, the persistent configuration can be loaded into memory: <pre><code>firewall-cmd --reload\n</code></pre></p> Display firewall rules<pre><code>firewall-cmd --list-all --permanent\n</code></pre> <p>Firewalld uses zones to define the level of trust for network connections. A connection can only be part of one zone, but a zone can be used for many network connections. Builtin zones have XML-format configs found in /usr/lib/firewalld/zones. <pre><code>firewall-cmd --get-active-zones     # Display active zones along with interfaces\nfirewall-cmd --info-zone=public     # Inspect zone\nfirewall-cmd --new-zone=testlab     # Create new zone\n</code></pre></p> <p>Firewalld rules are generally managed through builtin services. These bundle network settings together for well-known applications like SSH, etc. Builtin services are also XML-format configs found in /usr/lib/firewalld/services.</p> Services<pre><code>firewall-cmd --list-services\nfirewall-cmd --add-service=http\nfirewall-cmd --remove-service=http\n</code></pre> <p>Firewalld's config file is at /etc/firewalld/firewalld.conf /etc/firewalld/firewalld.conf<pre><code>AllowZoneDrifting=no\n</code></pre></p> <p>Since RHEL 8, firewalld's backend has been changed to nftables. /etc/firewalld/firewalld.conf<pre><code>FirewallBackend=nftables\n</code></pre></p>"},{"location":"Red-Hat/#httpd","title":"httpd","text":"<p>The Apache web server daemon is named httpd in RHEL and other RPM-based distributions. HTML content is served from /var/www/html by default, but this can be changed by modifying the DocumentRoot directive in the Apache config located at /etc/httpd/conf/httpd.conf.</p> /etc/httpd/conf/httpd.conf<pre><code>DocumentRoot \"/web\"\n# ...\n&lt;Directory \"/web\"&gt;\n</code></pre> <p>Containers must mount content to /usr/local/apache2/htdocs.</p> <p>Users can also serve files from their home directories, by default from a directory named public_html.</p>"},{"location":"Red-Hat/#podman","title":"podman","text":"<p>On RHEL, podman can be installed as a package or as part of a module <pre><code>dnf module install container-tools\n</code></pre></p> <p>With few exceptions, podman exposes a command-line API that closely imitates that of Docker.</p> Arch Linux <p>On Arch, /etc/subuid and /etc/subgid have to be set. These are colon-delimited files that define the ranges for namespaced UIDs and GIDs to be used by each user.  Conventionally, these ranges begin at 100,000 (for the first, primary user) and contain 65,536 possible values. <pre><code>terry:100000:65536\nalice:165536:65536\n</code></pre></p> <p>Then podman has to be migrated <pre><code>podman system migrate\n</code></pre></p> <p>Podman supports pulling from various repos using aliases that are defined in /etc/containers/registries.conf.d. RHEL and derivative distributions support additional aliases, some of which reference images that require a login.</p> <p>For example, Red Hat offers a Python 2.7 runtime from the RHSCL (Red Hat Software Collections) repository on registry.access.redhat.com, which does not require authentication. However, Python 3.8 is only available from registry.redhat.io, which does. Interestingly, other Python runtimes are available from the ubi7 and ubi8 repos from unauthenticated registries.</p> <p>Container images are stored in ~/.local/share/containers/storage. <pre><code>podman pull rhscl/httpd-24-rhel7 # (1)\n</code></pre></p> <ol> <li>Alias to registry.access.redhat.com/rhscl/httpd-24-rhel7</li> </ol> <p>The Z option is necessary on SELinux systems (like RHEL and derivatives) and tells Podman to label the content with a private unshared label. On systems running SELinux, rootless containers must be explicitly allowed to access bind mounts. Containerized processes are not allowed to access files without a SELinux label. <pre><code>podman run -d -v=/home/jasper/notes/site:/usr/share/nginx/html:Z -p=8080:80 --name=notes nginx\npodman run -d -v=/home/jasper/notes/site:/usr/local/apache2/htdocs:Z -p=8080:80 --name=notes httpd-24\n</code></pre></p> <p>Mapped ports can be displayed <pre><code>podman port -a\n</code></pre></p> <p>Output a SystemD service file from a container to STDOUT (this must be redirected to a file) <pre><code>podman generate systemd notes \\\n--restart-policy=always   \\\n--name                    \\ # (3)\n--files                   \\ # (2)\n--new                     \\ # (1)\n</code></pre></p> <ol> <li>Yield unit files that do not expect containers and pods to exist but rather create them based on their configuration files.</li> <li>Generate a file with a name beginning with the prefix (which can be set with --container-prefix or --pod-prefix) and followed by the ID or name (if --name is also specified)</li> <li>In conjunction with --files, name the service file after the container and not the ID number.</li> </ol>"},{"location":"Red-Hat/#rpm","title":"rpm","text":"<p>Query repos for information on a package <pre><code>rpm -qi $PACKAGE # --query --info\n</code></pre></p> <p>Upgrade or install a package, with progress bars <pre><code>rpm -Uvh $PACKAGE # --upgrade --verbose --hash\n</code></pre></p> <p>Display version of Fedora <pre><code>rpm -E %fedora\n</code></pre></p> <p>Import a keyring <pre><code>rpm --import \"https://build.opensuse.org/projects/home:manuelschneid3r/public_key\"\n</code></pre></p>"},{"location":"Red-Hat/#rpmkeys","title":"rpmkeys","text":"<p>Manage RPM keyrings</p> <p>Import a keyring <pre><code>rpmkeys --import $PUBKEY\n</code></pre></p>"},{"location":"Red-Hat/#glossary","title":"Glossary","text":""},{"location":"Red-Hat/#centos","title":"CentOS","text":"<p>A community distribution of Linux that was created by Gregory Kurtzer in 2004 and acquired by Red Hat in 2014. </p> <p>It has traditionally been considered downstream to RHEL, incorporating changes to RHEL after a delay of several months.  In fact, it is a rebuild of the publicly available source RPMs (SRPMs) of RHEL packages, which historically allowed CentOS maintainers to simply package and ship them rebranded.</p> <p>For years, CentOS was the distribution of choice for experienced Linux administrators who did not feel the need to pay for Red Hat's support. In December 2020, Red Hat announced that CentOS 8 support will end at the end of 2021 (rather than 2029), while CentOS 7 will continue to be supported until 2024.  This represented a shift in focus from CentOS Linux to CentOS Stream and a change from a fixed-release (or \"stable point release\") to a rolling-release distribution model.</p> <p>CentOS Stream was announced in September 2019 as a distribution of CentOS maintained on a model previously misidentified as rolling-release but now described as \"continuously delivered\", organized into Streams.  CentOS Stream originated  in an effort to get more community participation in development of RHEL, rather than merely passive consumption.</p>"},{"location":"Red-Hat/#fedora","title":"Fedora","text":"<p>Fedora is a community distribution supported by Red Hat launched as \"Fedora Core\" in 2003.  It has traditionally been considered upstream to RHEL, serving as a testing ground for new features and improvements.</p> <p>Fedora CoreOS is a Fedora edition built specifically for running containerized workloads securely and at scale.  Because containers can be deployed across many nodes for redundancy, the system can be updated and rebooted automatically without affecting uptime. CoreOS systems are meant to be immutable infrastructure, meaning they are only configured through the provisioning process and not modified in-place.  All systems start with a generic OS image, but on first boot it uses a system called Ignition to read an Ignition config (which is converted from a Fedora CoreOS Config file) from the cloud or a remote URL, by which it provisions itself, creating disk partitions, file systems, users, etc.</p>"},{"location":"Ricing/","title":"Tiling window managers","text":"<ul> <li>i3 is perhaps the most popular tiling window manager, typically used with polybar.</li> <li>awesome is written and configured in Lua. It originated as a fork of dwm, it offers creature comforts that make it the easiest for neophytes to tiling window managers.</li> <li>bspwm (\"Binary Space Partitioning Window Manager\") uses tree partitioning as the logic for organizing tiles, with the default being the \"dwindle\" pattern.  Notably, it uses two config files: <ul> <li>.bspwmrc which determines what programs to autoload but doesn't contain any key bindings</li> <li>.sxhkdrc which uses a syntax similar to i3 or herbstluft.</li> </ul> </li> <li>dwm: One of the oldest and lightest tiling window managers.  Because Suckless wants the source code not to exceed 2,000 lines of code, a lot of functionality is incorporated by means of patches, which modify the source code using diff files. </li> <li>herbstluft has a single pool of workspaces that is shared across all monitors.</li> <li>xmonad is written and configured in Haskell, making it challenging to configure.</li> </ul> <p>All window managers place an INI-format .desktop file in /usr/share/xsessions/. xmonad.desktop<pre><code>[Desktop Entry]\nName=xmonad\nComment=Tiling window manager\nExec=xmonad-start\nTerminal=false\n\n[Window Manager]\nSessionManaged=true\n</code></pre></p>"},{"location":"Ricing/#xmonad","title":"Xmonad","text":"Install<pre><code>dnf install xmonad \\\nxterm dmenu # (1)\n</code></pre> <ol> <li>At the recommendation of DistroTube.</li> </ol> <p>Default keybindings which must be known by the neophyte:</p> Default keybinding Description ++Alt+Shift+Enter++ Terminal ++Alt+Shift+C++ Close pane <p>The config is typically placed in $XDG_CONFIG_HOME/xmonad/ (resolving to ~/.config/xmonad/), although other locations are possible. A good default config is available here.</p> <p>Change Mod key to Super <pre><code>myModMask = mod4Mask\n</code></pre></p>"},{"location":"Security/","title":"Security","text":""},{"location":"Security/#gpg","title":"GPG","text":"<p>GPG keys are used to sign packages and repos. For example, yum repos and apt incorporate APIs and handle GPG keys.</p> <p>The KWallet Manager and GNOME Keyring (Seahorse) applications can also be used to manage GPG keys.</p>"},{"location":"Security/#pam","title":"PAM","text":"<p>Pluggable authentication modules form an authentication framework that can be used by \"PAM-aware applications\". These applications have config files that are found in /etc/pam.d The various pam modules have man pages prefixed with pam_, i.e. \"pam_wheel\" etc.</p>"},{"location":"Security/#commands","title":"Commands","text":""},{"location":"Security/#gpg_1","title":"gpg","text":"<p>Decrypt file <pre><code>gpg file.txt\n</code></pre></p> <p>Export GPG public key <pre><code>gpg --export --output ~/jdoe.pub\n</code></pre></p> <p>Import another person's public key <pre><code>gpg --import jdoe.pub\n</code></pre></p> <p>List available GPG keys <pre><code>gpg --list-key\n</code></pre></p> <p>Encrypt a file <pre><code>gpg --encrypt -r jdoe@dplaptop.lab.itpro.tv $FILE\n</code></pre></p> <p>Sign $FILE without encrypting it (produces file.asc) <pre><code>gpg --clearsign $FILE\n</code></pre></p> <p>Import another person's public key <pre><code>gpg --import ~/jdoe.pub\n</code></pre></p> <p>Send keys to $SERVER <pre><code>gpg --send-keys keyIDs --keyserver $SERVER\n</code></pre></p>"},{"location":"Security/#pass","title":"pass","text":"<p>The standard unix password manager, backed by GPG, is a command-line password manager and MFA program.</p> <p>The first step in using pass is generating a new key pair. </p> Display public keys<pre><code>gpg -k # --list-keys\n</code></pre> <p>Unwanted keys can be deleted by specifying the public key: <pre><code>gpg --delete-secret-and-public-keys \u2592\u2592\u2593\u2591\u2591\u2592\u2593\u2591\u2593\u2591\u2593\u2591\u2593\u2592\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2592\u2593\u2591\u2592\u2591\u2592\u2591\u2593\u2591\u2592\u2592\u2592\u2593\u2592\u2592\u2593\u2591\u2593\u2592\u2591\n</code></pre></p> <p>Now a password store can be initialized by providing that same email address. This email is stored at ~/.password-store/.gpg-id <pre><code>pass init email@example.com\n</code></pre></p> Add password<pre><code>pass add email\n</code></pre> <p>This produces a binary, encrypted file at ~/.password-store/email.gpg. The password can be retrieved, after authenticating with the master password, with the following: <pre><code>pass email # (1)\n</code></pre></p> <ol> <li>In fact, because this is simply a GPG encrypted file, GPG could be used equivalently. In fact, this appears to be the command executed by the pass shell script. <pre><code>gpg -dq ~/.password-store/email.gpg\n</code></pre></li> </ol> Display names of passwords<pre><code>pass ls # (1)\n</code></pre> <ol> <li>This command is equivalent to using tree on the password store directory. <pre><code>tree ~/.password-store\n</code></pre></li> </ol> <p>Pass can also handle OTP generation for MFA, as long as you can retrieve the OTP URI (beginning with otpauth://).  QR code images can be deciphered with zbarimg to retrieve these URIs. <pre><code>pass otp add mimecast # (1)\n</code></pre></p> <ol> <li>Note that otpauth URLs usually contain an embedded email address, which must match that of the intialized password store. If this identity does not match, an error that read \"There is no assurance this key belongs to the named user\" is produced .</li> </ol> Resources <p>Luke Smith video</p>"},{"location":"Shell/","title":"Shells","text":""},{"location":"Shell/#fish","title":"fish","text":"<p>Fish switch statements look completely different from bash case statements, with an incompatible syntax.</p> Conditionally setting $PATH:<pre><code>switch \"$PATH\" # (1)\ncase \"*$HOME/.cargo/bin*\" # (2)\necho '$PATH already contains $HOME/.cargo/bin' # (3)\ncase '*'\nset --global PATH $HOME/.cargo/bin $PATH # (4)\nend # (5)\n</code></pre> <ol> <li>Because the $PATH is rendered as a list delimited by whitespace, without quotes this statement will be expanded to many arguments and will produce an error.</li> <li>Double quotes must be used, because with single quotes fish will not expand the $HOME variable.</li> <li>I have not found an empty placeholder similar to <code>pass</code> in Python which could simply occupy space here.  Without a statement, fish appears to execute the following block by default.</li> <li>Environment variables use the set keyword. The --universal option, which would otherwise make sense here, does not work because $PATH is a global variable. Note that there is no equal sign, only a space separating the variable identifier and value.</li> <li>Bash equivalent<pre><code>case \":${PATH}:\" in\n*:\"$HOME/.cargo/bin\":*)\n;;\n*)\nexport PATH=\"$HOME/.cargo/bin:$PATH\"\n;;\nesac\n</code></pre></li> </ol> Setting environment variables<pre><code>set -x EDITOR /usr/bin/vim # (1)\n</code></pre> <ol> <li>Without -x this variable will not be visible to applications. Bash equivalent<pre><code>export EDITOR=/usr/bin/vim\n</code></pre></li> </ol> <p>Fish for-in loops are concluded with end. Set metadata in a loop<pre><code>for i in $(exa Godfrey*)\necho Processing $i\nset title $(string replace -r \"\\(.*mp3$\" \"\" $i) # (1)\nffmpeg -i $i -metadata title=\"$title\" -metadata album=\"Godfrey\" -metadata artist=\"Vlad TV\" -codec copy output/$i\nend\n</code></pre></p> <ol> <li>string replace is used here to remove the ending of a filename, including extension.</li> </ol>"},{"location":"Shell/#bash","title":"bash","text":"<p>The systemwide config for bash is at /etc/profile.</p> <p>Command-line arguments are available as the positional arguments <code>$1</code>, <code>$2</code>, etc. with the script itself being <code>$0</code>. Handling command-line arguments is conventionally done with the shift command in a <code>while</code> loop.</p> Conditionally setting $PATH<pre><code>\n</code></pre> <p>The shopt internal command is used to set (-s) or unset (-u) various shell settings.</p> Disable case sensitivity<pre><code>shopt -s nocasematch\n</code></pre> Tag audio with metadata<pre><code>\n</code></pre> <p>A more useful and less brittle version of this script may be possible using the getopts function to define a named parameter, rather than forcing the first positional argument to be one of a number of set values.</p>"},{"location":"Storage/","title":"Storage","text":"Create virtual disks<pre><code># Create sparse file\nfallocate -l 100M /tmp/disk0    # Create loopback device\nlosetup -f /tmp/disk0           </code></pre> Formatting filesystems<pre><code>mkfs.ext4 /dev/sda1\nfsck.ext4 /dev/sda1\n\nmkfs.xfs /dev/sda2\nxfs_repair /dev/sda2\n</code></pre> <p>Traditionally, filesystems are mounted using /etc/fstab, a whitespace-delimited config file.</p> <p>Typical fstab entries have six columns, delimited by whitespace:</p> <ul> <li>fs_spec describes the block special device. It can appear in any number of ways depending on the nature of the device</li> <li>fs_file describes the mount point</li> <li>fs_vfstype describes the type of filesystem</li> <li>fs_mntops describes mount options</li> <li>fs_freq is used by dump to determine which filesystems need to be dumped</li> <li>fs_passno is used by fsck to determine the order in which filesystem checks are done at boot time</li> </ul> /etc/fstab<pre><code># NFS\nnas:/export/storage     /mnt/nfs_storage        nfs     defaults,_netdev 0 0\n\n# SMB\n//nas/storage           /mnt/smb_storage        cifs    guest,uid=1000,iocharset=utf-8  0 0\n\n# VDO\n/dev/mapper/storage     /mnt/vdo_storage        xfs     _netdev,x-systemd.device-timeout=0,x-systemd.requires=vdo.service 0 0\n\n# Stratis\n\n# btrfs\nUUID=5530f7df-65b7-4fd5-8757-0e69aad14f75 /     btrfs   subvol=/@,defaults,noatime,autodefrag,compress=zstd,discard=async,ssd 0 0\nUUID=5530f7df-65b7-4fd5-8757-0e69aad14f75 /home btrfs   subvol=/@home,defaults,noatime,autodefrag,compress=zstd,discard=async,ssd 0 0\n</code></pre>"},{"location":"Storage/#zfs","title":"ZFS","text":"<p>ZFS is a technology that combines the functions of a 128-bit CoW filesystem, a volume manager, and software RAID. Like RAID, ZFS attempts to achieve data reliability by abstracting volumes over physical devices.  But ZFS improves on RAID in a variety of ways:</p> <ul> <li>ZFS achieves error handling by using checksum information to correct corrupted files. This is unlike hardware RAID mirrors, where failures occur silently and are typically only detected upon reading a corrupt file.</li> <li>ZFS writes use CoW meaning they are atomic and aren't affected by issues like RAID holes, a condition in which a stripe is only partially written before the system crashes, making the array inconsistent and corrupt after a restart</li> <li>ZFS can also transparently compress and encrypt data written to datasets.</li> </ul> <p>A disadvantage of ZFS is that it is too tightly bound to the operation of the kernel to operate in true userspace, and that is why each implementation is different for operating systems. ZFS on Linux (ZOL) is considered the ugly stepchild of the ZFS community despite the fact that the Linux implementation has the most features and community support.</p> <p>The largest structure in the ZFS taxonomy is the pool or zpool, an independent collection of one or more logical devices which provide space for filesystems (equivalent to a logical volume in LVM). Virtual devices or vdevs constitute a pool and are an abstraction of one or more storage devices (equivalent to volume groups in LVM).</p> <p>Vdevs support one of five topologies:</p> <ul> <li>Single-device vdevs cannot survive any failure</li> <li>Mirror vdevs duplicate every block on each of their devices</li> <li>RAIDz1, RAIDz2, or RAIDz3 provide one, two, or three parity blocks for each stripe. The minimum number of disks is always one more than the number of parity devices.</li> </ul> <p>Writes are distributed across available vdevs in accordance with their available free space, such that they fill more or less evenly over time. In ZFS parlance these pieces of data are called stripes.</p> <p>Special support classes of vdev such as spare, cache, log, and special represent vdevs that are dedicated to specialized tasks to enhance performance and reliabilities of the pool.</p>"},{"location":"Storage/#pool-creation","title":"Pool creation","text":"<p>A variety of ZFS topologies can be created on a single line by passing subcommands and arguments to zpool create.</p> <pre><code># Three-wide mirror\nzpool create tank mirror /dev/sd{a,b} # (1)\n\n# Two two-wide mirrors\nzpool create tank mirror /dev/sd{a,b} mirror /dev/sd{c,d}\n\n# Raidz\nzpool create tank raidz /dev/sd{a,b,c}\n\n# Setting a property at creation with -o\nzpool create tank mirror /dev/sd{a,b} -o compression=lz4\n</code></pre> <ol> <li>Zpools, or pools for short, are conventionally named \"tank\" in documentation.</li> </ol> <p>Additional vdevs (\"virtual device\") can be added to existing pools using zpool add using identical syntax.</p> <pre><code>zpool add tank mirror /dev/sd{e,f}\n</code></pre> <p>Pools are most commonly displayed using zpool status. Depending on the installation, by default disks are identified by UUID or block device name.</p> <pre><code>zpool status tank -g # GUIDs\nzpool status tank -L # Block device names\nzpool list -v\n\n# Destroy pool\nzpool destroy tank\n</code></pre>"},{"location":"Storage/#removing-devices","title":"Removing devices","text":"<p>zpool remove is used to remove spare, cache, log and top-level data devices that are not raidz (top-level raidz vdevs cannot be removed).</p> <pre><code># Hot spares\nzpool add tank spare sdg\nzpool remove tank sdg\n\n# When incorrectly adding a device to a pool\nzpool add tank sdh\nzpool remove sdh\nzpool add tank mirror sdh\n</code></pre> <p>Alternatively, zpool detach is used exclusively to remove a mirrored data device.</p> Mirrored arrays<pre><code>zpool add tank mirror sde sdf\nzpool detach sdb\n</code></pre>"},{"location":"Storage/#disk-replacement","title":"Disk replacement","text":"Replace disk<pre><code># Clear errors first\nzpool clear tank\n\n# Take disk offline\nzpool offline tank sdb\n\n# Replace disk physically\n\n# Begin the resilvering process\nzpool replace tank sdb sdc\n\n# Monitor resilvering process\nwatch zpool status tank\n</code></pre> <p>If a disk has gone bad, it must first be taken offline using zpool offline before physically replacing it. Failing to do this will produce errors.</p> <p>Sometimes the UUID must be provided to take a disk offline. This can be retrieved using blkid.</p> <pre><code>blkid /dev/sdb2\nzpool offline $UUID\n</code></pre> <p>Then zpool replace initiates the process of rebuilding redundant groups, or resilvering.</p> <p>Replacing a used disk with one that is unused (without physical replacement) similarly uses zpool replace.</p> <p>Ongoing resilvers can be cancelled using zpool detach:</p> <pre><code>zpool detach tank sdc\n</code></pre> <p>When simply moving a disk's physical location (from one bay to another), it must also be taken offline.</p> <pre><code>zpool offline tank sdb\n# Move disk physically\nzpool online tank sdb\n</code></pre> <p>Exporting a pool using zpool export takes it offline, allowing all disks to be removed.</p> <p>Importing a pool is also straightforward. Unless there is a name conflict, the name of the pool is all that is needed. The UUID of the pool can be retrieved by inspecting member disks using lsblk.</p> <pre><code>zpool export tank\nzpool import tank\n</code></pre> <p>Note that for TrueNAS, a pool should be imported through the GUI, otherwise attempting to create network shares will produce errors.</p>"},{"location":"Storage/#dataset-management","title":"Dataset management","text":"<p>A dataset in ZFS is equivalent to the btrfs subvolume, defined as an independently mountable POSIX filetree. Commands under zfs exist for displaying, creating, renaming, and destroying datasets:</p> Create dataset<pre><code>zfs create tank/dataset\nzfs list -r tank # (1)\nzfs rename tank pool\nzfs remove pool/dataset\n</code></pre> <ol> <li>Providing a pool name alone will not display contained datasets recursively.</li> </ol> <p>Dataset properties can be changed with zfs set:</p> Configure dataset<pre><code>zfs set mountpoint=/mnt/tank tank\nzfs set compression=on tank/dataset\nzfs set sync=disabled tank/dataset\nzfs set acme:disksource=vendorname  </code></pre> <p>A ZFS volume is a dataset that represents a block device. They are created with the -V option and can be found under /dev/zvol.</p> <pre><code>zfs create -V 5gb tank/vol\n</code></pre> <p>A volume can be shared as an iSCSI target by setting the shareiscsi property on the volume.</p>"},{"location":"Storage/#snapshot-management","title":"Snapshot management","text":"<p>Snapshots are read-only copies of file systems or volumes. They are managed using subcommands under zfs snapshot and appear as directories at the root of the file system of every dataset under .zfs/snapshot.</p> Create snapshot of a pool recursively<pre><code>zfs snapshot -r tank@now\n</code></pre> Display snapshots<pre><code>zfs list -t snapshot\n</code></pre> Snapshot management<pre><code>zfs snapshot tank@snapshot1\nzfs rollback tank@snapshot1\nzfs destroy tank@snapshot1\n</code></pre>"},{"location":"Storage/#migration","title":"Migration","text":"<p>Migrating pools is done using snapshots as well, with the commands zfs send and zfs receive. First a snapshot is taken, then:</p> <pre><code># Use pv to monitor progress\nzfs send -R source-tank@moving | pv | zfs receive -Fd receive-tank/ # (1)\n</code></pre> <ol> <li>Without -d, the recursive copy will be performed into the same destination without recreating the dataset. </li> </ol>"},{"location":"Storage/#btrfs","title":"Btrfs","text":"<p>Oracle's Btrfs \"B-tree file system\" aimed to repeat ZFS's advances during a period when it seemed that licensing challenges would prevent ZFS from becoming available on Linux. It was incorporated into the Linux kernel in 2009 but is still under active development.</p> <p>Several key differences in capabilities between Btrfs and ZFS:</p> <ul> <li>Topologies can be changed much more easily</li> <li>Btrfs uses less RAM</li> <li>Btrfs does not have the ability to allocate specific devices as caches</li> <li>As of 2017, Btrfs implementations of parity raid are not yet ready for production use</li> </ul> Resources <p>Unlike ZFS which has a lot of material in written and video form for potential users to learn from, BtrFS appears not to have much available. BtrFS does have an official wiki, but written articles on FOSS blogs focus on operation from the command-line but don't do a good job of describing the taxonomy of concepts, aside from the glossary.</p> <p>Users of ZFS, in contrast, have taken the trouble to create introductory material, including Ars Technica's ZFS 101 article, and many talks by enthusiasts like Philip Paeps.</p> <p>This might be because btrfs's concepts seem less well thought-out, or at least more poorly described. For example, the term subvolume is used in btrfs but the container for subvolumes is not \"volume\" but rather \"top-level subvolume\".</p> <p>Jim Salter from Ars Technica (who wrote the ZFS 101 article above) appears to have devoted some effort to fleshing out the topic:</p> <ul> <li> <p>Examining btrfs, Linux's perpetually half-finished filesystem</p> </li> <li> <p>Install and configure Samba server</p> </li> <li>Install Samba4 on RHEL 8 for File Sharing on Windows</li> <li>FreeNAS 11.3 - How to Set Up Windows SMB Shares</li> <li>BtrFS</li> <li>Creating and Destroying ZFS Storage Pools</li> <li>Managing devices in ZFS storage pools</li> <li>Getting started with btrfs for Linux</li> <li>Understanding Linux filesystems: ext4 and beyond</li> </ul>"},{"location":"Storage/#pool-management","title":"Pool management","text":"Create a storage pool<pre><code>mkfs.btrfs --data raid0 /dev/sd{a,b,c} # (1)\n</code></pre> <ol> <li>Valid arguments to -d/--data include raid0, raid1, raid1c3, raid1c4, raid5, raid6, raid10, single, or dup.</li> </ol> Add device<pre><code>btrfs device add /dev/sde /data\nbtrfs filesystem balance /data\n</code></pre> Remove device<pre><code>btrfs device delete /dev/sdb /data\nbtrfs filesystem balance /data\n</code></pre> Replace device<pre><code>btrfs device add /dev/sdc /data\nbtrfs device delete /dev/sdb /data\nbtrfs filesystem balance /data\n</code></pre>"},{"location":"Storage/#filesystem-management","title":"Filesystem management","text":"<p>Filesystems in btrfs are equivalent to ZFS datasets except that filesystems can be divided into \"subvolumes\".</p> Display subvolumes<pre><code># Subvolume names begin with **@**.\nbtrfs subvolume list /\n</code></pre> <p>Subvolumes can be mounted to separate mountpoints, even though they specify the same filesystem, using the subvol directive.</p> <pre><code>UUID=5530f7df-65b7-4fd5-8757-0e69aad14f75 /              btrfs   subvol=/@,defaults,noatime,autodefrag,compress=zstd,discard=async,ssd 0 0\nUUID=5530f7df-65b7-4fd5-8757-0e69aad14f75 /home          btrfs   subvol=/@home,defaults,noatime,autodefrag,compress=zstd,discard=async,ssd 0 0\n</code></pre> Rename filesystem<pre><code>btrfs filesystem label / tank\n\n# Verify\nbtrfs filesystem show\nblkid\n</code></pre> Measure free space<pre><code>btrfs filesystem df /\n\n# Show storage consumed, including how much is shared by all snapshots\nbtrfs fi du /home -s\n</code></pre>"},{"location":"Storage/#snapshots","title":"Snapshots","text":"<p>In btrfs, you can take snapshots of btrfs subvolumes only, and the snapshots are subvolumes themselves. In fact, by definition a btrfs snapshot is simply a subvolume that shares its data and metadata with some other subvolume using btrfs's CoW capabilities.</p> <pre><code>btrfs subvolume list /\n</code></pre> <p>These are stored in the .snapshots directory at the root of the subvolume as numbered subdirectories. Restoring from these snapshots can be done with simple file tools like rsync.</p> <pre><code>rsync -avz /.snapshots/141/snapshot/$PATH $PATH\n</code></pre> <p>Snapshots are read-only by default, but can also be writable. Snapshots can be investigated:</p> <pre><code>btrfs subvolume show /.snapshots/141/snapshot\n</code></pre> Delete snapshot<pre><code>btrfs subvolume delete /.snapshots/141/snapshot\n</code></pre>"},{"location":"Storage/#nfs","title":"NFS","text":"<p>NFS or \"Network File System\" is a distributed filesystem based on the RPC protocol that provides transparent access to remote disks.</p> <p>Shares or \"exports\" made available to clients are defined in /etc/exports.</p> /etc/exports on TrueNAS<pre><code>\"/mnt/pool/Documents\"\\\n    *(sec=sys,rw,no_subtree_check)\n\n# Read-only (\"rw\" is removed)\n\"/mnt/pool/Aliana\"\\\n    *(sec=sys,no_subtree_check)\n</code></pre> <p>Once exports are defined, the NFS server can be started <pre><code>systemctl enable --now nfs-server.service\n</code></pre></p> <p>Exports can be displayed using showmount -e.</p> <p>Shares can be manually mounted in /etc/fstab using the following syntax: <pre><code>127.0.0.1:/export/web_data1 /mnt/nfs_web_data1 nfs defaults,_netdev 0 0\n127.0.0.1:/export/web_data2 /mnt/nfs_web_data2 nfs defaults,_netdev 0 0\n</code></pre></p> <p>Better still is using autofs.</p> Resources <ul> <li> How to Share Files Using NFS: Linux Server Training 101</li> </ul> History <p>Modern NFS deployments in the wild are usually versions 3 or 4. NFS version 4 (released in 2003) has superior performance, requires only the additional mountd service and TCP port 2049 to be open.</p> <p>NFS version 3 was released in the early 1990s resolved earlier performance bottlenecks with a coherency scheme that allowed asynchronous writes. File locking was implemented by means of two separate daemons, lockd and statd. This approach was found to be unreliable, and NFSv4 incorporated file locking into the core protocol.</p>"},{"location":"Storage/#authentication","title":"Authentication","text":"<p>All versions of NFS support various security mechanisms:</p> <ul> <li>AUTH_NONE no authentication</li> <li>AUTH_SYS Unix-style user and group access control</li> <li>RPCSEC_GSS layer which is often used in combination with Kerberos</li> </ul> <p>AUTH_SYS depends on the client reporting user UID and GID, which are compared with the server's own values in /etc/passwd. This means that users with root access can su to whatever UID they wish, allowing them to access shares. This approach depends on the client to correctly report its access and is highly insecure.</p> <p>NFS servers intercept incoming requests made on behalf of UID 0 (root) and changes them to map them to that of another reason, a process called \"squashing root\". A placeholder account named \"nobody\" with UID 65534 is defined to be the account as whom a root user masquerades on an NFS server. Options in the exports file can control this behavior.</p> <p>In TrueNAS, the MAPROOT setting specifies a user account to which root logins should be mapped. Similarly, the MAPALL setting defines a user account to which any client account should be mapped.</p> <pre><code># Normal NFS share in TrueNAS\n\"/mnt/pool/Media/Pictures\"\\\n*(sec=sys,rw,no_subtree_check)\n\n# MAPALL set to user media with UID 3000\n\"/mnt/pool/Media/Videos\"\\\n*(sec=sys,rw,anonuid=3000,anongid=3000,all_squash,no_subtree_check)\n</code></pre> <p>AUTH_SYS authentication provide a good use-case to use certain commands like id.</p>"},{"location":"Storage/#smb","title":"SMB","text":"<p>The SMB implementation on Linux is called Samba.</p> <pre><code># Configure Samba \nmkdir /samba                   # Create a directory for the share\nchmod -R 0777 /samba\nchown -R nobody:nobody /samba  # Remove ownership, not necessary\n\n# Open firewall rule (not strictly necessary)\nfirewall-cmd --permanent --add-service=samba\nfirewall-cmd --reload\n\n# Verify\nfirewall-cmd --list-services </code></pre> <p>Configure the main Samba config file at /etc/samba/smb.conf. The name in brackets becomes the name of the share. <pre><code>[samba]\ncomment = Samba on Ubuntu\npath = /samba\nread only = no\nbrowsable = yes\n</code></pre></p> <p>Verify configuration</p> <pre><code>testparm\n</code></pre> SELinux<pre><code># Set SELinux context of share directory\nsemanage fcontext -a -t samba_share_t '/samba(/.*)?'\nrestorecon -vvFR /samba\n\n# Allow SELinux to work with Samba\nsetsebool -P samba_export_all_ro on\n</code></pre> <pre><code># Set up a Samba account\nsmbpasswd -a $USER\n\n# Restart Samba service\nsystemctl restart smbd\n\n# Browse all available shares\nsmbclient -L $HOST\n\n# Access SMB share, displaying the Samba CLI \"smb: \\&gt;\"\nsmbclient //$HOST/$USER -U $USER\n</code></pre> <p>On TrueNAS, the option to \"Allow Guest Access\" should be turned on, unless password-based authentication for specific users is desired. Also, the directory must have write permissions enabled to allow uploading. <pre><code>chmod o+w\n</code></pre> Bizarrely, the ability to navigate into subdirectories appears to depend on the owner execute bit. This may have something to do with anonymous guest access. <pre><code>chmod u+x\n</code></pre></p> <p>Permanently mounting a Samba share in /etc/fstab <pre><code>//nas/Videos /home/jasper/Videos cifs guest,uid=1000,iocharset=utf8 0 0\n</code></pre> Then mount the fstab file <pre><code>mount -a\n</code></pre></p>"},{"location":"Storage/#hdd-serial-numbers","title":"HDD serial numbers","text":"<pre><code># Produce a CSV of hard disk identifiers and their serial numbers using hdparm, grep, cut, and output redirection.\nfor l in {a..w} do echo -n \"/dev/sd$l,\" &gt;&gt; drives\n    hdparm -I /dev/sd$l | grep 'Serial Number' - |\ncut -d : -f 2 | tr -d '[:space:]' &gt;&gt; drives\n    echo '' &gt;&gt; drives;\ndone\n</code></pre>"},{"location":"Storage/#autofs","title":"Autofs","text":"<p>Auto File System offers an alternative way of mounting NFS shares that can save some system resources, especially when many shares are mounted. Autofs can mount NFS shares dynamically, only when accessed.</p> <pre><code>dnf install -y autofs\nsystemctl enable --now autofs.service\n</code></pre> <p>Mounts are defined in configs called maps. There are three map types:</p> <ul> <li>master map is /etc/auto.master by default. This can be defined using the master_map_name directive in the main configuration at /etc/autofs.conf.</li> <li>direct maps point to other files for mount details. They are notable for beginning with /-</li> <li>indirect maps also point to other files for mount details but provide an umbrella mount point which will contain all other mounts within it. Note that other mountpoints at this parent directory cannot coexist with autofs mounts.</li> </ul> <p>Here is an example indirect map that will mount to /data/sales.</p> /etc/auto.master.d/data.autofs<pre><code>/data /etc/auto.data\n</code></pre> /etc/auto.data<pre><code>sales -rw,soft 192.168.33.101:/data/sales\n</code></pre> <p>Map files also support wildcards. <pre><code>* 127.0.0.1:/home/&amp;\n</code></pre></p>"},{"location":"Storage/#lvm","title":"LVM","text":"LVM<pre><code>pvcreate /dev/vd{b,c,d}\nvgcreate group /dev/vd{b,c,d}\n\n# Use all free space\nlvcreate -l 100%FREE -n volume group # --extents, --name\n</code></pre>"},{"location":"Storage/#vdo","title":"VDO","text":"<p>Virtual disk optimizer (VDO) is a kernel module introduced in RHEL 7.5 that provides data deduplication and compression on block devices.</p> <p>The physical storage of a VDO volume is divided into a number of slabs, which are contiguous regions of the physical space.  All slabs for a given volume have the same size, which can be any power of 2 multiple of 128 MB up to 32 GB (2 GB by default). The maximum number of slabs is 8,192. The maximum physical storage of the VDO is provided to the user on creation.</p> <p>Like LVM volumes, VDO volumes appear under /dev/mapper</p> <p>VDO appears not to be installed by default, but it is available in the BaseOS repo. <pre><code>dnf install vdo\nsystemctl enable --now vdo\n</code></pre></p> Create a VDO volume<pre><code>vdo create --name=web_storage --device=/dev/xvdb --vdoLogicalSize=10G\nvdostats --human-readable\nmkfs.xfs -K /dev/mapper/web_storage\nudevadm settle\n</code></pre> /etc/fstab<pre><code>/dev/mapper/storage     /mnt/vdo_storage        xfs     _netdev,x-systemd.device-timeout=0,x-systemd.requires=vdo.service 0 0\n</code></pre>"},{"location":"Storage/#stratis","title":"Stratis","text":"<p>Stratis is an open-source managed pooled storage solution in the vein of ZFS or btrfs.</p> <p>Stratis block devices can be disks, partitions, LUKS-encrypted volumes, LVM logical volumes, or DM multipath devices. Stratis pools are mounted under /stratis and, like other pooled storage systems, support multiple filesystems. Stratis file systems are thinly provisioned and formatted with xfs, although vanilla xfs utilities cannot be used on Stratis file systems.</p> <pre><code>dnf -y install stratisd stratis-cli\nsystemctl enable --now stratisd\n</code></pre> Pool management<pre><code>stratis pool create pool /dev/sd{a,b,c}\n\n# An error about the devices being \"owned\" can be resolved by wiping it.\nwipefs -a /dev/sda\n\n# Display block devices managed by Stratis, equivalent to\n#   pvs\nstratis blockdev\n\n# Create filesystem\nstratis fs create pool files\n\n# Confirm\nstratis fs\n\n# Expand pool, equivalent to \n#   zpool add pool /dev/sdd\nstratis pool add-data pool /dev/sdd\n</code></pre> /etc/fstab<pre><code>\n</code></pre> Snapshot management<pre><code># Save snapshot\nstratis fs snapshot pool files files-snapshot\n\n# Restore from snapshot\nstratis fs rename files files-orig\nstratis fs rename files-snapshot files\numount /mnt/files; mount /mnt/files\n</code></pre>"},{"location":"Storage/#glossary","title":"Glossary","text":""},{"location":"Storage/#commands","title":"Commands","text":""},{"location":"Storage/#blkid","title":"blkid","text":""},{"location":"Storage/#blkid_1","title":"blkid","text":""},{"location":"Storage/#btrfs_1","title":"btrfs","text":""},{"location":"Storage/#fallocate","title":"fallocate","text":""},{"location":"Storage/#fallocate_1","title":"fallocate","text":"<p>Create a file of a given size with the <code>--length</code>/<code>-l</code> option</p> 1 gigabyte1 gibibyte <pre><code>fallocate -l 1GB $FILENAME # gigabyte\n</code></pre> <pre><code>fallocate -l 1G $FILENAME  # gibibyte\n</code></pre>"},{"location":"Storage/#filefrag","title":"filefrag","text":""},{"location":"Storage/#hdparm","title":"hdparm","text":""},{"location":"Storage/#hdparm_1","title":"hdparm","text":"<pre><code>hdparm -I /dev/sda\n</code></pre>"},{"location":"Storage/#losetup","title":"losetup","text":""},{"location":"Storage/#losetup_1","title":"losetup","text":"Create a loopback device (i.e. a virtual block device) <pre><code>losetup -f /tmp/file1\n</code></pre>"},{"location":"Storage/#lsblk","title":"lsblk","text":""},{"location":"Storage/#lsblk_1","title":"lsblk","text":"Display filesystems<pre><code>lsblk -f # --fs\n</code></pre>"},{"location":"Storage/#sfdisk","title":"sfdisk","text":""},{"location":"Storage/#sfdisk_1","title":"sfdisk","text":"<p>Script-based partition table editor, similar to fdisk and gdisk, which can be run interactively.  It does not interface with GPT format, neither is it designed for large partitions.</p> <pre><code># Display size of a partition or disk\nsfdisk -s $PARTITION\nsfdisk -s $DEVICE\n\n# Apply consistency checks to partition or disk\nsfdisk -V $PARTITION\nsfdisk --verify $DEVICE\n\n# Create a partition\nsfdisk $DEVICE\n</code></pre>"},{"location":"Storage/#shred","title":"shred","text":""},{"location":"Storage/#shred_1","title":"shred","text":"Write random data to an unmounted disk for {n} passes <pre><code>shred --iterations=n\n</code></pre>"},{"location":"Storage/#wipefs","title":"wipefs","text":""},{"location":"Storage/#mount","title":"mount","text":""},{"location":"Storage/#mount-fstab-file","title":"Mount fstab file","text":"mount -a"},{"location":"Storage/#lvm_1","title":"LVM","text":""},{"location":"Storage/#lvm_2","title":"lvm","text":"<pre><code>lvm version\n</code></pre>"},{"location":"Storage/#lvresize","title":"lvresize","text":"Resize existent logical volume Marketing in volume group vg1 to have an additional 10 gigabytes of space <pre><code>lvresize -L +10G /dev/vg1/Marketing\n</code></pre>"},{"location":"Storage/#pvcreate","title":"pvcreate","text":"<pre><code>pvcreate /dev/sd{a,b,c}\n</code></pre>"},{"location":"Storage/#lvresize_1","title":"lvresize","text":"<p>Resize existent logical volume Marketing in volume group vg1 to have an additional 10 gigabytes of space <pre><code>lvresize -L +10G /dev/vg1/Marketing\n</code></pre></p> <p>It is possible to use LVM to format the storage media when installing CentOS or RHEL on a virtual machine, even if there is only a single disk. This will result in a swap partition being created as a small logical volume. This can be removed: <pre><code>swapoff cs/swap\nlvremove cs/swap\n</code></pre> Then the remaining logical volume mounted to root can be expanded: <pre><code>lvresize -l 100%VG cs/root\n</code></pre></p>"},{"location":"Storage/#pvcreate_1","title":"pvcreate","text":"<pre><code>pvcreate /dev/loop0\n</code></pre>"},{"location":"SystemD/","title":"SystemD","text":"<p>SystemD is the de facto Linux init system since replacing Sysvinit and Upstart in all major distributions.  SystemD organizes resources into units, which can be managed by daemons and manipulated by SystemD utilities.</p> <p>It was designed by a pair of Red Hat developers in 2010 to be a general purpose system manager. It offers parallel execution, explicit dependencies between services, an escape from slow shell scripts, and per-daemon resource control and watchdogs.</p>"},{"location":"SystemD/#tasks","title":"Tasks","text":""},{"location":"SystemD/#scheduling-services","title":"Scheduling services","text":"<p>Services can be scheduled to start with timers. sshd.timer<pre><code>[Unit]\nDescription=Starts sshd service at beginning of workday, and shuts it down at the end.\n\n[Timer]\nUnit=sshd.service\nOnCalendar=Mon..Fri *-*-* 09:00:00\n\n[Install]\nWantedBy=timers.target\n</code></pre></p> <p>Now, when stopping sshd manually the following output is printed. <pre><code>Warning: Stopping sshd.service, but it can still be activated by:\n  sshd.timer\n</code></pre></p> <p>The service can be scheduled to shutdown within the service file itself using the RuntimeMaxSec directive. sshd.service<pre><code>RuntimeMaxSec=36000 # i.e. 10 hours\n</code></pre></p> <p>This unfortunately will result in the service being reported as failed. This failure can be cleared with this command: <pre><code>systemctl reset-failed\n</code></pre></p>"},{"location":"SystemD/#masking","title":"Masking","text":"On TrueNAS, the libvirtd socket is masked by default.  This means that virsh is not able to connect to the hypervisor until it is unmasked and the service restarted. <pre><code>systemctl unmask libvirtd.socket\nsystemctl restart libvirtd.service\nvirsh connect qemu:///system\n</code></pre>"},{"location":"SystemD/#glossary","title":"Glossary","text":""},{"location":"SystemD/#service-files","title":"Service files","text":"<p>Service files are a type of unit file which have replaced earlier init scripts and describe how to manage a service or application on the server. Active services are placed in /etc/systemd/system, whereas inactive service files distributed with installed packages are placed in /usr/lib/systemd/system.</p> <p>Docker container as a service: <pre><code>[Unit]\nDescription=Notes Container (Docker)\n\n[Service]\nExecStart=/usr/bin/docker start notes\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p>"},{"location":"SystemD/#slice","title":"Slice","text":"<p>A slice unit is a unit configuration file ending in \".slice\" which manages resources of a group of processes. SystemD slices implement and build on Linux cgroups.</p> <p>Slices exist in a hierarchy below the root slice (-.slice) and are used to group scopes and services</p> <ul> <li>Scopes contain unrelated processes but not necessarily hierarchically</li> <li>Services are from unit files or Transient Runtime Services and contain processes</li> </ul> <p>Root slices themselves only contain scopes and other slices.</p> <ul> <li> <p>user.slice contains all user-related slices and scopes, named after the pattern user-UID.slice</p> <ul> <li>session.slice is created for every login session</li> </ul> </li> <li> <p>system.slice contain slices, scopes, and services</p> </li> <li> <p>machine.slice contains all container-related slices, scopes, and services.</p> </li> </ul> <p>Services can be assigned to specific slices explicitly by editing the value of the Slice key in the service file. Keys like CPUWeight can assign cgroup resource controls. Other such controls can be viewed in the systemd.resource-control(5) man page. <pre><code>[Unit]\nSlice=user.slice\nCPUWeight=50\n</code></pre></p>"},{"location":"SystemD/#sysvinit","title":"SysVinit","text":"<p>SysVInit is the oldest init system used in Linux.</p> <p>In SysVinit, which used bash scripts to run and manage servicesj, processes were started serially and synchronously, wasting time and system resources. For years, a common mitigation was to run services in the background, simulating concurrency.</p>"},{"location":"SystemD/#target-files","title":"Target files","text":"<p>Target files are equivalent to SysVInit runlevels.</p> SystemD target SysVInit runlevel poweroff.target 0 rescue.target 1 multi-user.target 3 graphical.target 5 reboot.target 6 emergency.target emergency"},{"location":"SystemD/#timers","title":"Timers","text":"<p>Timer files are systemd unit files with names ending in .timer that control service files. For each timer file, a matching unit file must exist describing the unit to activate when the timer elapses. By default, systemd will search for a service file with a filename matching that of the timer, but failing that a specific unit can be specified with the Unit key within the timer file itself.</p> Display timers<pre><code>systemctl list-timers\nsystemctl status *timer\n</code></pre> <p>Like other unit files, timer files may include Unit and Install sections, but must include the Timer section.</p> <p>Specifying time is done using timestamps which can be monotonic or realtime.</p> <ul> <li>Monotonic timers are defined relative to various system hooks using the following directives: OnActiveSec, OnBootSec, OnStartupSec, OnUnitActiveSec, and OnUnitInactiveSec.</li> <li>Realtime timers define timers according to calendar event expressions, denoting real-world dates and times as humans understand them. </li> </ul> <p>Validate timestamps: <pre><code>systemd-analyze calendar '*-*-* 00:00:00' --iterations\n</code></pre></p> <p>systemd-run can be used for one-off events as a substitute for anacron. <pre><code>systemd-run --on-active=-30sec /bin/touch /home/user/file\n</code></pre> This command creates a transient unit file, whose name is provided in the output. <pre><code>systemctl cat run-u97.service\n</code></pre></p>"},{"location":"SystemD/#unit-files","title":"Unit files","text":"<p>Unit files are case-sensitive .ini files organized into sections. Unit files can be found in several directories:</p> <ul> <li>/lib/systemd/system where the system's copy of unit files are placed by default</li> <li>/etc/systemd/system where unit files override the system default</li> <li>/run/systemd/system where run-time unit definitions are found and given a higher priority than the system default in /lib but lower than that in /etc. These unit files are created dynamically and lost on reboot.</li> </ul> <p>Unit files come in many different types which can be identified by their filename extension (i.e. .service, target, etc.).</p>"},{"location":"SystemD/#upstart","title":"Upstart","text":"Upstart was an init system developed by Canonical for Ubuntu meant to replace SysVinit, but it was abandoned in 2014."},{"location":"SystemD/#commands","title":"Commands","text":""},{"location":"SystemD/#hostnamectl","title":"hostnamectl","text":"Permanently change hostname <pre><code>hostnamectl set-hostname $HOSTNAME\n</code></pre>"},{"location":"SystemD/#journalctl","title":"journalctl","text":"<p>Clean up old logs</p> <pre><code>journalctl --disk-usage # (3)\njournalctl --rotate # (1)\njournalctl --vacuum-time=1d # (2)\n</code></pre> <ol> <li>Ask journal daemon to rotate journal files, immediately archiving and renaming currently active journal files.</li> <li>--vacuum-size, --vacuum-time, and --vacuum-files can be used singly or in combination to enforce limits on archived journal files.</li> <li>Show current disk usage of all journal files</li> </ol> <p>Display logs</p> <pre><code>journalctl -r # --reverse (1)\njournalctl -f # --follow (2)\n</code></pre> <ol> <li>Display output in reverse (newest entries first)</li> <li>Continuously update the display as new log entries are created</li> </ol> <p>By default, SystemD logs to memory. This can be changed by adjusting /etc/systemd/journald.conf.  This requires the directory /var/log/journal to exist.</p> Persistent logging<pre><code>[Journal]\nStorage=persistent\n</code></pre>"},{"location":"SystemD/#localectl","title":"localectl","text":"Change locale to French <pre><code>localectl set-locale LANG=fr_FR.utf8\n</code></pre>"},{"location":"SystemD/#loginctl","title":"loginctl","text":"Enable user lingering, which allows users that are not logged in to run long-running services. <pre><code>loginctl enable-linger\nloginctl show-user | grep Linger - # Confirm\n</code></pre>"},{"location":"SystemD/#systemctl","title":"systemctl","text":"Services<pre><code>systemctl list-unit-files --type=service    # Display all services\nsystemctl enable --now $SERVICE             # Configure service to start on boot and start it immediately\nsystemctl status $SERVICE\nsystemctl is-active $SERVICE \nsystemctl disable $SERVICE\nsystemctl mask $SERVICE                     # Prevent service from being started inadvertently by another process\nsystemctl restart $SERVICE\n</code></pre> Boot targets<pre><code>systemctl get-default\nsystemctl set-default graphical.target\nsystemctl isolate emergency.target          # Change target\nsystemctl suspend                           # Suspend system\n</code></pre> <p>--user specifies the service manager of the calling user. <pre><code>systemctl --user enable --now container-notes.service # (1)\nsystemctl --user status container-notes.service\n</code></pre></p> <ol> <li>Here, container-notes.service has been created at ~/.config/systemd/user</li> </ol>"},{"location":"SystemD/#systemd-analyze","title":"systemd-analyze","text":"Check security of a service<pre><code>systemd-analyze security sshd.service\n</code></pre>"},{"location":"SystemD/#systemd-cgls","title":"systemd-cgls","text":"systemd-cgls recursively shows the contents of the selected cgroup hierarchy in a tree."},{"location":"SystemD/#systemd-delta","title":"systemd-delta","text":"<p>Show files that are overridden with systemd.</p> <p>Display differences among files when they are overridden <pre><code>systemd-delta --diff\n</code></pre></p>"},{"location":"Virtualization/","title":"Virtualization","text":"<p>The typical virtualization stack on Linux is referred to as QEMU/KVM; both of these are separate technologies.</p>"},{"location":"Virtualization/#tasks","title":"Tasks","text":""},{"location":"Virtualization/#check-cpu-for-virtualization-support","title":"Check CPU for virtualization support","text":"<pre><code>grep -E 'svm|vmx' /proc/cpuinfo # (1)\n</code></pre> <ol> <li>AMD CPUs will have svm in the flags section, whereas Intel CPUs will have vmx.</li> </ol>"},{"location":"Virtualization/#virtual-machine","title":"Virtual machine","text":"<p>The easiest way to create a VM is with the Boxes GNOME Desktop Environment application or virt-manager.</p> <pre><code>virt-install \\\n--cdrom=/tmp/debian-9.0.0-amd64-netinst.iso \\\n--vcpus=1 --memory=1024 --disk size=5 \\\n--os-variant=debian8\n    --name=linuxconfig-vm \\\n</code></pre>"},{"location":"Virtualization/#virtual-networking","title":"Virtual networking","text":"Using Boxes, a new interface will be created (apparently named tap0, etc) and slaved to a bridge, or a bridge will be created if none exist."},{"location":"Virtualization/#console-access","title":"Console access","text":"<p>In order to enable console access to a domain, the serial-gettty@.service service must be started. <pre><code>systemctl enable --now serial-getty@ttyS0.service\n</code></pre> Now console access is available from the host <pre><code>virsh console $DOMAIN\n</code></pre></p> <p>Note that the console will remain at 24 lines by 80 columns no matter if the terminal window is resized.</p>"},{"location":"Virtualization/#virtual-disk","title":"Virtual disk","text":"CoW filesystems <p>If the disk image is created on a filesystem that does not support O_DIRECT (ref. <code>man 5 open</code>), i.e. COW filesystem like btrfs and ZFS, the cache must be disabled.  This appears to be impossible on btrfs, so disk images must be created on partitions with non-CoW alternatives, like ext4 and xfs.</p> <p>A virtual disk can be created in various ways and in various formats (the sparse QCOW2 disk image format, associated with QEMU, is preferred).</p> <p>One way is to create the image using a utility like qemu-img. Create qcow2 disk image<pre><code>qemu-img create -f qcow2 disk0.qcow2 5G # (1)\n</code></pre></p> <ol> <li>By omitting -f, qemu-img will create a RAW format file.</li> </ol> <p>Alternatively, and more circuitously, a volume can be created within a storage pool. <pre><code>virsh pool-define-as --name Disks --type dir --target /disk/Disks # (1)\nvirsh pool-start Disks # (2)\nvirsh vol-create-as Disks disk0.qcow2 10G --format qcow2\n</code></pre></p> <ol> <li>A pool is deleted by first making sure its contents are deleted (but not the containing folder, in the case of a directory-based storage pool) <pre><code>virsh pool-destroy Disks  # Destroy the contents\nvirsh pool-delete Disks   # Delete directory\nvirsh pool-undefine Disks # Delete resource\n</code></pre></li> <li>Alternatively, activate pool on boot <pre><code>virsh pool-autostart Disks\n</code></pre></li> </ol> <p>Regardless of the method, the disk image is then attached to the domain, whether or not it is running.</p> <pre><code>virsh attach-disk rhel \\\n--source /tmp/Disks/disk0.qcow2 \\\n--target vdb --cache none \\\n--driver qemu --subdriver qcow2 \\\n--persistent # (1)\n</code></pre> <ol> <li>--persistent is necessary for the disk to remain attached after a shutdown. If the domain is not running, --config is necessary. <pre><code>virsh attach-disk rhel \\\n--source /tmp/Disks/disk0.qcow2 \\\n--target vdb --cache none \\\n--driver qemu --subdriver qcow2 \\\n--config\n</code></pre> This can be reversed with the following command. <pre><code>virsh detach-disk rhel /tmp/Disks/disk0.qcow2\nvirhs detach-disk rhel vdb # Specifying target instead\n</code></pre></li> </ol>"},{"location":"Virtualization/#snapshots","title":"Snapshots","text":"<p>Snapshots of VMs can be taken from within Boxes or via the command-line: <pre><code>virsh snapshot-create-as rhel --name \"Disks added\"\n</code></pre></p> <p>These produce XML records that can then be viewed: note that the field that Boxes uses to identify each snapshot is actually \"description\". <pre><code>virsh snapshot-dumpxml rhel \"Disks added\"\n</code></pre></p> <p>Snapshots can be renamed by changing the value of the name element in a text editor. <pre><code>virsh snapshot-edit rhel --snapshotname \"Disks added\" --rename\n</code></pre></p> <p>Revert to a snapshot <pre><code>virsh snapshot-revert rhel --snapshotname </code></pre></p>"},{"location":"Virtualization/#custom-resolution","title":"Custom resolution","text":"<p>Specify a custom resolution in a VM using KVM <pre><code>cvt 2560 1440\nxrandr --newmode \"2560x1440_60.00\" 312.25 2560 2752 3024 3488  1440 1443 1448 1493 -hsync +vsync\nxrandr --addmode Virtual-1 2560x1440_60.00\nxrandr --output Virtual-1 --mode 2560x1440_60.0\n</code></pre></p> <p>On a Hyper-V VM, this method will not work, but a simple change to a line in /etc/default/grub will do the trick after running update-grub and restarting</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash video=hyperv_fb:1920x1080\"\n</code></pre>"},{"location":"Virtualization/#commands","title":"Commands","text":""},{"location":"Virtualization/#virt-install","title":"virt-install","text":""},{"location":"Virtualization/#virsh","title":"virsh","text":"<p>virsh is the main interface for managing libvirt guest domains. In virsh terminology, the word domain refers to a VM.</p> <p>virsh commands can be entered from the virsh shell or from the command-line.</p> <pre><code>virsh list --all\nvirsh start rhel\nvirsh shutdown rhel\nvirsh destroy rhel  # Forcefully\nvirsh undefine rhel\nvirsh autostart rhel # (1)\n</code></pre> <ol> <li>Start domain at boot</li> </ol> <p>Inspect a running domain <pre><code>virsh dominfo $DOMAIN\nvirsh domiflist $DOMAIN\nvirsh domblklist $DOMAIN\nvirsh domrename $DOMAIN $NEWNAME`\n</code></pre></p> <p>Virsh uses XML documents to define domain resources. <pre><code>virsh dumpxml $DOMAIN\nvirsh edit $DOMAIN\n</code></pre></p> <p>Several virsh commands come in two varieties, one to handle XML documents that define resources and another to define them inline from the command-line.</p> XML CLI Description pool-define pool-define-as Define a persistent storage pool vol-create vol-create-as Create a volume <p>The commands ending in -as also support use of the --print-xml option, which will output the equivalent XML document to stdout.</p>"},{"location":"Virtualization/#glossary","title":"Glossary","text":""},{"location":"Virtualization/#boxes","title":"Boxes","text":"Boxes is a GNOME Desktop Environment application intended as a more user-friendly alternative to virt-manager to make virtualization easier for end-users."},{"location":"Virtualization/#domain","title":"domain","text":"In libvirt, a domain is a guest VM."},{"location":"Virtualization/#kvm","title":"KVM","text":"<p>KVM is a FreeBSD and Linux kernel module that allows the kernel to function as a hypervisor.</p> <p>KVM was first merged into kernel 2.6.20.</p>"},{"location":"Virtualization/#libvirt","title":"libvirt","text":"libvirt is an open-source API, daemon, and management tool for managing virtualization. It is a C toolkit to interact with the virtualization capabilities of the Linux KVM module, but it can also be used with KVM along with other virtualization technologies like QEMU."},{"location":"Virtualization/#qemu","title":"QEMU","text":"QEMU is an open-source emulator that interoperates with KVM to run VMs at near-native speed when the guest architecture is the same as that of the host."},{"location":"Virtualization/#storage-pool","title":"storage pool","text":"In libvirt, a storage pool is a file, directory, or storage device managed by libvirt to provide storage for domains."},{"location":"Applications/bind/","title":"BIND","text":"<pre><code>apt install bind9 bind9-utils bind9-dnsutils -y\n</code></pre> <p>Set BIND to IPv4 mode in the service parameters file:</p> /etc/default/bind9<pre><code>OPTIONS=\"-4 -u bind\"\n</code></pre> <p>BIND configs have a unique syntax that make heavy use of the semicolon. The main config is at /etc/named.conf on Arch and RHEL systems and at /etc/bind/named.conf on Ubuntu.</p> <p>A DNS zone is a database with resource records for a specific sub-tree in the domain space.</p> <p>A DNS zone requires a start of authority (SOA) record. For readability, admins typically break the record apart into lines with comments describing each field following a semicolon.</p> Representative SOA record<pre><code>@ IN SOA ns1.example.com. hostmaster.example.com. (\n                          2022070601 ; serial number\n                          1d         ; refresh period\n                          3h         ; retry period\n                          3d         ; expire time\n                          3h )       ; minimum TTL\n</code></pre> <p>Allow recursive queries from trusted clients</p> /etc/bind/named.conf.options<pre><code>acl \"trusted\" {\n    10.128.10.11;    # ns1 - can be set to localhost\n    10.128.20.12;    # ns2\n    10.128.100.101;  # host1\n    10.128.200.102;  # host2\n};\n</code></pre> <p>Allow recursion</p> /etc/bind/named.conf.options<pre><code>options {\n    directory \"/var/cache/bind\";\n\n    recursion yes;\n    allow-recursion {trusted; };\n    listen-on { 0.0.0.0; };\n    allow-transfer { none; };\n    forwarders { 192.168.1.1; };\n};\n</code></pre> <p>Now zone files can be specified in named.conf.local. An additional zone and zone file must be specified for every private subnet.</p> /etc/bind/named.conf.local<pre><code>zone \"mydns\" {\n        type master;\n        file \"/etc/bind/zones/db.mydns\";\n}\n</code></pre> <p>The actual zone files can be copied from /etc/bind/db.local and edited manually.</p> /etc/bind/zones/db.mydns<pre><code>\n</code></pre>"},{"location":"Applications/ffmpeg/","title":"ffmpeg","text":""},{"location":"Applications/ffmpeg/#convert-format","title":"Convert format","text":"ffmpeg is most often used to convert file formats for media from the command-line Convert mp3 to m4a<pre><code>ffmpeg -i media.{mp3,m4a}\n</code></pre>"},{"location":"Applications/ffmpeg/#display-metadata","title":"Display metadata","text":"<pre><code>ffmpeg -hide_banner -i $INPUT\n</code></pre>"},{"location":"Applications/ffmpeg/#specify-metadata","title":"Specify metadata","text":"<p>Metadata is defined as key/value pairs, although not all formats support all metadata. This example adds metadata but does not reencode the input file. <pre><code>ffmpeg -i $INPUTFILE -metadata title=$TITLE -metadata year=$YEAR -codec copy $OUTPUTFILE\n</code></pre></p> Tag all audio in a directory<pre><code>for INPUT in $(ls *.mp3);\ndo\nffmpeg -i \"$INPUT\" -c copy -metadata genre=Amapiano \"${INPUT/.mp3/_tagged.mp3}\"\ndone\n</code></pre>"},{"location":"Applications/ffmpeg/#concatenating-multiple-files","title":"Concatenating multiple files","text":"<p>It is possible to combine many files into one. The canonical way of doing this is by first assembling a list of filenames. These must appear in a specific format: <pre><code>file 'file1.mp3'\nfile 'file2.mp3'\n# etc...\n</code></pre></p> <p>This can be done quickly by piping the output of ls to a file, then editing it manually. <pre><code>echo $(ls -1 *.mp3) &gt; files\n</code></pre></p> <p>Then this file can be used by ffmpeg, specifying the concat demuxer as the argument to -f <pre><code>ffmpeg -f concat -i files -c copy compilation.mp3\n</code></pre></p> <p>Chapters will be accepted with the right container (apparently not mp3). Note that mp3 files cannot be placed into a m4a container without re-encoding. Also note that the -map_metadata option must be specified after the second infile, because its argument refers to the second infile as if it were zero-indexed. <pre><code>ffmpeg -f concat -i files -i chapters -map_metadata 1 -c copy compilation.m4a\n</code></pre></p>"},{"location":"Applications/git/","title":"Git","text":""},{"location":"Applications/git/#git_1","title":"Git","text":"<p>Git is a very complex utility with multiple commands and subcommands and a strong dependency on version control system concepts.</p> <p>The most basic useful command may be clone which simply downloads a repository. <pre><code>git clone https://gitlab.gnome.org/GNOME/gtk.git\n\n# The depth of the tree can be specified, and various configuration parameters can be passed with -c\ngit clone https://gitlab.gnome.org/GNOME/gtk.git --depth 1 -c http.sslVerify=false\n</code></pre></p> <p>Add file, located in <code>$HOME</code> to the git repo at <code>$PATH</code></p> <pre><code>git --git-dir=$PATH.git --work-tree=$HOME add file\n</code></pre> <pre><code># Update index to include all files in the working tree, including removals\ngit add -A # --no-ignore-removal\n\n# Stage all modifications in work-tree, including deletions\ngit add -u\n</code></pre> <p>See a list of branches.  A \"*\" indicates that branch is checked out. <pre><code>git branch\n</code></pre></p> <pre><code># Display the last commit for each branch\ngit branch -v\n\n# Display branches that have not been merged\ngit branch --no-merged\n</code></pre> <pre><code># Discard unstaged uncommitted changes to file\ngit checkout -- file\n\n# Switch to branch\ngit checkout branch\n</code></pre> <p>Apply a single, specific commit from another branch <pre><code>git cherry-pick commit\n</code></pre></p> <p>Provides a frontend to the INI formatted config files typically found within <code>.git/config</code> (or <code>~/.gitconfig</code> when using <code>--global</code>)</p> <pre><code># Set up alias \"br\" for `branch`\ngit config --global alias.br branch # (1)\n\n# Store authentication details in a cache\ngit config --global credential.helper cache # (2)\n</code></pre> <ol> <li>Equivalent to: <pre><code>[alias]\nbr = branch\n</code></pre></li> <li>Equivalent to  <pre><code>[credential]\nhelper = cache\n</code></pre></li> </ol> <p>Show commits between January 1 and January 5, 2016 <pre><code>git log --after=\"2016-01-01\" --before=\"2016-01-05\"\n</code></pre></p> <p>See commits that are on {branch} but not on {master} <pre><code>git log master..branch\n</code></pre></p> <p>Show tracked files <pre><code>git ls-files\n</code></pre></p> <p>Show tracked files, each line is terminated by a null byte <pre><code>git ls-files -z\n</code></pre></p> <p>Show tracked files that have been deleted <pre><code>git ls-files --deleted\n</code></pre></p> <p>Move or rename a tracked file <pre><code>git mv file\n</code></pre></p> <p>Transfer data from local branch {master} to remote {origin} <pre><code>git push -u origin master\n</code></pre></p> <p>To add changes to a commit that is not the most recent, a rebase is necessary.  First stash the changes to be added, then initiate a rebase and mark the commit to be edited with <code>edit</code> or <code>e</code>.  Leave the other commits alone, save, and drop back to the stash.  Pop the stash (<code>git stash pop</code>), which will apply the changes stored in the most recent stash.  Now you can stage the changes and commit: <pre><code>git commit --amend --no-edit\n</code></pre></p> <p>Finally, continue the rebase, rewriting the rest of the commits against the new one. <pre><code>git rebase --continue\n</code></pre></p> <p>Combine branches by replaying the changes made on one branch to another <pre><code>git rebase\n</code></pre></p> <p>Add remote repo <pre><code>git remote add $REPO $URL\n</code></pre></p> <pre><code># Display URL of remote repo\ngit remote get-url $REPO\n\n# Set url for existing repo\ngit remote set-url $URL $REPO\n</code></pre> <p>Undo unstaged changes since last commit <pre><code>git reset --hard\n</code></pre></p> <p>Reset master to state before last commit <pre><code>git reset --hard HEAD~\n</code></pre></p> <p>Remove (committed) changes in {commit} <pre><code>git revert commit\n</code></pre></p> <p>Remove tracked file from repo <pre><code>git rm file\n</code></pre></p> <p>Stash changes to work-tree <pre><code>git stash\n</code></pre> View stashes in stash stack <pre><code>git stash list\n</code></pre> Apply changes in most recent stash <pre><code>git stash apply\n</code></pre> Apply changes in stash <code>$STASH</code> <pre><code>git stash apply stash@$STASH\n</code></pre></p>"},{"location":"Applications/git/#tasks","title":"Tasks","text":""},{"location":"Applications/git/#existing-codebase","title":"Existing codebase","text":"<p>An existing directory can be turned into a repo.</p> <pre><code>git init\n\n# Define an upstream repo\ngit remote add origin $REPO\n\n# Before a first commit, the global email and user name must be set.\ngit config --global user.name $NAME\ngit config --global user.email $EMAIL\n\n# Add and commit as normal\ngit add .\ngit commit -m $MESSAGE\n\n# The first push must have the upstream repo set\ngit push --set-upstream origin master\n</code></pre>"},{"location":"Applications/git/#rebasing","title":"Rebasing","text":"<p>Rebase changes committed to branch onto MASTER</p> <pre><code>git checkout $BRANCH\ngit rebase $MASTER\n</code></pre> <p>This will rewind $BRANCH to the commit shared by the two branches, then applying all changes made subsequently to $MASTER. </p> <p><pre><code>git checkout &lt;master&gt;\ngit merge &lt;branch&gt;\n</code></pre> Now the history will appear as though all changes were made in series, when they were actually made in parallel on separate branches. Move the last commit to a new branch <pre><code>git branch test         # create a new branch with current HEAD\ngit reset --hard HEAD~  # reset master to before last commit \ngit checkout test       # continue on new branch\n</code></pre> Line endings Git will automatically append CRLF endings on Windows. This setting can be displayed with the following command: <pre><code>git config core.autocrlf\n</code></pre> In order to disable this, adjust the setting <pre><code>git config core.autocrlf false\n</code></pre></p>"},{"location":"Applications/git/#squashing","title":"Squashing","text":"Sometimes many commits are made to resolve a single issue. These should be \"squashed\". To squash the last 4 commits: <pre><code>git rebase -i HEAD~4\n</code></pre> This will open a text editor where you will have to select what to do with each of the 4 commits. Most recent commits are at the bottom, and at least the top (oldest) commit has to remain \"pick\" in order to squash the others. The repo will have to be force-pushed once these changes have been made.  <pre><code>git push --force\n</code></pre> To add changes to the most recent commit, stage changes as normal (including removals), but when committing use the <code>--amend</code> option. This will present an editor, allowing you to edit the commit message, if necessary. [6] <pre><code>git add README.md\ngit commit --amend \n</code></pre> To split up <code>$COMMIT</code> <pre><code>git rebase -i \"$COMMIT\"^ # Start a rebase at the commit you want to split\n</code></pre> Mark the commit to be split with <code>edit</code>. Now reset state to the previous commit <pre><code>git reset HEAD^\n</code></pre> The files are presented unstaged, and can be added to new commits as needed. Finally, finish the rebase <pre><code>git rebase --continue\n</code></pre>"},{"location":"Applications/git/#tig","title":"tig","text":"<p>Provides a curses-based browser that allows you to navigate the commits in the current branch.  It is essentially a wrapper around <code>git log</code>, and therefore accepts the same arguments that can be passed to it. Tig's config is at ~/.tigrc.</p> <pre><code># Browse the commit history for a single file\ntig $FILE\n\n# Filter commits to a date range\ntig --after=\"2017-01-01\" --before=\"2018-05-16\" -- $FILE\n\n# Find who made a change to a file and why\ntig blame $FILE\n\n# Browse stash\ntig stash\n\n# Browse refs\ntig refs\n\n# Navigate the output of git grep\ntig grep -i foo lib/Bar\n\n# Pipe a list of commit IDs to tig\ngit rev-list --author=olaf HEAD | tig show --stdin\n</code></pre>"},{"location":"Applications/gnome/","title":"GNOME","text":"<p>GTK3 attempted to get away from strong dependency on theming engines by introducing CSS stylesheets. This was supposed to make application theming simple and portable.</p> <p>In GTK4 you can choose either a theming engine or CSS stylesheets.</p>"},{"location":"Applications/gnome/#keyring","title":"Keyring","text":"<p>GNOME Keyring is a collection of components that store and manage application access to secrets, passwords, keys, and certificates.</p> <p>GNOME Keyring can be managed:</p> <ul> <li>Seahorse via GUI</li> <li>secret-tool which uses (and is included in) libsecret</li> <li>gnome-keyring-query which uses the archived libgnome-keyring</li> </ul>"},{"location":"Applications/gnome/#extensions","title":"Extensions","text":"<p>GNOME Extensions provide a variety of popular hacks and changes to the Shell. They are managed by gnome-extensions-app but they are typically added from the GNOME Extensions website using a browser plugin.</p> <p>These extensions are added to ~/.local/share/gnome-shell/extensions, but many of them can also be made available to all users by installing them using a package manager, in which case they are placed in /usr/share/gnome-shell/extensions.</p> <p>Extensions appear to be mostly JavaScript applications, so they can probably simply be git cloned into the respective directories as well.</p>"},{"location":"Applications/gnome/#configuration","title":"Configuration","text":"<p>dconf is a key-based blob database for storing GNOME configurations and application settings. These settings are stored as keys grouped under paths in a way analogous to the Windows Registry. </p> <ul> <li>dconf is also a CLI utility for reading and writing individual values or entire directories to and from a dconf database.  Direct manipulation of dconf is discouraged, rather users and developers are encouraged to use dconf-editor or gsettings.</li> <li>GSettings is a high-level API for application settings that serves as the frontend for dconf as well as a CLI utility for changing user settings.</li> </ul> <p>A dconf profile is a list of binary dconf databases, typically stored at /etc/dconf/profile/user</p> <p>Here is a representative dconf profile. user is the name of the user database, typically found at ~/.config/dconf/ or /etc/dconf/profile/user and local and site refer to binary databases named as such in /etc/dconf/db/. <pre><code>service-db:keyfile/user # (1)\nuser-db:user\nsystem-db:local\nsystem-db:site\n</code></pre></p> <ol> <li>This line sets the dconf keyfile backend, required when home directories are mounted over NFS</li> </ol> <p>Keyfiles are INI-format configs placed in directories like local.d/ that allow dconf settings to be specified declaratively. <pre><code>[org/gnome/desktop/input-sources]\nxkb-options=['terminate:ctrl_alt_bksp', 'compose:ralt'] # (1)\n</code></pre></p> <ol> <li>Equivalent to: <pre><code>gsettings set org.gnome.desktop.input-sources xkb-options=['terminate:ctrl_alt_bksp', 'compose:ralt']\n</code></pre></li> </ol>"},{"location":"Applications/gnome/#tasks","title":"Tasks","text":""},{"location":"Applications/gnome/#desktop-background","title":"Desktop background","text":"Create a keyfile for the local database in /etc/dconf/db/local.d/01-background <pre><code>[org/gnome/desktop/background]\n\npicture-uri='file:///usr/local/share/backgrounds/wallpaper.jpg'\npicture-options='scaled'\nprimary-color='000000'\nsecondary-color='FFFFFF'\n</code></pre>"},{"location":"Applications/gnome/#custom-application-shortcut","title":"Custom application shortcut","text":"<p>Custom shortcuts are stored in dconf using a \"relocatable schema\" which has three keys: name, command, and binding.</p> <pre><code>gsettings set org.gnome.setting-daemon.plugins.media-keys.custom-keybinding:/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/custom0 name 'Terminal'\ngsettings set org.gnome.setting-daemon.plugins.media-keys.custom-keybinding:/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/custom0 binding '&lt;Super&gt;Enter'\ngsettings set org.gnome.setting-daemon.plugins.media-keys.custom-keybinding:/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/custom0 command '/usr/bin/gnome-terminal'\n</code></pre> <p>Note that this doesn't seem to work...</p>"},{"location":"Applications/gnome/#file-associations","title":"File associations","text":"<p>File associations are stored in .desktop files stored in /usr/share/applications/.  These INI-format files store all kinds of metadata on installed applications, including names and keywords in all supported languages. Filetypes are stored under the MimeType key as semicolon-delimited MIME Types.</p> <pre><code>[Desktop Entry]\nType=Application\nMimeType=application/x-newtype\nName=My Application 1\nExec=myapplication1\n</code></pre> <p>MIME Type descriptors as stored as XML files stored in /usr/share/mime/packages/:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;mime-info xmlns=\"http://www.freedesktop.org/standards/shared-mime-info\"&gt;\n&lt;mime-type type=\"application/x-newtype\"&gt;\n&lt;comment&gt;new mime type&lt;/comment&gt;\n&lt;glob pattern=\"*.xyz\"/&gt;\n&lt;/mime-type&gt;\n&lt;/mime-info&gt;\n</code></pre>"},{"location":"Applications/gnome/#applications","title":"Applications","text":""},{"location":"Applications/gnome/#gio","title":"gio","text":"<p>GIO (Gnome Input/Output) is a library that facilitates interaction with virtual file systems. Applications built with the GIO library can access GVFS mounts, which can have many backends.</p> <p>GIO commands appear to substitute for common POSIX commands and GNU utilities (e.g. <code>gio cat</code>, <code>gio mkdir</code>, <code>gio rename</code>, etc).</p> <p>Set custom GIO metadata</p> <pre><code># Read (empty) attribute of new file\ngio info -a 'metadata::*' /tmp/myfile\n\n# Create attribute\ngio set -t string /tmp/myfile 'metadata::mynote' 'Please remember to delete this file!'\n\n# Rename file\ngio move /tmp/myfile /tmp/newfile\n\n# Confirm that attribute still exists\ngio info -a 'metadata::*' /tmp/newfile # (1)\n</code></pre> <ol> <li>gio info appears to have replaced the earlier gvfs-info and gvfs-mime utilities used to inspect registered MIME types.</li> </ol>"},{"location":"Applications/gnome/#gsettings","title":"gsettings","text":"<p>gsettings is the CLI frontend intended to support changes to GNOME application settings, stored in dconf databases.</p> Examples<pre><code># Change function of Caps Lock\ngsettings set org.gnome.desktop.input-sources xkb-options \"['caps:ctrl_modifier']\"\n\n# Change mouse cursor size to various sizes. This can also be done in GNOME as Settings &gt; Accessibility\ngsettings set org.gnome.desktop.interface cursor-size 24 # (1)\n\n# Enable GTK Inspector\ngsettings set org.gtk.Settings.Debug enable-inspector-keybinding true # (2)\n</code></pre> <ol> <li>Valid sizes include 24, 32, 48, 64, and 96</li> <li>Can be run with Ctrl+Shift+D</li> </ol>"},{"location":"Applications/gnome/#notify-send","title":"notify-send","text":"Used for displaying desktop notifications on GNOME Desktop Environment <pre><code>notify-send -i face-smile Hello \"Hello, World!\"\n</code></pre>"},{"location":"Applications/nginx/","title":"Nginx","text":"<p>Nginx (\"engine-x\") is described as an event-based reverse proxy server. This refers to the fact that it has an asynchronous architecture, unlike its competitors Apache and IIS which create a new blocking thread per connection. Nginx is much newer than Apache which started in 1995, although it has seen widespread adoption since 2008, growing mostly at Apache's expense. A typical and favored deployment is to place Nginx in the front-end and Apache in the back-end to combine the advantages of both platforms.</p> <p>Nginx follows the convention of even version numbers being stable and odd numbers being mainline or development.</p>  Red Hat Ubuntu <p><pre><code># /etc/yum.repos.d/nginx.repo\n\n[nginx]\nname=nginx repo\nbaseurl=http://nginx.org/packages/centos/7/$basearch/\ngpgcheck=0\nenabled=1\n</code></pre> <pre><code>dnf install nginx\n</code></pre></p> <p><pre><code># /etc/apt/sources.list\n\ndeb http://nginx.org/packages/ubuntu/ trusty nginx\ndeb-src http://nginx.org/packages/ubuntu/ trusty nginx\n</code></pre> <pre><code>curl -fsSL http://nginx.org/keys/nginx_signing.key\napt-key add nginx_signing.key\napt install nginx\n</code></pre></p> <p>Depending on installation method and distribution, configurations can exist in various directories. A config can be explicitly specified at runtime with <code>--conf-path</code>/<code>-c</code>.</p> <p>This option also appears in the output of <code>ps</code> for the Nginx master process, which is one way of interrogating which config is being used for the current Nginx instance. Nginx can also be interrogated for its default config with <code>-t</code></p> <p>Nginx config files contain directives: </p> <ul> <li>Simple directives like <code>listen *:80;</code> contain a name, multiple optional parameters, and a closing semicolon.  Parameters themselves can pass a value after an equal sign, i.e. <code>backlog=511</code>.</li> <li>Context directives (or simply \"contexts\", also \"block directives\") like <code>events</code>, <code>http</code>, and <code>server</code> wrap a group of other directives in a pair of braces and can be nested. Most simple directives can only be declared in specific contexts.</li> <li>There is also an implied main context which wraps all the contents of the file, and putting a simple directive into the main context means making it a top-level statemtn.</li> <li>Comments can be written using <code>#</code></li> </ul>"},{"location":"Applications/nginx/#examples","title":"Examples","text":"<p>A very simple representative config that creates an HTTP server listening on port 80 of every network interface, with no HTTP Host specified, from the specified root path:</p> Default<pre><code>events {\n}\n\nhttp {\nserver {\n}\n}\n</code></pre> Expanded with explicit values<pre><code>user nobody nogroup;\nworker_processes 1;\n\nevents {\nworker_connections 512;\n}\n\nhttp {\nserver {\nlisten *:80;\nserver_name \"\";\nroot /usr/share/nginx/html;\n}\n}\n</code></pre> <pre><code>http {\nserver {\nlisten 8080;\nroot /www;\n\nlocation /images {\nroot /;\n}\n}\n}\nevents { }\n</code></pre> <pre><code>nginx -s stop\nnginx -s start\nnginx restart\n</code></pre>"},{"location":"Applications/nginx/#reverse-proxy","title":"Reverse proxy","text":"<p>Each Nginx virtual server should be described by a file in the /etc/nginx/sites-available directory.  These are linked to by symlinks placed in /etc/nginx/sites-enabled. Configuring a reverse proxy involves associating routes to proxied servers in these virtual server configs. </p> <pre><code>server {\nlisten 80;\nlocation / {\nproxy_pass \"http://127.0.0.1:8000\";\n}\n}\n</code></pre> <p>The configuration to serve static files placed in the local directory /path/to/staticfiles from the URL /static is: <pre><code>location /static/ {\nroot /path/to/staticfiles/\n}\n</code></pre></p>"},{"location":"Applications/nginx/#load-balancer","title":"Load balancer","text":"<p>A load balancer is similar to a reverse proxy, with the following differences.</p> <ul> <li>Load balancers perform reverse proxy across many backends, rather than a single one</li> <li>Load balancers operate at either Layer 7 or Layer 4, whereas a reverse proxy operates only at Level 7</li> <li>Load balancers are expected to handle much higher scale.</li> </ul> <p>Load balancers themselves tend to be load balanced by DNS servers, which can serve multiple A records to clients which are supposed to choose one of the IP addresses at random. Some DNS providers like AWS Route 53 randomize the order of these records per query.</p> <pre><code>http {\nupstream backend {\nserver 192.0.2.10;\nserver 192.0.2.11;\n}\n\nserver {\nlisten 80;\n\nlocation / {\nproxy_pass http://backend;\n}\n}\n}\n</code></pre>"},{"location":"Applications/nginx/#glossary","title":"\ud83d\udcd8 Glossary","text":""},{"location":"Applications/pam/","title":"PAM","text":""},{"location":"Applications/rhythmbox/","title":"Rhythmbox","text":""},{"location":"Applications/rhythmbox/#rhythmbox","title":"Rhythmbox","text":"<p>Rhythmbox's database is an XML file located at $HOME/.local/share/rhythmbox/rhythmdb.xml</p> <pre><code>&lt;?xml version=\"1.0\" standalone=\"yes\"?&gt;\n&lt;rhythmdb version=\"2.0\"&gt;\n&lt;entry type=\"song\"&gt;\n&lt;title/&gt;\n&lt;genre/&gt;\n&lt;artist/&gt;\n&lt;album/&gt;\n&lt;location&gt;file:///Music/...&lt;/location&gt;\n&lt;!-- snip --&gt;\n&lt;/entry&gt;\n&lt;/rhythmdb&gt;\n</code></pre>"},{"location":"Applications/screen/","title":"screen","text":""},{"location":"Applications/selinux/","title":"SELinux","text":"<p>Info</p> <ul> <li>SELinux (Arch Linux Wiki)</li> </ul> <p>SELinux implements Mandatory Access Control (MAC) in Linux, which is distinguished from traditional Linux access controls (file permission octets, the use of sudo, etc) which constitute Discretionary Access Control (DAC).</p> <p>SELinux's config is at /etc/selinux/config Example config<pre><code># This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n#     enforcing - SELinux security policy is enforced.\n#     permissive - SELinux prints warnings instead of enforcing.\n#     disabled - No SELinux policy is loaded.\nSELINUX=enforcing\n# SELINUXTYPE= can take one of these three values:\n#     targeted - Targeted processes are protected,\n#     minimum - Modification of targeted policy. Only selected processes are protected.\n#     mls - Multi Level Security protection.\nSELINUXTYPE=targeted\n</code></pre></p>"},{"location":"Applications/selinux/#contexts","title":"Contexts","text":"<p>SELinux security contexts define access controls and are also referred to as \"labels\". All system objects have such contexts associated with them, stored in the extended attributes of the filesystem.</p> <p>Contexts are compared to subject, verb, and object in English sentences and have the following structure: <pre><code>user_u:role_r:type_t:level\n</code></pre></p> <p>The user or user identity can be associated with one or more roles. User identities are suffixed with _u, and there a eight such identities builtin. </p> <ul> <li>By default, all non-root users are mapped to unconfined_u as is root itself, which means they operate with unlimited privileges. </li> <li>Users labeled with user_u cannot run su or sudo or programs in their home directories.</li> </ul> <p>The role is an attribute of the RBAC security model that classifies who is allowed to access what (domains, types). It can be associated to one or more types and is suffixed with _r.</p> <p>The type (for file contexts) or domain (for process contexts) defines what processes or domains the user can access. Types are suffixed with _t.</p> <ul> <li>Certain files have their own types, like passwd_file_t which is associated with /etc/passwd.</li> <li>Builtin and user-created file contexts are stored in the file_contexts and file_contexts.local files under /etc/selinux/targeted/contexts/files/.</li> </ul> <p>A level is an attribute of Multi-Level Security and Multi-Category Security.</p> <p>SELinux extends existing utilities to handle contexts with the -Z flag: <pre><code>ps auxZ\nls -Z\nid -Z\n</code></pre></p>"},{"location":"Applications/selinux/#booleans_1","title":"Booleans","text":"In SELinux, booleans refers to optional settings that can be turned on and off."},{"location":"Applications/selinux/#tasks","title":"Tasks","text":""},{"location":"Applications/selinux/#samba-share","title":"Samba share","text":"<p>Enabling a Samba file share requires setting a specific SELinux context using semanage <pre><code>semanage fcontext -a -t samba_share_t '/samba(/.*)?'\n</code></pre></p> <p>The context must then be restored <pre><code>restorecon -vvFR /samba\n</code></pre></p>"},{"location":"Applications/selinux/#troubleshooting-apache","title":"Troubleshooting Apache","text":"<p>In this scenario, an Apache httpd daemon fails to start due to SELinux.</p> <p>SELinux provides recommended commands to resolve the issue in the audit log at /var/log/messages: <pre><code>grep httpd /var/log/messages | less\n</code></pre></p> <p>Generate and install a new policy module from the logs <pre><code>ausearch -c httpd --raw | audit2allow -M my-httpd\nsemodule -i my-httpd\n</code></pre></p>"},{"location":"Applications/selinux/#apache-home-directories","title":"Apache home directories","text":"<p>A feature of Apache is that users can host personal websites from the directory named public_html in their home directories. When Apache policies are in effect, this directory is automatically given the http_user_content_t tag, which will allow the httpd daemon to host the website at the path /~user where \"user\" is the name of the user. <pre><code>curl localhost/~user/index.html\n</code></pre></p> <p>However, without a specific boolean enabled, the files will not be accessible: <pre><code>setsebool -P httpd_enable_homedirs 1\n</code></pre></p>"},{"location":"Applications/selinux/#apache-port","title":"Apache port","text":"<p>Default Apache settings appear in the main config file located at /etc/httpd/conf/httpd.conf.</p> <p>For example, the default directory served by Apache can be changed by setting a new value for the DocumentRoot directive. /etc/httpd/conf/httpd.conf<pre><code>DocumentRoot \"/web\"\n# ...\n&lt;Directory \"/web\"&gt;\n</code></pre></p> <p>Create example content <pre><code>echo \"Hello, World!\" &gt; /web/index.html\n</code></pre> The context for the content must be set. <pre><code># Set context manually\nchcon -R /web -t http_content_t\n\n# Alternatively, set policy and restore the contexts\nsemanage -a -t httpd_sys_content_t '/web(/.*)?'\nrestorecon -R /web\n</code></pre></p> <p>The default port can also be changed with the Listen directive. /etc/httpd/conf/httpd.conf<pre><code>Listen 1000\n</code></pre></p> <p>Now Apache will attempt to serve /web from port 1000, however while SELinux is enforcing policy this port will not be accessible to the daemon, and in fact Apache will exit with an error code.</p> <p>These errors can be inspected in a variety of ways. <pre><code>journalctl -xe\nausearch -m AVC -ts recent\n\n# Find SELinux message IDs, which provide remediation tips:\ngrep sealert /var/log/messages\nsealert -l $MESSAGE_ID\n</code></pre></p> Add port context<pre><code>semanage port -a -t http_port_t -p tcp 1000\n</code></pre> <p>Now starting httpd succeeds, and we can confirm that port 1000 is open: <pre><code>ss -nlt\n</code></pre></p>"},{"location":"Applications/selinux/#commands","title":"Commands","text":"<p>SELinux commands are separated into get/set varieties similar to PowerShell cmdlets:</p> get set object getenforce setenforce Operating mode getsebool setsebool Booleans <p>Similarly,</p> restore change object restorecon chcon Contexts"},{"location":"Applications/selinux/#audit2allow","title":"audit2allow","text":"Generate policy module from logs of denied operations<pre><code>ausearch -c httpd --raw | audit2allow -M my-httpd\n</code></pre>"},{"location":"Applications/selinux/#ausearch","title":"ausearch","text":"<p>Display events in a date range <pre><code>ausearch --start $STARTDATE --end $ENDDATE\n</code></pre></p> <p>Search events for today for logins of UID 500 <pre><code>ausearch --start today --loginuid 500\n</code></pre></p> <p>Search for events associated with an executable. <pre><code>ausearch -c httpd --raw\n</code></pre></p> Display recent (10 minutes) events<pre><code>ausearch -m AVC -ts recent\n</code></pre>"},{"location":"Applications/selinux/#chcon","title":"chcon","text":"<p>Change context of a file to be hosted via httpd <pre><code>chcon system_u:object_r:httpd_sys_content_t:s0 index.html\nchcon -t httpd_sys_content_t index.html # (1)\n</code></pre></p> <ol> <li>Change only the type portion of the context.</li> </ol>"},{"location":"Applications/selinux/#getenforce","title":"getenforce","text":"<p>getenforce displays the operating mode of SELinux, which can be one of three values:</p> <ul> <li>enforcing</li> <li>permissive</li> <li>disabled</li> </ul>"},{"location":"Applications/selinux/#getsebool","title":"getsebool","text":"Display all booleans<pre><code>getsebool -a # (1)\n</code></pre> <ol> <li>A more descriptive listing of the booleans can be displayed with semanage <pre><code>semanage bool -l\n</code></pre></li> </ol>"},{"location":"Applications/selinux/#restorecon","title":"restorecon","text":"Restore security context policy <pre><code>restorecon -R /web\n</code></pre>"},{"location":"Applications/selinux/#seinfo","title":"seinfo","text":"List users<pre><code>seinfo -u\n</code></pre>"},{"location":"Applications/selinux/#semanage","title":"semanage","text":"<p>semanage is used to configure certain elements of SELinux policy without requiring modification to or recompilation from policy sources.</p> File contexts<pre><code>semanage fcontext -l\nsemanage fcontext -a -t httpd_sys_content_t /web </code></pre> Port contexts<pre><code>semanage port -l\nsemanage port -a -t http_port_t -p tcp 1000\n</code></pre> Booleans<pre><code>semanage bool -l\n</code></pre> <p>Examine the mapping between Linux login names and SELinux users. <pre><code>semanage login -l\n</code></pre></p>"},{"location":"Applications/selinux/#semodule","title":"semodule","text":"Install a policy module<pre><code>semodule -i my-httpd.pp\n</code></pre>"},{"location":"Applications/selinux/#sestatus","title":"sestatus","text":"<pre><code>sestatus\n</code></pre>"},{"location":"Applications/selinux/#setenforce","title":"setenforce","text":"<pre><code>setenforce 0 # Permissive\nsetenforce 1 # Enforcing\n</code></pre>"},{"location":"Applications/selinux/#setsebool","title":"setsebool","text":"<pre><code># Allow SELinux to work with Samba (-P makes the change persistent)\nsetsebool -P samba_export_all_ro 1\n\n# Allow httpd to serve HTML from home directories\nsetsebool -P httpd_enable_homedirs 1\n\n# Prevent runtime changes to SELinux mode\nsetsebool -P secure_mode_policyload 1\n</code></pre>"},{"location":"Applications/ssh/","title":"SSH","text":"<p>SSH is a secure protocol and the most common way of remotely administering Linux servers and other equipment. SSH uses symmetrical encryption, which means a single key encrypts outbound messages to and inbound messages from the other participant.</p> <p>The SSH session is established in two stages:</p> <ol> <li>Negotiate session key</li> <li>Authenticate the user</li> </ol> <p>The symmetrical key used for the session, called the session key, is negotiated through the asymmetrical Diffie-Hellman key exchange protocol. This algorithm combines private data with public data from the other participant to produce the identical, session key, and the encryption used for the rest of the connection is called binary packet protocol.</p> <p>The simplest and least secure method of authentication is password-based. Although the password is sent through the encryption, it is still considered vulnerable to brute-force attacks.</p> <p>SSH key pairs, which are asymmetric, are recommended. These are what is generated by ssh-keygen, and stored in $HOME/.ssh with names that reflect the encryption algorithm, i.e. \"id_rsa\" and \"id_rsa.pub\", etc. The public key, used to encrypt data for the private key, can be freely shared. In fact, this is the purpose of ssh-copy-id, to share the public key. However the private key, which is used for decryption, must be kept secret.</p> <p>The client sends an ID for the key pair it wants to authenticate with to the server. The server then checks the authorized_keys file of the requested account for the ID. A random number is generated by the server, encrypted with the public key, and sent to the client. The client then decrypts the random number and combines it with the shared session key and calculates the MD5 hash. This hash is then sent back to the server, which checks the calculation.</p> <p>Note that the SSH server is named openssh-server in Ubuntu repos and the service is named ssh, as opposed to sshd on Red Hat systems.</p>"},{"location":"Applications/ssh/#tasks","title":"Tasks","text":""},{"location":"Applications/ssh/#port-forwarding","title":"Port forwarding","text":"<p>Port forwarding is accomplished in one of two ways, local or remote with respect to the server not the client.</p> <ul> <li>Local (<code>-L</code>): connections to the client are forwarded through the SSH tunnel to the SSH server. This technique is used to provide functionality similar to a VPN, where remote access is made possible to content on a private network, such as file shares or web applications that are not exposed publicly.</li> <li>Remote (<code>-R</code>)...</li> <li>Dynamic</li> </ul> <p>Here, a private web application served locally on ssh-server will be served on the client at the same port. The first \"localhost\" can actually be omitted, since the connection will be exposed on localhost host by default and is almost universally.</p> <p>The confusing part is the second \"localhost\", because that is actually in reference to the ssh server itself. <pre><code>ssh -L localhost:80:localhost:80 ssh-server\n</code></pre> It is actually possible to forward a request to another host on ssh-server's network, creating a jump box. Here a connection on the client's localhost:81 is forwarded to ssh-server, which then sends it to 192.168.1.1:80. <pre><code>ssh -L 81:192.168.1.1:80 ssh-server\n</code></pre></p>"},{"location":"Applications/ssh/#x-forwarding","title":"X forwarding","text":"<pre><code>ssh -Y user@host\n</code></pre> Have remote system use local computer {me.luna.edu}'s X display <pre><code>export DISPLAY=me.luna.edu:0\n</code></pre>"},{"location":"Applications/ssh/#fail2ban","title":"fail2ban","text":"<p>&lt;!-- On the first ban, f2b creates a new chain named f2b-name where \"name\" is the name of the jail, as defined in the config.</p> /etc/fail2ban/action.d/iptables.conf<pre><code>actionstart = &lt;iptables&gt; -N f2b-&lt;name&gt;\n</code></pre> <p>This chain becomes the target of banned IPs, which are somehow added (although I don't know how). --&gt;</p> <p>Fail2ban is an intrusion prevention framework written in Python and that runs as a service. It can be installed from most distributions' repos.</p> <p>The jail is a key concept in f2b that couples filters and actions definitions.</p> <p>Fail2ban is configured through .ini-format configs found in /etc/fail2ban. It is recommended not to edit the default configs ending in .config but rather to create a custom config called jail.local which will be automatically loaded by the service.</p> Example jail<pre><code>[sshd]\nenabled=true\nport=ssh\nfilter=sshd\nlogpath=/var/log/auth.log\nmaxretry=0\nfindtime=300\nbantime=3600\n</code></pre> <p>Failed logins can be checked by running lastb, and connections are also logged to SystemD. <pre><code>journalctl -ru sshd\n</code></pre></p> <pre><code># Display banned IPs\nfail2ban-client banned\n\n# Manually ban an IP\nfail2ban-client set sshd banip $IPADDRESS\n\n# Manually unban an IP\nfail2ban-client unban $IPADDRESS\n</code></pre>"},{"location":"Applications/ssh/#configuration","title":"Configuration","text":"<p>Server and client configuration both use the same set of keywords that can be defined inline on invocation or in config files.</p>"},{"location":"Applications/ssh/#client-configuration","title":"Client configuration","text":"<pre><code>Host home\n    HostName 192.168.1.1\n    User root\n    Port 50022\nSetEnv BAT_THEME=OneHalfLight # (1)\nLocalForward 8080 localhost:8080 # (2)\n</code></pre> <ol> <li>SetEnv allows environment variables to be set in a remote session.  However, these same environment variables must be explicitly specified in AcceptEnv key of the server configuration. This entry will set a specific syntax highlighting theme for use on the bat CLI utility. /etc/ssh/sshd_config<pre><code>AcceptEnv BAT_THEME\n</code></pre></li> <li>The use of LocalForward here is equivalent to the use of the -L option at the command-line: <pre><code>ssh -L 8080:localhost:8080 $SERVER\n</code></pre></li> </ol>"},{"location":"Applications/ssh/#transient-servers","title":"Transient servers","text":"<p>Prevent recording a transient server to the client's known hosts file. This is useful when SSHing to many hosts from a single client, say while managing a corporate environment.</p> <pre><code>UserKnownHostsFile /dev/null # (1)\nStrictHostKeyChecking no # (2)\n</code></pre> <ol> <li>UserKnownHostsFile</li> <li>StrictHostKeyChecking</li> </ol>"},{"location":"Applications/ssh/#canonical-hostname","title":"Canonical hostname","text":"<p>In corporate environments with verbose domain names, canonical hostnames can be configured to automatically append repetitive domain names (\"canonicalize\") to destination hosts.</p> <p>In this example, any connection made to a hostname beginning with \"server\" will append \"example.com\".</p> <pre><code>Host server*\n    CanonicalDomains example.com # (1)\nCanonicalizeHostname always # (2)\n</code></pre> <ol> <li>CanonicalDomains</li> <li>CanonicalizeHostname</li> </ol>"},{"location":"Applications/ssh/#server-configuration","title":"Server configuration","text":"/etc/ssh/sshd_config<pre><code># Disable cleartext passwords\nPasswordAuthentication no\n\n# Disable root login\nPermitRootLogin no\n</code></pre>"},{"location":"Applications/ssh/#commands","title":"Commands","text":""},{"location":"Applications/ssh/#endlessh","title":"endlessh","text":"Log verbosity<pre><code># Silent\nendlessh\n\n# Normal\nendlessh -v\n\n# Debug\nendlessh -vv\n</code></pre> Log verbosity<pre><code>LogLevel 0 # silent\nLogLevel 1 # normal\nLogLevel 2 # debug\n</code></pre>"},{"location":"Applications/ssh/#ssh-add","title":"ssh-add","text":"<pre><code>eval $(ssh-agent); ssh-add &lt;(cat \"$(keyFile.secureFilePath)\")\n</code></pre> <p>ssh-add adds private key identities to the OpenSSH authentication agent, ssh-agent. Notably, ssh-add requires the SSH_AUTH_SOCK environment variable set by ssh-agent.</p> <p>When run without arguments, it adds the private keys found in ~/.ssh, i.e.</p> <ul> <li>id_rsa</li> <li>id_dsa</li> <li>id_ecdsa</li> <li>id_ecdsa_sk</li> <li>id_ed25519</li> <li>id_ed25519_sk</li> </ul>"},{"location":"Applications/ssh/#ssh-agent","title":"ssh-agent","text":"<pre><code>eval $(ssh-agent); ssh-add &lt;(cat \"$(keyFile.secureFilePath)\")\n</code></pre> <p>ssh-agent is a helper program that keeps track of identity keys and passphrases, allowing them to be used without further interaction, similar to SSO. Running it produces output that is meant to be used with the eval command in order to set the environment variables SSH_AGENT_PID and SSH_AUTH_SOCK, which are needed by ssh-add.</p> <p>If only a single process is running, the -k option will kill it (although it is possible to fork multiple ssh-agent processes). <pre><code>ssh-agent -k # (1)\n</code></pre></p> <ol> <li>Equivalent to <pre><code>kill $SSH_AGENT_PID\n</code></pre></li> </ol>"},{"location":"Applications/ssh/#ssh-copy-id","title":"ssh-copy-id","text":"<p>This command copies the SSH public key to a specified account's ~/.ssh/authorized_keys file.</p> <p>In Windows, this command is not available, so a workaround is to simply pipe the public key over SSH itself.</p> <pre><code>type $env:USERPROFILE\\.ssh\\id_rsa.pub | ssh {IP-ADDRESS-OR-FQDN} \"cat &gt;&gt; .ssh/authorized_keys\"\n</code></pre>"},{"location":"Applications/ssh/#ssh-keygen","title":"ssh-keygen","text":"Generate host keys<pre><code>sudo ssh-keygen -A\n</code></pre>"},{"location":"Applications/syncthing/","title":"Syncthing","text":""},{"location":"Applications/syncthing/#syncthing","title":"SyncThing","text":""},{"location":"Applications/vim/","title":"vim","text":"<p>Unlike WYSIWYG editors which optimize input for writing text, vim optimizes for editing it. Vim offers a composable language for expressing these editing changes whose syntax can be composed into two elements, operations and text objects, which are analogous to verbs and nouns in language. YouTube</p> <p>The framework of understanding vim's syntax as a language appears to date back to an influential 2011 Stack Overflow post.</p> <p>On Unix-derived operating systems the main config file for Vim is placed at $HOME/.vimrc. On Windows it is placed at $HOME/_vimrc.</p>"},{"location":"Applications/vim/#syntax","title":"Syntax","text":""},{"location":"Applications/vim/#normal","title":"normal","text":"<p>Use <code>:normal</code> to define a series of normal-mode commands</p> Select all lines of a buffer<pre><code>:normal ggVG\n</code></pre>"},{"location":"Applications/vim/#keybindings","title":"Keybindings","text":"<p>There are two kinds of keybindings in Vim</p> <ul> <li>Recursive using command words <code>map</code>, <code>nmap</code>, <code>vmap</code>, etc. In these keybindings, the mapping itself is interpreted.</li> <li>Nonrecursive </li> </ul> <p>There are two types of keycodes:</p> <ul> <li>Vim keycodes which are identifiable as being in angle brackets: <code>&lt;Space&gt;</code>, <code>&lt;Return&gt;</code>, etc</li> <li>Terminal keycodes that appear similar to <code>^[[1;2A</code>. These may or may not be identifiable with the keycodes which the Linux kernel maps to raw keyboard scancodes. [ref][archwiki:Keyboard_input]</li> </ul> <p>The leader key is used to create more complicated keybindings using any arbitrary keypress, for example using <code>,</code> or <code>&lt;Space&gt;</code>.</p> <pre><code>let mapleader = ' '\n</code></pre>"},{"location":"Applications/vim/#autocommands","title":"Autocommands","text":"Autocommands expose an API that allows handling editor events like <code>BufNewFile</code>, <code>BufReadPost</code>, <code>BufWritePost</code>, <code>BufWinLeave</code>, and especially to implement functionality specific to filetypes. <p>Highlight added lines in green and removed lines in red in .diff files  <pre><code>filetype on\n\naugroup PatchDiffHighlight\n  autocmd!\n  autocmd FileType diff syntax enable\naugroup END\n</code></pre></p> <p>Turn syntax highlighting on only for certain filetypes  <pre><code>augroup PatchDiffHighlight\n  autocmd!\n  autocmd BufEnter *.patch,*.rej,*.diff syntax enable\naugroup END\n</code></pre></p>"},{"location":"Applications/vim/#color","title":"Color","text":"<pre><code>; Change the color of ELEMENT\nhighlight ELEMENT ctermfg=COLOR ctermbg=COLOR guifg=#abc123 guibg=#abc123\n\n; Select alternative colorschemes\n:colo[rscheme] &lt;tab&gt;\n\n; Display all available colorschemes\n:colo &lt;C-d&gt;\n</code></pre> <p>Clear custom color commands <pre><code>:highlight clear\n:hi clear\n</code></pre> Set file format to Unix/DOS <pre><code>:set fileformat=unix\n:set fileformat=dos\n</code></pre></p>"},{"location":"Applications/vim/#completion","title":"Completion","text":"<ul> <li>Context-aware completion</li> <li>Ctrl+X Ctrl+L</li> <li>Omni completion Ctrl+X Ctrl+O</li> </ul>"},{"location":"Applications/vim/#tasks","title":"Tasks","text":""},{"location":"Applications/vim/#invoking-to-a-specific-line-number","title":"Invoking to a specific line number","text":"<pre><code># Open with cursor at line 13\nvim .bashrc +13\n</code></pre>"},{"location":"Applications/vim/#configuration","title":"Configuration","text":"Configuration<pre><code>\" Prevent vim from creating backups files\nset nobackup\n\n\" Set relative line numbers\nset rnu\n</code></pre>"},{"location":"Applications/vim/#search-and-replace","title":"Search and replace","text":"<pre><code>\" Replace foo with bar across all lines, wherever they occur\n%s/foo/bar/g\n</code></pre>"},{"location":"Applications/vim/#mapping-keys","title":"Mapping keys","text":"Map Alt+J and Alt+K to move lines of text up or down <pre><code>nnoremap &lt;A-j&gt; :m .+1&lt;CR&gt;==\nnnoremap &lt;A-k&gt; :m .-2&lt;CR&gt;==\ninoremap &lt;A-j&gt; &lt;Esc&gt;:m .+1&lt;CR&gt;==gi\ninoremap &lt;A-k&gt; &lt;Esc&gt;:m .-2&lt;CR&gt;==gi\nvnoremap &lt;A-j&gt; :m '&gt;+1&lt;CR&gt;gv=gv\nvnoremap &lt;A-k&gt; :m '&lt;-2&lt;CR&gt;gv=gv\n</code></pre>"},{"location":"Applications/vim/#yanking-stdout","title":"Yanking STDOUT","text":"<p>To run a shell command from the normal mode command line, you simply run the <code>!</code> (\"bang\") command in normal mode. <pre><code>:!env\n</code></pre></p> <p>However to store the output of that command into a register, you must run a command like the following, which stores the output of the shell command into the a register. <pre><code>:let @a = system('env')\n</code></pre> The register signified by <code>@\"</code> will be placed into the buffer by the put command (<code>p</code>). <pre><code>:let @\" = system('env')\n</code></pre></p> <p>Alternatively <pre><code>:put =system('env')\n</code></pre></p>"},{"location":"Applications/vim/#filetype-associated-settings","title":"Filetype-associated settings","text":"Set indentation behavior specific to YAML<pre><code>autocmd FileType yaml setlocal ai ts=2 sw=2 et\n</code></pre>"},{"location":"Applications/vim/#plugins","title":"Plugins","text":"<p>Vim 8 supports native loading of plugins (put in $HOME/.vim/pack/start/</p> <p>vim-plug is a popular plugin manager.</p> <p>Install a plugin to provide Rust language support <pre><code>Plug 'rust-lang/rust.vim'\n</code></pre></p>"},{"location":"Applications/vim/#mouse-support","title":"Mouse support","text":"From here <pre><code>set mouse=a\n</code></pre>"},{"location":"Applications/vim/#language-definition","title":"Language definition","text":"<p>Syntax highlighting for various languages are stored in syntax files, stored in /usr/share/vim/vim82/syntax.</p> <p>Defining highlighting for pymdownx snippets</p> <pre><code>syn match markdownPymdownxSnippet '^-\\{2,}8&lt;-\\{2,} .*' \" (1)\nhi def link markdownPymdownxSnippet Error\n</code></pre> <ol> <li>Note that the quantifier specifying at least two instances of the preceding hyphen requires the initial brace to be escaped. However, the open angle bracket does not.</li> </ol>"},{"location":"Applications/youtube-dl/","title":"Youtube dl","text":""},{"location":"Applications/youtube-dl/#yt-dlp","title":"yt-dlp","text":"<pre><code># Download the highest quality stream in mp4 format\nyt-dlp -f 'bestvideo[ext=mp4]+bestaudio' $URL\n\n# Re-encode a video into mp4 and provide a filename\nyt-dlp --recode-video mp4 -o '%(upload_date&gt;%Y%m%d)s.mp4' $URL\n</code></pre>"}]}