# Overview

Containers run applications in an **isolated namespace**, meaning it only has access to resources that are made available to it by the container runtime.
**Resource governance** means that a container has access only to a specified number of processor cycles, system memory, and other resources.
Containers allow applications to be packaged with their dependencies in **container images**, which will run the same regardless of underlying operating system or infrastructure and are downloaded from **container registries** like **Docker Hub**.
Container registries are not to be confused with **repositories**, which are subcomponents of registries.

<div class="grid cards" markdown>

-   #### [Storage](https://docs.docker.com/storage/)

    ---

    By default, data written at the container layer is not persistent.
    Persistent data storage is achieved in one of two ways

    - **Volumes** are managed by the runtime and not accessible by other processes.
    - [**Bind mounts**](https://docs.docker.com/storage/bind-mounts/) can be stored anywhere on the host system.
    - **tmpfs** refers to mounts that are stored in the host system's memory only and which are not persistent.

    === ":simple-podman: Podman"

        ```sh
        podman volume create $VOL_NAME

        podman volume ls

        podman volume inspect $VOL_NAME

        podman volume rm $VOL_NAME
        ```

    === ":simple-docker: Docker"

        ```sh
        docker volume create $VOL_NAME

        docker volume ls

        docker volume inspect $VOL_NAME

        docker volume rm $VOL_NAME
        ```

</div>

## Cgroups

??? info "History"

    "Task Control Groups" were first merged into kernel [2.6.24](https://kernelnewbies.org/Linux_2_6_24) with the ability for multiple hierarchies to be created.
    The logic behind the creation of multiple hierarchies was to enable maximum flexibility in policy creation.
    However, because a controller could only belong to a single hierarchy, after some years this began to be seen as a [design flaw](https://lwn.net/Articles/484251/).
    
    This motivated a redesign into a [single unified cgroup hierarchy](https://lwn.net/Articles/601840/),
    and v2 was merged in [3.16](https://kernelnewbies.org/Linux_3.16) and made stable in [4.5](https://kernelnewbies.org/Linux_4.5).


**Control groups** or  **cgroups** are a Linux kernel feature that isolates, labels, and manages resources (CPU time, memory, and network bandwidth) for a collection of **tasks** (processes).


Cgroup **subsystems** (also called controllers or resource controllers in documentation) [represent](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/setting-limits-for-applications_managing-monitoring-and-updating-the-kernel#what-kernel-resource-controllers-are_setting-limits-for-applications) a single resource (i.e. **io**, **cpu**, **memory**, **devices**).

- **freezer** suspends or resumes tasks in a cgroup
- **net\_cls** tags network packets with a class identifier that allows the Linux traffic controller to identify packets originating from a particular cgroup task
- **net\_prio** allows network traffic to be prioritized per interface
- **ns** the namespace subsystem

Since cgroups v2, all mounted controllers reside in a single unified hierarchy.
A list of these is generated by the Linux kernel at **/proc/cgroups**.

You can confirm that the **cgroup2** filesystem is mounted at **/sys/fs/cgroup**
```sh
mount -l | grep cgroup -
```

**Namespaces** wrap global system resources (mount points, network devices, hostnames) in an [abstraction](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/setting-limits-for-applications_managing-monitoring-and-updating-the-kernel#what-kernel-resource-controllers-are_setting-limits-for-applications) that makes it appear to processes within that namespace that they have their own isolated instance of that resource.

Process IDs in the same **namespace** can have access to one another, whereas those in different namespaces cannot. 
Spawning a process in a new namespace prevents it from seeing the host's context, so an interactive shell like `zsh` spawned in its own namespace will report its PID as `1`, even though the host will assign its own PID. 

## Tasks

<div class="grid cards" markdown>

-   #### Elasticsearch

    ---

    ```sh
    # Create a new network
    docker network create elastic
    ```

    !!! info "Optional: verify image with [cosign](https://github.com/sigstore/cosign)"

        ```sh
        wget https://artifacts.elastic.co/cosign.pub
        cosign verify --key cosign.pub # => (1)
        ```

        1. 
        ```
        --8<-- "includes/Output/cosign"
        ```

-   #### Windows Server

    ---

    Windows Server 2016 supports **Windows Server Containers** and **Hyper-V Containers**, which create a separate copy of the operating system kernel for each container.
    The "Containers" feature must be installed on Windows Server 2016 hosts, and to create Hyper-V containers the Hyper-V role must also be installed (although the Hyper-V management tools are not necessary if VMs are not going to created).
    Windows container hosts need to have Windows installed to C:.

    Nano Server once could serve as Docker hosts, but no longer; Nano Servers are now intended to be deployed as containers themselves.

    The [Powershell Docker module](https://github.com/microsoft/Docker-PowerShell "PowerShell for Docker") has been deprecated for years.


??? info "Container registries"

    - **ghcr.io**: [GitHub Container Registry](https://ghcr.io)
    - **gcr.io** [Google Cloud Artifact Registry](https://gcr.io)
    - **mcr.microsoft.com**: [Microsoft Artifact Registry](https://mcr.microsoft.com/)


</div>

## Commands

<div class="grid cards" markdown>

-   #### cgconfig

    ---

    --8<-- "includes/Commands/c/cgconfig.md"

-   #### systemd-cgls

    ---

    --8<-- "includes/Commands/s/systemd-cgls.md"

</div>

## Glossary

**apiVersion**{: #apiversion } :material-kubernetes:
:   
    Kubernetes object field found in **Type** metadata.

    apiVersion is typically **v1**, but for some object types the API group is specified, i.e. for Deployments: 
    
    ```yaml
    apiVersion: apps/v1
    ```



**Deployment**{: #deployment } :material-kubernetes:
:   
    A uniformly managed set of Pod instances, all based on the same container image.
    The **Deployment controller** enables **release capabilities**, the deployment of new Pod versions with no downtime.
    Exposing a Deployment creates a [**Service**](#service).

**Desired State Management**{: #desired-state-management} :material-kubernetes:
:   
    The **Desired State Management** system is used by Kubernetes to describe a cluster's desired state declaratively.

**emptyDir**{: #emptydir }
:   
    Ephemeral Kubernetes volume type that shares the Pod's lifetime and where data is stored in RAM.
    emptyDir volumes can use [tmpfs](#tmpfs) file systems.

**ENTRYPOINT**{: #entrypoint }
:   
    Rarely used Docker declaration.
    When ENTRYPOINT is present, the [CMD](#cmd) declaration becomes the default argument passed to the command in ENTRYPOINT.

    The Kubernetes **--command** flag (**`pod.spec.containers.command`** resource) can override the contents of ENTRYPOINT.

**etcd**{: #etcd } :material-kubernetes:
:   
    Distributed key-value data store

**Event**{: #event } :material-kubernetes:
:   
    Kubernetes object type that contains information about what happened to the object.
    Events are deleted one hour after creation by default.
    ```sh
    kubectl get events
    kubectl get ev
    ```
    Unlike most other objects, Event manifests have no **spec** or **status** sections.

#### Helm
:   
    Helm is a package manager for Kubernetes. 

    Helm packages are refered to as **charts**.
    Charts are a collection of files and directories that adhere to a specification.
    A chart is **packed** when tarred and gzipped.

    - **Chart.yaml** contains metadata
    - **templates/** contains Kubernetes manifests potentially annotated with templating directives
    - **values.yaml** provides default configuration


    It is managed using the **helm** CLI utility.

    ```sh title="Create a new chart"
    helm create foo
    ```

    There is no longer a default Helm repository, although there are many available at the [Artifact Hub](https://artifacthub.io)

**kind**{: #kind } :material-kubernetes:
:   
    Kubernetes object field found in the **Type** metadata which specifies the type of resource, i.e. [Node](#node), [Deployment](#deployment), [Service](#service), [Pod](#pod), etc.

**kubeconfig**{: #kubeconfig} [:material-kubernetes:]()
:   
    A [**kubeconfig**](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) is YAML configuration file located at **$HOME/.kube/config** by default.
    A colon-delimited list of kubeconfigs can be specified by setting the **`KUBECONFIG`** environment variable.
    A kubeconfig can be explicitly specified with the **--kubeconfig** flag.

**Label**{: #label } [:material-kubernetes:]()
:   
    [**Labels**](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) are key-value pairs that are attached to Kubernetes objects.

    Config for a Pod with two labels:
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
    name: label-demo
    labels:
        environment: production
        app: nginx
    spec:
    containers:
    - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
    ```

**Master node**{: #master-node } :material-kubernetes:
:   
    A master node runs 3 processes, called master (control plane) components: 

    - **kube-apiserver**{: #kube-apiserver } exposes a RESTful API and serves as a glue between other Kubernetes components
    - **kube-scheduler**{: #kube-scheduler } determines how to balance container workloads across nodes using an algorithm
    - **kube-controller-manager**{: #kube-controller-manager } performs cluster operations like managing nodes and making changes to desired status

**millicore**{: #millicore } (m) :material-kubernetes:
:   
    One-thousandth of a vCPU or a CPU core and the preferred measurement unit of compute resources in Kubernetes (i.e. 128m = 12.8% of a CPU core and 2000m = 2 CPU cores).



**Node**{: #node } :material-kubernetes:
:   
    A **node** or worker is any container host that accepts workloads from the master node. 
    Each node is equipped with a container runtime like Docker, which it uses to create and destroy containers according to instructions from the master server.

    Each node runs 2 processes:

    - **kubelet**{: #kubelet } communicates with Kubernetes cluster services
    - **kube-proxy**{: #kube-proxy } handles container network routing using iptables rules

**PersistentVolume**{: #persistentvolume} [:material-kubernetes:](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
:   
    A **PersistentVolume** is a piece of storage in the cluster that has been provisioned using Storage Classes.

**PersistentVolumeClaim**{: #persistentvolumeclaim} :material-kubernetes:
:   
    ![](/img/persistent-volume-claims.png)
    A **PersistentVolumeClaim** requests either Disk or File storage of a particular StorageClass, access mode, and size. It is bound to a [PersistentVolume](#persistenvolume) once an available storage resource has been assigned to the pod requesting it.

**Pod**{: #pod } :material-kubernetes:
:   
    A **pod** is the most basic unit that K8s deals with, representing one or more tightly-coupled containers that should be controlled as a single application (typically one main container with subsidiary helper containers). 
    Every container should have only a single process, so if several processes need to communicate they should be implemented as separate containers in a pod.
    
    A pod's containers should:

    - operate closely together
    - share a lifecycle
    - always be scheduled on the same node
   






**tmpfs**{: #tmpfs }
:   
    RAM-backed file system used in Docker containers

**Volume**{: #volume }
:   
    A volume is a special directory in the Docker host that can be mounted to the container that is used to achieve persistent storage.

    In Azure, a volume represents a way to store, retrieve, and persist data across pods and through the application lifecycle. 
    In the context of Azure, Kubernetes can use two types of data volume:

    - **Azure Disks** using Azure Premium (SSDs) or Azure Standard (HDDs).
    - **Azure Files** using a SMB 3.0 share backed by an Azure Storage account.



